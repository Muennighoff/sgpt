{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1434579c",
   "metadata": {},
   "source": [
    "#### **BEIR SGPT**\n",
    "\n",
    "This notebook uses Log-Prob extraction to rerank BM25 predictions with GPT Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1854a",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e63139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3950, 0.0391, 0.8199, 0.9315, 0.2679, 0.5504, 0.0635, 0.1426, 0.1325,\n",
       "        0.8696], device='cuda:1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick torch & cuda test\n",
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.rand(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed30034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miniconda3/envs/semanticsearch/lib/python3.8/site-packages/beir/util.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from beir import util, LoggingHandler\n",
    "import logging\n",
    "# Code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223aefc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing kaggle.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile kaggle.json\n",
    "{\"username\":\"YOURNAME\",\"key\":\"APIKEY\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbb75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dir /home/.kaggle\n",
    "!kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179aa9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/.kaggle/kaggle.json'\n",
      "Downloading beirbm25results.zip to /home/repos/semanticsearch\n",
      "100%|████████████████████████████████████████| 433M/433M [01:32<00:00, 4.97MB/s]\n",
      "100%|████████████████████████████████████████| 433M/433M [01:32<00:00, 4.93MB/s]\n",
      "Archive:  beirbm25results.zip\n",
      "  inflating: beir_bm25_ndcgs.json    \n",
      "  inflating: results_arguana.json    \n",
      "  inflating: results_climate-fever.json  \n",
      "  inflating: results_cqadupstack_android.json  \n",
      "  inflating: results_cqadupstack_english.json  \n",
      "  inflating: results_cqadupstack_gaming.json  \n",
      "  inflating: results_cqadupstack_gis.json  \n",
      "  inflating: results_cqadupstack_mathematica.json  \n",
      "  inflating: results_cqadupstack_physics.json  \n",
      "  inflating: results_cqadupstack_programmers.json  \n",
      "  inflating: results_cqadupstack_stats.json  \n",
      "  inflating: results_cqadupstack_tex.json  \n",
      "  inflating: results_cqadupstack_unix.json  \n",
      "  inflating: results_cqadupstack_webmasters.json  \n",
      "  inflating: results_cqadupstack_wordpress.json  \n",
      "  inflating: results_dbpedia-entity.json  \n",
      "  inflating: results_fever.json      \n",
      "  inflating: results_fiqa.json       \n",
      "  inflating: results_hotpotqa.json   \n",
      "  inflating: results_msmarco.json    \n",
      "  inflating: results_nfcorpus.json   \n",
      "  inflating: results_nq.json         \n",
      "  inflating: results_quora.json      \n",
      "  inflating: results_scidocs.json    \n",
      "  inflating: results_scifact.json    \n",
      "  inflating: results_trec-covid.json  \n",
      "  inflating: results_webis-touche2020.json  \n"
     ]
    }
   ],
   "source": [
    "!mv kaggle.json /home/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d 'ANONYMIZED/beirbm25results'\n",
    "!unzip beirbm25results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062271e1",
   "metadata": {},
   "source": [
    "##### GPT Cross Encoder Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab48b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Reranker & Logprobs extraction\n",
    "##### Taken & adapted from: https://github.com/EleutherAI/lm-evaluation-harness/blob/ff8de12027f6d2d774bf56c3885f75122a37377c/lm_eval/models/gpt2.py#L110\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def encode(requests, tokenizer):\n",
    "    new_reqs = []\n",
    "    # Changed the order from original; as requests is queries, docs & we want query to be the continuation\n",
    "    for continuation, context in requests:\n",
    "        if context == \"\":\n",
    "            # end of text as context\n",
    "            context_enc = [tokenizer.eos_token_id]\n",
    "        else:\n",
    "            context_enc = tokenizer.encode(context, add_special_tokens=False)\n",
    "\n",
    "        continuation_enc = tokenizer.encode(continuation, add_special_tokens=False)\n",
    "\n",
    "        new_reqs.append(((context, continuation), context_enc, continuation_enc))\n",
    "\n",
    "    return new_reqs \n",
    "    #self._loglikelihood_tokens(new_reqs)\n",
    "\n",
    "import collections\n",
    "\n",
    "def group(arr, fn):\n",
    "    res = collections.defaultdict(list)\n",
    "\n",
    "    for ob in arr:\n",
    "        res[fn(ob)].append(ob)\n",
    "    \n",
    "    return list(res.values())\n",
    "\n",
    "class Reorderer:\n",
    "    def __init__(self, arr, fn):\n",
    "        self.size = len(arr)\n",
    "        arr = list(enumerate(arr))\n",
    "        arr = group(arr, lambda x: fn(x[1]))\n",
    "        arr = [\n",
    "            ([y[0] for y in x], x[0][1]) for x in arr\n",
    "        ]\n",
    "        arr.sort(key=lambda x: fn(x[1]))\n",
    "\n",
    "        self.arr = arr\n",
    "        \n",
    "    \n",
    "    def get_reordered(self):\n",
    "        return [x[1] for x in self.arr]\n",
    "    \n",
    "    def get_original(self, newarr):\n",
    "        res = [None] * self.size\n",
    "        cov = [False] * self.size\n",
    "\n",
    "        for (inds, _), v in zip(self.arr, newarr):\n",
    "            for ind in inds: \n",
    "                res[ind] = v\n",
    "                cov[ind] = True\n",
    "        \n",
    "        assert all(cov)\n",
    "        \n",
    "        return res\n",
    "\n",
    "def chunks(iter, n):\n",
    "    arr = []\n",
    "    for x in iter:\n",
    "        arr.append(x)\n",
    "        if len(arr) == n:\n",
    "            yield arr\n",
    "            arr = []\n",
    "    \n",
    "    if arr: yield arr\n",
    "\n",
    "def _model_call(inps, model):\n",
    "    \"\"\"\n",
    "    inps: a torch tensor of shape [batch, sequence]\n",
    "    the size of sequence may vary from call to call\n",
    "    returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
    "    logits retuned from the model\n",
    "    \"\"\"\n",
    "    return model(inps)[0][:, :, :50257]\n",
    "\n",
    "def _loglikelihood_tokens(requests, model, max_length, device, disable_tqdm=False, batch_size=1, \n",
    "                          sub_select_idx=None, instruction_len=0, tokenizer=None, debug=False):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        def _collate(x):\n",
    "            # the negative sign on len(toks) sorts descending - this has a few advantages:\n",
    "            # - time estimates will always be over not underestimates, which is more useful for planning\n",
    "            # - to know the size of a batch when going through the list, you know the first one is always the batch padded context length.\n",
    "            #   this is useful to simplify the batching logic and more importantly to make automatic adaptive batches much much easier to implement\n",
    "            # - any OOMs will happen right away rather than near the end\n",
    "\n",
    "            toks = x[1] + x[2]\n",
    "            return (-len(toks), tuple(toks))\n",
    "        \n",
    "        # TODO: automatic (variable) batch size detection for vectorization\n",
    "        reord = Reorderer(requests, _collate)\n",
    "        for chunk in chunks(tqdm(reord.get_reordered(), disable=disable_tqdm), batch_size):\n",
    "            inps = []\n",
    "            contlens = []\n",
    "            inplens = []\n",
    "\n",
    "            padding_length = None\n",
    "\n",
    "            # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "            # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "            # again because vectorizing is annoying\n",
    "\n",
    "            for _, context_enc, continuation_enc in chunk:\n",
    "                # sanity check\n",
    "                assert len(context_enc) > 0\n",
    "                assert len(continuation_enc) > 0\n",
    "                assert len(continuation_enc) <= max_length\n",
    "\n",
    "                # how this all works:\n",
    "                #          CTX      CONT\n",
    "                # inp    0 1 2 3|4 5 6 7 8 9 <- last token is deleted by inp[:, :-1]\n",
    "                # gpt2    \\               \\\n",
    "                # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the [:, -len(continuation_enc):, :self.VOCAB_SIZE] slice\n",
    "                # cont_toks      4 5 6 7 8 9\n",
    "\n",
    "                # Original:\n",
    "                #inp = torch.tensor(\n",
    "                #    (context_enc + continuation_enc)[-(self.max_length+1):][:-1]\n",
    "                #, dtype=torch.long).to(self.device)\n",
    "                \n",
    "                # Modified\n",
    "                # when too long to fit in context, truncate from the left & remove fin token # NM: + After the initial instruction\n",
    "                inp = torch.tensor(\n",
    "                    # Instruction + Text + Continuation\n",
    "                    # Truncation from right: [:(max_length+1-instruction_len)]\n",
    "                    # Truncation from left:  [-(max_length+1-instruction_len):]\n",
    "                    #(context_enc[:instruction_len] + ((context_enc[instruction_len:] + continuation_enc)[:(max_length+1-instruction_len)]))[:-1]\n",
    "                    (context_enc[:instruction_len] + ((context_enc[instruction_len:] + continuation_enc)[-(max_length+1-instruction_len):]))[:-1]\n",
    "                , dtype=torch.long).to(device)\n",
    "                inplen, = inp.shape\n",
    "\n",
    "                cont = continuation_enc\n",
    "\n",
    "                # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "                padding_length = padding_length if padding_length is not None else inplen\n",
    "\n",
    "                # pad to length\n",
    "                inp = torch.cat([\n",
    "                    inp, # [seq]\n",
    "                    torch.zeros(padding_length - inplen, dtype=torch.long).to(inp.device) # [padding_length - seq]\n",
    "                ], dim=0)\n",
    "\n",
    "                if debug:\n",
    "                    print(\"Model Input\")\n",
    "                    print(tokenizer.decode(inp))\n",
    "\n",
    "                inps.append(inp.unsqueeze(0))\n",
    "                contlens.append(cont)\n",
    "                inplens.append(inplen)\n",
    "               \n",
    "            if sub_select_idx:\n",
    "                if debug:\n",
    "                    print(\"Subselecting tokens:\")\n",
    "                    print(tokenizer.decode(sub_select_idx))\n",
    "                # Subselect vocab for softmax by masking out all other vocab\n",
    "                mask = torch.zeros_like(output_logits)\n",
    "                mask[:,:,sub_select_idx] = 1\n",
    "                output_logits = output_logits.masked_fill(mask == 0, float('-inf'))\n",
    "                multi_logits = F.log_softmax(output_logits, dim=-1).cpu()\n",
    "            else:\n",
    "                multi_logits = F.log_softmax(_model_call(torch.cat(inps, dim=0), model), dim=-1).cpu()  # [batch, seq, vocab]\n",
    "\n",
    "            for (cache_key, _, _), logits, inp, inplen, cont_toks in zip(chunk, multi_logits, inps, inplens, contlens):\n",
    "                contlen = len(cont_toks)\n",
    "\n",
    "                logits = logits[inplen-contlen:inplen].unsqueeze(0) # [1, seq, vocab]\n",
    "\n",
    "                greedy_tokens = logits.argmax(dim=-1)\n",
    "\n",
    "                # cont_toks :: [1, seq]\n",
    "                cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(0)\n",
    "                \n",
    "                if debug:\n",
    "                    print(\"Continuation Given\")\n",
    "                    print(tokenizer.batch_decode(cont_toks))\n",
    "                    print(\"Continuation Produced\")\n",
    "                    print(tokenizer.batch_decode(greedy_tokens))\n",
    "\n",
    "                max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "                #last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "\n",
    "                # cont_toks are the vocab indices that make up the perfect continuation\n",
    "                # Hence we gather those vocab indices from the logits, i.e. their probabilities\n",
    "                logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(-1) # [1, seq]\n",
    "\n",
    "                #answer = (float(logits.sum()), bool(max_equal))\n",
    "                # Sum to get a total score of that continuation\n",
    "                res.append(float(logits.sum()))\n",
    "\n",
    "                # partial caching\n",
    "                #if cache_key is not None:\n",
    "                #    self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "                #res.append(answer)\n",
    "\n",
    "    return reord.get_original(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47717442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from beir.reranking import Rerank\n",
    "\n",
    "class GPTRanker:\n",
    "    def __init__(self, model=None, model_path=\"EleutherAI/gpt-neo-1.3B\", use_prompt=True, prompt_doc=\"{}\\n\", prompt_doc_start=\"{}\\n{}\\n\",\n",
    "                 debug=False, fewshots=\"\", **kwargs):\n",
    "        \"\"\"\n",
    "        GPTRanker producing log-probabilities for reranking doc & query with a GPT-like model\n",
    "        Args:\n",
    "            model_path: HuggingFace weight name of a decoder transformer model\n",
    "            use_prompt: Whether to use a prompt\n",
    "            prompt_doc: Prompting scheme to embed document and query \n",
    "                Needs to contain two {} as query is not used for logprobs in this ranker\n",
    "            prompt_doc_start: Prompting scheme specifically used for the first example, e.g. to include description\n",
    "            fewshots: Fewshot example to use [doc, query]\n",
    "            debug: To get information while running\n",
    "        \"\"\"\n",
    "        \n",
    "        #if device is None:\n",
    "        self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        if model is not None:\n",
    "            # Allow specific model loads, e.g. for GPT-J half-precision; Or parallel\n",
    "            self.model = model\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        if self.model.config.model_type == 'gpt2':\n",
    "            self.max_length = self.model.config.n_ctx\n",
    "        elif self.model.config.model_type == 'gptj':\n",
    "            self.max_length = self.model.config.n_ctx\n",
    "        elif self.model.config.model_type == 'gpt_neo':\n",
    "            self.max_length = self.model.config.max_position_embeddings\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model of type {self.model.config.model_type}\")\n",
    "            \n",
    "        # Truncation will be done from the left in the log likelihood\n",
    "        self.prompt_doc = prompt_doc\n",
    "        self.use_prompt = use_prompt\n",
    "        self.instruction_len = len(self.tokenizer.tokenize(self.prompt_doc[:self.prompt_doc.index(\"{\")]))\n",
    "        self.debug = debug\n",
    "    \n",
    "        self.fewshots = fewshots\n",
    "        if self.fewshots:\n",
    "            # doc, query\n",
    "            self.fewshots = prompt_doc_start.format(self.fewshots[0], self.fewshots[1])\n",
    "            # Still take overflowing tokens away from the current doc (not the fewshot doc)\n",
    "            self.instruction_len += len(self.tokenizer.tokenize(self.fewshots))\n",
    "            \n",
    "    def predict(self, sentences: List[Tuple[str,str]], batch_size: int, **kwags) -> List[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          sentences: [query, document]\n",
    "          batch_size: Unused\n",
    "\n",
    "        Returns:\n",
    "          log_probs: float log probability for each query-doc pair\n",
    "        \"\"\"\n",
    "        # TODO: Possibly feed in batched?; Depending on model size?\n",
    "        if self.use_prompt:\n",
    "            # Leave queries as is, as all its tokens will be used to compute the loglikelihoods\n",
    "            sentences = [(query, self.fewshots + self.prompt_doc.format(doc)) for (query, doc) in sentences]\n",
    "\n",
    "        encoded = encode(sentences, self.tokenizer)\n",
    "        # loglikelihood batch_size is not the batch_size fed into this func\n",
    "        log_probs = _loglikelihood_tokens(encoded, self.model, self.max_length, self.device, instruction_len=self.instruction_len, \n",
    "                                          tokenizer=self.tokenizer, debug=self.debug)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cedb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-16 09:43:49 - Loading faiss with AVX2 support.\n",
      "2022-01-16 09:43:49 - Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2022-01-16 09:43:49 - Loading faiss.\n",
      "2022-01-16 09:43:49 - Successfully loaded faiss.\n",
      "2022-01-16 09:45:08 - \n",
      "-------------------- Running prompt quoraD: Question Body: {} Question Title: --------------------\n",
      "\n",
      "2022-01-16 09:45:13 - \n",
      "---------- Running quora ----------\n",
      "\n",
      "2022-01-16 09:45:13 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38136b3ce7504fc8b28cbe51b40d7a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/522931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-16 09:45:15 - Loaded 522931 TEST Documents.\n",
      "2022-01-16 09:45:15 - Doc Example: {'text': 'What is the step by step guide to invest in share market in india?', 'title': ''}\n",
      "2022-01-16 09:45:15 - Loading Queries...\n",
      "2022-01-16 09:45:15 - Loaded 10000 TEST Queries.\n",
      "2022-01-16 09:45:15 - Query Example: Which question should I ask on Quora?\n",
      "2022-01-16 09:45:24 - \n",
      "\n",
      "2022-01-16 09:45:24 - NDCG@1: 0.7230\n",
      "2022-01-16 09:45:24 - NDCG@3: 0.7701\n",
      "2022-01-16 09:45:24 - NDCG@5: 0.7895\n",
      "2022-01-16 09:45:24 - NDCG@10: 0.8077\n",
      "2022-01-16 09:45:24 - NDCG@100: 0.8277\n",
      "2022-01-16 09:45:24 - NDCG@1000: 0.8312\n",
      "2022-01-16 09:45:24 - \n",
      "\n",
      "2022-01-16 09:45:24 - MAP@1: 0.6310\n",
      "2022-01-16 09:45:24 - MAP@3: 0.7294\n",
      "2022-01-16 09:45:24 - MAP@5: 0.7476\n",
      "2022-01-16 09:45:24 - MAP@10: 0.7596\n",
      "2022-01-16 09:45:24 - MAP@100: 0.7669\n",
      "2022-01-16 09:45:24 - MAP@1000: 0.7672\n",
      "2022-01-16 09:45:24 - \n",
      "\n",
      "2022-01-16 09:45:24 - Recall@1: 0.6310\n",
      "2022-01-16 09:45:24 - Recall@3: 0.7969\n",
      "2022-01-16 09:45:24 - Recall@5: 0.8495\n",
      "2022-01-16 09:45:24 - Recall@10: 0.9022\n",
      "2022-01-16 09:45:24 - Recall@100: 0.9770\n",
      "2022-01-16 09:45:24 - Recall@1000: 0.9957\n",
      "2022-01-16 09:45:24 - \n",
      "\n",
      "2022-01-16 09:45:24 - P@1: 0.7230\n",
      "2022-01-16 09:45:24 - P@3: 0.3324\n",
      "2022-01-16 09:45:24 - P@5: 0.2202\n",
      "2022-01-16 09:45:24 - P@10: 0.1217\n",
      "2022-01-16 09:45:24 - P@100: 0.0145\n",
      "2022-01-16 09:45:24 - P@1000: 0.0015\n",
      "2022-01-16 09:45:29 - Starting To Rerank Top-100....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997731/997731 [17:40:34<00:00, 15.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-17 03:28:44 - \n",
      "\n",
      "2022-01-17 03:28:44 - NDCG@1: 0.7343\n",
      "2022-01-17 03:28:44 - NDCG@3: 0.7915\n",
      "2022-01-17 03:28:44 - NDCG@5: 0.8127\n",
      "2022-01-17 03:28:44 - NDCG@10: 0.8297\n",
      "2022-01-17 03:28:44 - NDCG@100: 0.8441\n",
      "2022-01-17 03:28:44 - NDCG@1000: 0.8441\n",
      "2022-01-17 03:28:44 - \n",
      "\n",
      "2022-01-17 03:28:44 - MAP@1: 0.6408\n",
      "2022-01-17 03:28:44 - MAP@3: 0.7494\n",
      "2022-01-17 03:28:44 - MAP@5: 0.7692\n",
      "2022-01-17 03:28:44 - MAP@10: 0.7813\n",
      "2022-01-17 03:28:44 - MAP@100: 0.7879\n",
      "2022-01-17 03:28:44 - MAP@1000: 0.7879\n",
      "2022-01-17 03:28:44 - \n",
      "\n",
      "2022-01-17 03:28:44 - Recall@1: 0.6408\n",
      "2022-01-17 03:28:44 - Recall@3: 0.8229\n",
      "2022-01-17 03:28:44 - Recall@5: 0.8794\n",
      "2022-01-17 03:28:44 - Recall@10: 0.9277\n",
      "2022-01-17 03:28:44 - Recall@100: 0.9772\n",
      "2022-01-17 03:28:44 - Recall@1000: 0.9772\n",
      "2022-01-17 03:28:44 - \n",
      "\n",
      "2022-01-17 03:28:44 - P@1: 0.7343\n",
      "2022-01-17 03:28:44 - P@3: 0.3454\n",
      "2022-01-17 03:28:44 - P@5: 0.2299\n",
      "2022-01-17 03:28:44 - P@10: 0.1270\n",
      "2022-01-17 03:28:44 - P@100: 0.0145\n",
      "2022-01-17 03:28:44 - P@1000: 0.0014\n"
     ]
    }
   ],
   "source": [
    "### Main Loop A: Using fewshot=0, varying prompts with GPT Reranker ###\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "\n",
    "### GPT-Neo ###\n",
    "\n",
    "#model_path = \"EleutherAI/gpt-neo-125M\"\n",
    "#model_out_name = \"gptneo01\" \n",
    "\n",
    "#model_path = \"EleutherAI/gpt-neo-1.3B\"\n",
    "#model_out_name = \"gptneo13\" \n",
    "\n",
    "#model_path = \"EleutherAI/gpt-neo-2.7B\"\n",
    "#model_out_name = \"gptneo27\" \n",
    "\n",
    "### GPT-J ###\n",
    "\n",
    "from transformers import GPTJForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"EleutherAI/gpt-j-6B\"\n",
    "model_out_name = \"gptj\"\n",
    "\n",
    "# Option a) - Half-precision on one GPU (~14GB total GPU Memory needed)\n",
    "#model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "#device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#model = model.to(device)\n",
    "\n",
    "# Option b) - Full-precision on two GPUs (~28GB total GPU Memory needed) [This is used in the experiments]\n",
    "model = GPTJForCausalLM.from_pretrained(model_path)\n",
    "model.parallelize()\n",
    "\n",
    "# Set to True for GPT J ;;; Set to False & comment out the above if using GPT-Neo\n",
    "use_custom_model = True\n",
    "\n",
    "\n",
    "prompts_ablations = {\n",
    "    \"A\": \"{} \",\n",
    "    \"B\": \"{}\\n\",\n",
    "    \"C\": \"Document:\\n{}\\n\\nQuery:\\n\",\n",
    "    \"D\": \"Body:{}\\n\\nTitle:\\n\",\n",
    "    \"E\": \"selected document:\\n{}\\n\\nrelevant query:\\n\",\n",
    "    \"F\": \"The selected text is:\\n{}\\n\\nThe relevant query is:\\n\",\n",
    "    \"G\": 'Documents are searched to find matches with the same content.\\nThe document \"{}\" is a good search result for \"',\n",
    "    \"H\": 'Documents are searched to find matches with the same content.\\nDocument: \"{}\"\\n\\nThe above document is a good match for the query: \"',\n",
    "    \"I\": '# Get matching document and query with the same content\\nget_document()\\n{}\\nget_query_matching_document()\\n\"',\n",
    "    \n",
    "    # Quora ablations (Make sure to set datasets = [\"quora\"] only):\n",
    "    #\"quoraA\": 'Questions are searched to find matches with the same content.\\nThe question \"{}\" is a good search result for \"',\n",
    "    #\"quoraB\": 'Below are two similar questions asking the same thing.\\nThe question \"{}\" is similar to \"',\n",
    "    #\"quoraC\": 'These two questions are the same: 1. {} 2.',\n",
    "    #\"quoraD\": 'Question Body: {} Question Title:',\n",
    "    #\"quoraE\": ('Question Body: {} Question Title: {}\\n', 'Question Body: {} Question Title:'),\n",
    "    \n",
    "    \n",
    "    # Use fewshots=1 scripts\n",
    "    #\"J\": \"Documents are searched to find matches with the same content.\\nDocument:\\n{}\\nQuery:\\n{}\\n\", # \"Document:\\n{}\\nQuery:\\n\",),\n",
    "    #\"K\": \"Document:\\n{}\\nQuery:\\n{}\\n\",# \"Document:\\n{}\\nQuery:\\n\",),\n",
    "    \n",
    "    # Use script using GPTYesRanker\n",
    "    #\"L\": 'An intelligent, helpful bot is given. The bot responds \"Yes\" if the document is a fit to the query and \"No\" otherwise.\\n###\\nDocument: {}\\nQuery: {}\\nBot:',\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "# All datasets\n",
    "datasets = [\"trec-covid\", \"webis-touche2020\", \"nfcorpus\", \"scifact\", \"fiqa\", \"dbpedia-entity\",\n",
    "            \"nq\", \"hotpotqa\", \"quora\", \"fever\", \"climate-fever\", \"arguana\", \"msmarco\", \"scidocs\", \"cqadupstack\",\n",
    "            \"signal1m\", \"trec-news\", \"bioasq\", \"robust04\"]\n",
    "\n",
    "# Main prompt\n",
    "prompts = {\"G\": 'Documents are searched to find matches with the same content.\\nThe document \"{}\" is a good search result for \"',}\n",
    "\n",
    "\n",
    "def clean_titles(corpus):\n",
    "    for k in corpus:\n",
    "        if \"title\" in corpus[k] and corpus[k][\"title\"] is None:\n",
    "            corpus[k][\"title\"] = \"\"\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def run_reranking(results_bm25_path, results_path, data_path, top_k=100, k_values=[1, 3, 5, 10, 100, 1000]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_bm25_path: Path to .json results from bm25 for the dataset\n",
    "        results_path: Path to .json to write rerank results\n",
    "        top_k: How many docs to rerank per query\n",
    "        k_values: For how many docs per query to compute the scores\n",
    "    \"\"\"\n",
    "    \n",
    "    split = \"dev\" if \"msmarco\" in data_path else \"test\"\n",
    "    \n",
    "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=split)\n",
    "    \n",
    "    corpus = clean_titles(corpus) if \"robust04\" in data_path else corpus\n",
    "    \n",
    "    with open(results_bm25_path, 'r') as fp:\n",
    "        results_bm25 = json.load(fp)\n",
    "    \n",
    "    # Optional, make sure results are correct\n",
    "    ndcg_bm25, _map_bm25, recall_bm25, precision_bm25 = EvaluateRetrieval.evaluate(qrels, results_bm25, k_values)\n",
    "\n",
    "    # Rerank top-100 results using the reranker provided\n",
    "    results_rerank = reranker.rerank(corpus, queries, results_bm25, top_k=top_k)\n",
    "    \n",
    "    # Save rerank results\n",
    "    with open(results_path, 'w') as fp:\n",
    "        json.dump(results_rerank, fp)\n",
    "\n",
    "    #### Evaluate retrieval using NDCG@k, MAP@K ...\n",
    "    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, results_rerank, k_values)\n",
    "\n",
    "    return (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision)\n",
    "\n",
    "\n",
    "for prompt_id, prompt_doc in prompts.items():\n",
    "    \n",
    "    scores_out_path = f\"beir_scores_{model_out_name}_{prompt_id}.json\"\n",
    "    \n",
    "    # Optionally skip\n",
    "    #if os.path.exists(os.path.join(os.getcwd(), scores_out_path)):\n",
    "    #    continue\n",
    "\n",
    "    ndcgs_bm25 = {}\n",
    "    ndcgs = {}\n",
    "    \n",
    "    logging.info(f\"\\n{'-' * 20} Running prompt {prompt_id}: {prompt_doc} {'-' * 20}\\n\")\n",
    "    \n",
    "    if use_custom_model:\n",
    "        reranker = Rerank(GPTRanker(model=model, model_path=model_path, use_prompt=True, prompt_doc=prompt_doc), batch_size=128)\n",
    "    else:\n",
    "        reranker = Rerank(GPTRanker(model_path=model_path, use_prompt=True, prompt_doc=prompt_doc, debug=False), batch_size=128)\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "\n",
    "        logging.info(f\"\\n{'-' * 10} Running {dataset} {'-' * 10}\\n\")\n",
    "        \n",
    "        if not(os.path.exists(os.path.join(os.getcwd(), 'datasets', dataset))):\n",
    "            url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "            out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "            data_path = util.download_and_unzip(url, out_dir)\n",
    "            print(\"Dataset downloaded here: {}\".format(data_path))\n",
    "            \n",
    "        # Load the dataset into BEIR\n",
    "        data_path = f\"datasets/{dataset}\"\n",
    "\n",
    "        # cqadupstack - Contains several sub datasets\n",
    "        if dataset == \"cqadupstack\":\n",
    "            cqa_ndcgs_bm25, cqa_maps_bm25, cqa_recalls_bm25, cqa_precisions_bm25 = [], [], [], []\n",
    "            cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "            for sub_dataset in os.listdir(data_path):\n",
    "                sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "                \n",
    "                results_bm25_path = f\"results_{dataset}_{sub_dataset}.json\"\n",
    "                results_path = f\"results_{model_out_name}_prompt{prompt_id}_{dataset}_{sub_dataset}.json\"\n",
    "                # Skip if already computed these results\n",
    "                if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                    continue\n",
    "\n",
    "                (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, sub_data_path)\n",
    "\n",
    "                cqa_ndcgs_bm25.append(ndcg)\n",
    "                cqa_maps_bm25.append(_map)\n",
    "                cqa_recalls_bm25.append(recall)\n",
    "                cqa_precisions_bm25.append(precision)\n",
    "\n",
    "                cqa_ndcgs.append(ndcg)\n",
    "                cqa_maps.append(_map)\n",
    "                cqa_recalls.append(recall)\n",
    "                cqa_precisions.append(precision)\n",
    "\n",
    "            for (metric, metric_group) in [(ndcg_bm25, cqa_ndcgs_bm25), (_map_bm25, cqa_maps_bm25), (recall_bm25, cqa_recalls_bm25), (precision_bm25, cqa_precisions_bm25)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in metric_group]) / len(metric_group)\n",
    "\n",
    "            for (metric, metric_group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in metric_group]) / len(metric_group)\n",
    "\n",
    "            logging.info(\"CQA Final BM25\")\n",
    "            logging.info(f\"{ndcg_bm25}\")\n",
    "            logging.info(f\"{_map_bm25}\")\n",
    "            logging.info(f\"{recall_bm25}\")\n",
    "            logging.info(f\"{precision_bm25}\")\n",
    "\n",
    "            logging.info(\"CQA Final\")\n",
    "            logging.info(f\"{ndcg}\")\n",
    "            logging.info(f\"{_map}\")\n",
    "            logging.info(f\"{recall}\")\n",
    "            logging.info(f\"{precision}\")\n",
    "\n",
    "        else:\n",
    "            results_bm25_path = f\"results_{dataset}.json\"\n",
    "            results_path = f\"results_{model_out_name}_prompt{prompt_id}_{dataset}.json\"\n",
    "            # Skip if already computed these results\n",
    "            if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                continue\n",
    "            (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, data_path)\n",
    "\n",
    "        ndcgs[dataset] = ndcg\n",
    "        ndcgs_bm25[dataset] = ndcg_bm25\n",
    "\n",
    "        # Optionally clean-up each time to avoid running out of space\n",
    "        # !rm -r datasets\n",
    "\n",
    "    with open(scores_out_path, 'w') as fp:\n",
    "        json.dump(ndcgs, fp)\n",
    "        \n",
    "    # Optionally also save the bm25 results tho they should be the same each time    \n",
    "    #with open(f\"./beir_scores_bm25_{prompt_id}.json\", 'w') as fp:\n",
    "    #    json.dump(ndcg_bm25, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fe461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-14 08:41:33 - Loading faiss with AVX2 support.\n",
      "2022-01-14 08:41:33 - Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2022-01-14 08:41:33 - Loading faiss.\n",
      "2022-01-14 08:41:33 - Successfully loaded faiss.\n",
      "2022-01-14 08:41:33 - \n",
      "-------------------- Running prompt quoraE: Question Body: {} Question Title: --------------------\n",
      "\n",
      "2022-01-14 08:41:33 - \n",
      "---------- Running quora ----------\n",
      "\n",
      "2022-01-14 08:41:33 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23031816db754d7bb5d0780a0f337122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/522931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-14 08:41:35 - Loaded 522931 TEST Documents.\n",
      "2022-01-14 08:41:35 - Doc Example: {'text': 'What is the step by step guide to invest in share market in india?', 'title': ''}\n",
      "2022-01-14 08:41:35 - Loading Queries...\n",
      "2022-01-14 08:41:35 - Loaded 10000 TEST Queries.\n",
      "2022-01-14 08:41:35 - Query Example: Which question should I ask on Quora?\n",
      "2022-01-14 08:41:45 - \n",
      "\n",
      "2022-01-14 08:41:45 - NDCG@1: 0.7230\n",
      "2022-01-14 08:41:45 - NDCG@3: 0.7701\n",
      "2022-01-14 08:41:45 - NDCG@5: 0.7895\n",
      "2022-01-14 08:41:45 - NDCG@10: 0.8077\n",
      "2022-01-14 08:41:45 - NDCG@100: 0.8277\n",
      "2022-01-14 08:41:45 - NDCG@1000: 0.8312\n",
      "2022-01-14 08:41:45 - \n",
      "\n",
      "2022-01-14 08:41:45 - MAP@1: 0.6310\n",
      "2022-01-14 08:41:45 - MAP@3: 0.7294\n",
      "2022-01-14 08:41:45 - MAP@5: 0.7476\n",
      "2022-01-14 08:41:45 - MAP@10: 0.7596\n",
      "2022-01-14 08:41:45 - MAP@100: 0.7669\n",
      "2022-01-14 08:41:45 - MAP@1000: 0.7672\n",
      "2022-01-14 08:41:45 - \n",
      "\n",
      "2022-01-14 08:41:45 - Recall@1: 0.6310\n",
      "2022-01-14 08:41:45 - Recall@3: 0.7969\n",
      "2022-01-14 08:41:45 - Recall@5: 0.8495\n",
      "2022-01-14 08:41:45 - Recall@10: 0.9022\n",
      "2022-01-14 08:41:45 - Recall@100: 0.9770\n",
      "2022-01-14 08:41:45 - Recall@1000: 0.9957\n",
      "2022-01-14 08:41:45 - \n",
      "\n",
      "2022-01-14 08:41:45 - P@1: 0.7230\n",
      "2022-01-14 08:41:45 - P@3: 0.3324\n",
      "2022-01-14 08:41:45 - P@5: 0.2202\n",
      "2022-01-14 08:41:45 - P@10: 0.1217\n",
      "2022-01-14 08:41:45 - P@100: 0.0145\n",
      "2022-01-14 08:41:45 - P@1000: 0.0015\n",
      "2022-01-14 08:42:08 - Starting To Rerank Top-100....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997731/997731 [3:11:31<00:00, 86.82it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-14 11:56:55 - \n",
      "\n",
      "2022-01-14 11:56:55 - NDCG@1: 0.6598\n",
      "2022-01-14 11:56:55 - NDCG@3: 0.7257\n",
      "2022-01-14 11:56:55 - NDCG@5: 0.7497\n",
      "2022-01-14 11:56:55 - NDCG@10: 0.7726\n",
      "2022-01-14 11:56:55 - NDCG@100: 0.7959\n",
      "2022-01-14 11:56:55 - NDCG@1000: 0.7959\n",
      "2022-01-14 11:56:55 - \n",
      "\n",
      "2022-01-14 11:56:55 - MAP@1: 0.5794\n",
      "2022-01-14 11:56:55 - MAP@3: 0.6833\n",
      "2022-01-14 11:56:55 - MAP@5: 0.7030\n",
      "2022-01-14 11:56:55 - MAP@10: 0.7171\n",
      "2022-01-14 11:56:55 - MAP@100: 0.7255\n",
      "2022-01-14 11:56:55 - MAP@1000: 0.7255\n",
      "2022-01-14 11:56:55 - \n",
      "\n",
      "2022-01-14 11:56:55 - Recall@1: 0.5794\n",
      "2022-01-14 11:56:55 - Recall@3: 0.7650\n",
      "2022-01-14 11:56:55 - Recall@5: 0.8265\n",
      "2022-01-14 11:56:55 - Recall@10: 0.8915\n",
      "2022-01-14 11:56:55 - Recall@100: 0.9772\n",
      "2022-01-14 11:56:55 - Recall@1000: 0.9772\n",
      "2022-01-14 11:56:55 - \n",
      "\n",
      "2022-01-14 11:56:55 - P@1: 0.6598\n",
      "2022-01-14 11:56:55 - P@3: 0.3166\n",
      "2022-01-14 11:56:55 - P@5: 0.2128\n",
      "2022-01-14 11:56:55 - P@10: 0.1203\n",
      "2022-01-14 11:56:55 - P@100: 0.0145\n",
      "2022-01-14 11:56:55 - P@1000: 0.0014\n"
     ]
    }
   ],
   "source": [
    "### Main Loop B: Using fewshot=1, varying prompts with GPT Reranker ###\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "model = \"EleutherAI/gpt-neo-125M\"\n",
    "model = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = \"EleutherAI/gpt-neo-2.7B\"\n",
    "\n",
    "prompts = {\"J\": (\"Documents are searched to find matches with the same content.\\nDocument:\\n{}\\nQuery:\\n{}\\n\", \"Document:\\n{}\\nQuery:\\n\",),\n",
    "           \"K\": (\"Document:\\n{}\\nQuery:\\n{}\\n\", \"Document:\\n{}\\nQuery:\\n\",),\n",
    "           \n",
    "           # Quora ablations\n",
    "           #\"quoraE\": ('Question Body: {} Question Title: {}\\n', 'Question Body: {} Question Title:'),\n",
    "}\n",
    "\n",
    "\n",
    "MIN_CORP_QUERY_LEN = 0 \n",
    "# For Quora set the below to avoid a short question with no meaning:\n",
    "# MIN_CORP_QUERY_LEN = 10 # Prompt will be Question Body: Why do I have nightmares? Question Title: What causes a nightmare?\n",
    "\n",
    "\n",
    "def get_match_len(qid, tokenizer, corpus, queries, qrels, get_corpus_id=False):\n",
    "    \"\"\"\n",
    "    Get the shortest corpus len given a query\n",
    "    \n",
    "    Args:\n",
    "        qid: id of a query\n",
    "        tokenizer: HF Tokenizer\n",
    "        corpus, queries, qrels: Dataset\n",
    "        get_corpus_id: Whether to return the min len corpus id\n",
    "    \"\"\"\n",
    "    \n",
    "    query_len = len(tokenizer.tokenize(queries[qid]))\n",
    "    \n",
    "    corpora = qrels[qid]\n",
    "    corpora_lens = []\n",
    "    for corpus_id, score in corpora.items():\n",
    "        corpus_len = len(tokenizer.tokenize(corpus[corpus_id][\"text\"]))\n",
    "        # Prefer corpora with high score (won't matter for most cases where all scores are 1)\n",
    "        if (corpus_len + query_len) > MIN_CORP_QUERY_LEN:\n",
    "            corpora_lens.append((corpus_len + query_len) / (score + 1e-10))\n",
    "        else:\n",
    "            corpora_lens.append(float(\"inf\"))\n",
    "    # Optionally get the shortest fitting corpus id\n",
    "    if get_corpus_id: return list(corpora.keys())[np.argmin(corpora_lens)]\n",
    "    return min(corpora_lens)\n",
    "\n",
    "def run_reranking(results_bm25_path, results_path, data_path, top_k=100, k_values=[1, 3, 5, 10, 100, 1000], fewshots=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_bm25_path: Path to .json results from bm25 for the dataset\n",
    "        results_path: Path to .json to write rerank results\n",
    "        top_k: How many docs to rerank per query\n",
    "        k_values: For how many docs per query to compute the scores\n",
    "    \"\"\"\n",
    "    split = \"dev\" if \"msmarco\" in data_path else \"test\"\n",
    "    \n",
    "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=split)\n",
    "    \n",
    "    with open(results_bm25_path, 'r') as fp:\n",
    "        results_bm25 = json.load(fp)\n",
    "    \n",
    "    # Optional, make sure results are correct\n",
    "    ndcg_bm25, _map_bm25, recall_bm25, precision_bm25 = EvaluateRetrieval.evaluate(qrels, results_bm25, k_values)\n",
    "    \n",
    "    if fewshots:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        # Find shortest query\n",
    "        min_q_id = min(qrels, key=lambda x: get_match_len(x, tokenizer, corpus, queries, qrels))\n",
    "        # Find matching shortest corpus\n",
    "        min_corp_id = get_match_len(min_q_id, tokenizer, corpus, queries, qrels, get_corpus_id=True)\n",
    "\n",
    "        query_shot = queries[min_q_id]\n",
    "        corp_shot = corpus[min_corp_id][\"text\"]\n",
    "        \n",
    "        fewshots = [corp_shot, query_shot]\n",
    "        \n",
    "    reranker = Rerank(GPTRanker(model_path=model, use_prompt=True, prompt_doc=prompt_doc,\n",
    "                                fewshots=fewshots, prompt_doc_start=prompt_doc_start, debug=False), batch_size=128)\n",
    "\n",
    "    # Rerank top_k results using the reranker provided\n",
    "    results_rerank = reranker.rerank(corpus, queries, results_bm25, top_k=top_k)\n",
    "    \n",
    "    # Save rerank results\n",
    "    with open(results_path, 'w') as fp:\n",
    "        json.dump(results_rerank, fp)\n",
    "\n",
    "    #### Evaluate retrieval using NDCG@k, MAP@K ...\n",
    "    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, results_rerank, k_values)\n",
    "\n",
    "    return (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision)\n",
    "\n",
    "\n",
    "for prompt_id, (prompt_doc_start, prompt_doc) in prompts.items():\n",
    "    \n",
    "    scores_out_path = f\"beir_scores_gpt_{prompt_id}.json\"\n",
    "    # TODO: RENAME TO f\"beir_gptprompt{prompt_id}_ndcgs.json\"\n",
    "    if os.path.exists(os.path.join(os.getcwd(), scores_out_path)):\n",
    "        continue\n",
    "\n",
    "    ndcgs_bm25 = {}\n",
    "    ndcgs = {}\n",
    "    \n",
    "    logging.info(f\"\\n{'-' * 20} Running prompt {prompt_id}: {prompt_doc} {'-' * 20}\\n\")\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "\n",
    "        logging.info(f\"\\n{'-' * 10} Running {dataset} {'-' * 10}\\n\")\n",
    "        \n",
    "        if not(os.path.exists(os.path.join(os.getcwd(), 'datasets', dataset))):\n",
    "            url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "            out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "            data_path = util.download_and_unzip(url, out_dir)\n",
    "            print(\"Dataset downloaded here: {}\".format(data_path))\n",
    "            \n",
    "        # Load the dataset into BEIR\n",
    "        data_path = f\"datasets/{dataset}\"\n",
    "\n",
    "        # cqadupstack - Contains several sub datasets\n",
    "        if dataset == \"cqadupstack\":\n",
    "            cqa_ndcgs_bm25, cqa_maps_bm25, cqa_recalls_bm25, cqa_precisions_bm25 = [], [], [], []\n",
    "            cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "            for sub_dataset in os.listdir(data_path):\n",
    "                sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "                \n",
    "                results_bm25_path = f\"results_{dataset}_{sub_dataset}.json\"\n",
    "                results_path = f\"results_gpt_prompt{prompt_id}_{dataset}_{sub_dataset}.json\"\n",
    "                # Skip if already computed these results\n",
    "                if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                    continue\n",
    "\n",
    "                (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, sub_data_path)\n",
    "\n",
    "                cqa_ndcgs_bm25.append(ndcg)\n",
    "                cqa_maps_bm25.append(_map)\n",
    "                cqa_recalls_bm25.append(recall)\n",
    "                cqa_precisions_bm25.append(precision)\n",
    "\n",
    "                cqa_ndcgs.append(ndcg)\n",
    "                cqa_maps.append(_map)\n",
    "                cqa_recalls.append(recall)\n",
    "                cqa_precisions.append(precision)\n",
    "\n",
    "            for (metric, group) in [(ndcg_bm25, cqa_ndcgs_bm25), (_map_bm25, cqa_maps_bm25), (recall_bm25, cqa_recalls_bm25), (precision_bm25, cqa_precisions_bm25)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "            for (metric, group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "            logging.info(\"CQA Final BM25\")\n",
    "            logging.info(f\"{ndcg_bm25}\")\n",
    "            logging.info(f\"{_map_bm25}\")\n",
    "            logging.info(f\"{recall_bm25}\")\n",
    "            logging.info(f\"{precision_bm25}\")\n",
    "\n",
    "            logging.info(\"CQA Final\")\n",
    "            logging.info(f\"{ndcg}\")\n",
    "            logging.info(f\"{_map}\")\n",
    "            logging.info(f\"{recall}\")\n",
    "            logging.info(f\"{precision}\")\n",
    "\n",
    "        else:\n",
    "            results_bm25_path = f\"results_{dataset}.json\"\n",
    "            results_path = f\"results_gpt_prompt{prompt_id}_{dataset}.json\"\n",
    "            # Skip if already computed these results\n",
    "            if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                continue\n",
    "            (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, data_path)\n",
    "\n",
    "        ndcgs[dataset] = ndcg\n",
    "        ndcgs_bm25[dataset] = ndcg_bm25\n",
    "\n",
    "        # Optionally clean-up each time to avoid running out of space\n",
    "        # !rm -r datasets\n",
    "\n",
    "    with open(scores_out_path, 'w') as fp:\n",
    "        json.dump(ndcgs, fp)\n",
    "    # Optionally also save the bm25 results tho they should be the same each time    \n",
    "    with open(f\"./beir_scores_bm25_{prompt_id}.json\", 'w') as fp:\n",
    "        json.dump(ndcg_bm25, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59e01db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-17 06:47:22 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc794ca34ef6479ebf9f504ff38d4e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/522931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-17 06:47:24 - Loaded 522931 TEST Documents.\n",
      "2022-01-17 06:47:24 - Doc Example: {'text': 'What is the step by step guide to invest in share market in india?', 'title': ''}\n",
      "2022-01-17 06:47:24 - Loading Queries...\n",
      "2022-01-17 06:47:24 - Loaded 10000 TEST Queries.\n",
      "2022-01-17 06:47:24 - Query Example: Which question should I ask on Quora?\n",
      "2022-01-17 06:47:25 - \n",
      "\n",
      "2022-01-17 06:47:25 - NDCG@1: 0.7343\n",
      "2022-01-17 06:47:25 - NDCG@3: 0.7915\n",
      "2022-01-17 06:47:25 - NDCG@5: 0.8127\n",
      "2022-01-17 06:47:25 - NDCG@10: 0.8297\n",
      "2022-01-17 06:47:25 - NDCG@100: 0.8441\n",
      "2022-01-17 06:47:25 - \n",
      "\n",
      "2022-01-17 06:47:25 - MAP@1: 0.6408\n",
      "2022-01-17 06:47:25 - MAP@3: 0.7494\n",
      "2022-01-17 06:47:25 - MAP@5: 0.7692\n",
      "2022-01-17 06:47:25 - MAP@10: 0.7813\n",
      "2022-01-17 06:47:25 - MAP@100: 0.7879\n",
      "2022-01-17 06:47:25 - \n",
      "\n",
      "2022-01-17 06:47:25 - Recall@1: 0.6408\n",
      "2022-01-17 06:47:25 - Recall@3: 0.8229\n",
      "2022-01-17 06:47:25 - Recall@5: 0.8794\n",
      "2022-01-17 06:47:25 - Recall@10: 0.9277\n",
      "2022-01-17 06:47:25 - Recall@100: 0.9772\n",
      "2022-01-17 06:47:25 - \n",
      "\n",
      "2022-01-17 06:47:25 - P@1: 0.7343\n",
      "2022-01-17 06:47:25 - P@3: 0.3454\n",
      "2022-01-17 06:47:25 - P@5: 0.2299\n",
      "2022-01-17 06:47:25 - P@10: 0.1270\n",
      "2022-01-17 06:47:25 - P@100: 0.0145\n",
      "quora  & 0.82967\n",
      "quoraD: 0.82967\n"
     ]
    }
   ],
   "source": [
    "# Compute scores based on results.json\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "\n",
    "def compute_result(results_path, data_path, top_k=100, k_values=[1, 3, 5, 10, 100]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_path: Path to .json to read rerank results\n",
    "        top_k: How many docs to rerank per query\n",
    "        k_values: For how many docs per query to compute the scores\n",
    "    \"\"\"\n",
    "    split = \"dev\" if \"msmarco\" in data_path else \"test\"\n",
    "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=split)\n",
    "    \n",
    "    with open(results_path, 'r') as fp:\n",
    "        results = json.load(fp)\n",
    "    \n",
    "    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, results, k_values)\n",
    "\n",
    "    return (ndcg, _map, recall, precision)\n",
    "\n",
    "\n",
    "# All datasets\n",
    "datasets = [\"trec-covid\", \"webis-touche2020\", \"nfcorpus\", \"scifact\", \"fiqa\", \"dbpedia-entity\",\n",
    "            \"nq\", \"hotpotqa\", \"quora\", \"fever\", \"climate-fever\", \"arguana\", \"msmarco\", \"scidocs\", \"cqadupstack\",\n",
    "            \"signal1m\", \"trec-news\", \"bioasq\", \"robust04\"]\n",
    "\n",
    "prompts = {\n",
    "    \"G\": 'Documents are searched to find matches with the same content.\\nThe document \"{}\" is a good search result for \"',\n",
    "}\n",
    "\n",
    "\n",
    "latex_help = {}\n",
    "latex_help_avgs = {}\n",
    "results_prefix = \"\"\n",
    "model_name = \"gptj\"\n",
    "# Make empty string if no maxrerank in title (if none in title  = 100)\n",
    "maxrerank = \"\"\n",
    "\n",
    "for prompt_id, prompt_doc in prompts.items():\n",
    "    \n",
    "    scores_out_path = f\"{results_prefix}beir_{model_name}_prompt{prompt_id}{maxrerank}_ndcgs.json\"\n",
    "    ndcgs = {}\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        \n",
    "        data_path = f\"datasets/{dataset}\"\n",
    "\n",
    "        if dataset == \"cqadupstack\":\n",
    "            cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "            for sub_dataset in os.listdir(data_path):\n",
    "                sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "                results_path = f\"{results_prefix}results_{model_name}_prompt{prompt_id}{maxrerank}_{dataset}_{sub_dataset}.json\"\n",
    "                assert os.path.exists(os.path.join(os.getcwd(), results_path)), f\"Missing path: {results_path}\"\n",
    "\n",
    "                ndcg, _map, recall, precision = compute_result(results_path, sub_data_path)\n",
    "                cqa_ndcgs.append(ndcg)\n",
    "                cqa_maps.append(_map)\n",
    "                cqa_recalls.append(recall)\n",
    "                cqa_precisions.append(precision)\n",
    "\n",
    "            for (metric, group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "        else:\n",
    "            results_path = f\"{results_prefix}results_{model_name}_prompt{prompt_id}{maxrerank}_{dataset}.json\"\n",
    "            assert os.path.exists(os.path.join(os.getcwd(), results_path)), f\"Missing path: {results_path}\"\n",
    "            ndcg, _map, recall, precision = compute_result(results_path, data_path)\n",
    "           \n",
    "        latex_help.setdefault(dataset, \"\")\n",
    "        latex_help[dataset] += f\" & {ndcg['NDCG@10']}\"\n",
    "        latex_help_avgs.setdefault(prompt_id, 0)\n",
    "        latex_help_avgs[prompt_id] += ndcg['NDCG@10']\n",
    "\n",
    "        ndcgs[dataset] = ndcg\n",
    "\n",
    "    with open(scores_out_path, 'w') as fp:\n",
    "        json.dump(ndcgs, fp)\n",
    "        \n",
    "for k, v in latex_help.items():\n",
    "    print(f\"{k} {v}\")\n",
    "\n",
    "print(\"& \".join([f\"{k}: {round(v/len(datasets), 5)}\" for k,v in latex_help_avgs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove kaggle.json again\n",
    "!rm /home/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3a8345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# References\n",
    "# ipynb error\n",
    "# conda install -c conda-forge ipywidgets\n",
    "\n",
    "# tensorflow error; metric already exists\n",
    "# downgrade to 2.5.0\n",
    "# ~/miniconda3/envs/semanticsearch/bin/pip install tensorflow==2.5.0\n",
    "\n",
    "# CUDA error: no kernel image is available for execution on the device\n",
    "# Installing the below worked\n",
    "# ~/miniconda3/envs/semanticsearch/bin/pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "\n",
    "# Kill Nvidia processes when out of memory\n",
    "# kill -9 PID\n",
    "\n",
    "# Try to clear cache without killing\n",
    "# del reranker\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Kaggle API\n",
    "# kaggle datasets init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a96dae",
   "metadata": {},
   "source": [
    "##### Reranker module not using query logprobs, but custom output, e.g. \"Yes\" [Prompt Ablation L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd774768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"trec-covid\": [{\"NDCG@1\": 0.88, \"NDCG@3\": 0.82642, \"NDCG@5\": 0.78436, \"NDCG@10\": 0.76104, \"NDCG@100\": 0.50256, \"NDCG@1000\": 0.19689}, {\"MAP@1\": 0.00246, \"MAP@3\": 0.00653, \"MAP@5\": 0.00974, \"MAP@10\": 0.01779, \"MAP@100\": 0.09178, \"MAP@1000\": 0.09178}, {\"Recall@1\": 0.00246, \"Recall@3\": 0.00674, \"Recall@5\": 0.01033, \"Recall@10\": 0.02003, \"Recall@100\": 0.11731, \"Recall@1000\": 0.11731}, {\"P@1\": 0.92, \"P@3\": 0.88, \"P@5\": 0.82, \"P@10\": 0.802, \"P@100\": 0.5084, \"P@1000\": 0.05084}], \"webis-touche2020\": [{\"NDCG@1\": 0.28571, \"NDCG@3\": 0.2358, \"NDCG@5\": 0.23434, \"NDCG@10\": 0.2339, \"NDCG@100\": 0.3984, \"NDCG@1000\": 0.3984}, {\"MAP@1\": 0.0238, \"MAP@3\": 0.04423, \"MAP@5\": 0.05902, \"MAP@10\": 0.08103, \"MAP@100\": 0.15511, \"MAP@1000\": 0.15511}, {\"Recall@1\": 0.0238, \"Recall@3\": 0.05602, \"Recall@5\": 0.08951, \"Recall@10\": 0.15152, \"Recall@100\": 0.56093, \"Recall@1000\": 0.56093}, {\"P@1\": 0.30612, \"P@3\": 0.2381, \"P@5\": 0.24082, \"P@10\": 0.21633, \"P@100\": 0.09531, \"P@1000\": 0.00953}], \"nfcorpus\": [{\"NDCG@1\": 0.41391, \"NDCG@3\": 0.35953, \"NDCG@5\": 0.33014, \"NDCG@10\": 0.30014, \"NDCG@100\": 0.24486, \"NDCG@1000\": 0.23199}, {\"MAP@1\": 0.04024, \"MAP@3\": 0.07252, \"MAP@5\": 0.08338, \"MAP@10\": 0.09726, \"MAP@100\": 0.11941, \"MAP@1000\": 0.11941}, {\"Recall@1\": 0.04024, \"Recall@3\": 0.08157, \"Recall@5\": 0.10041, \"Recall@10\": 0.13031, \"Recall@100\": 0.21671, \"Recall@1000\": 0.21671}, {\"P@1\": 0.43046, \"P@3\": 0.34437, \"P@5\": 0.28874, \"P@10\": 0.22682, \"P@100\": 0.06166, \"P@1000\": 0.00617}], \"scifact\": [{\"NDCG@1\": 0.5, \"NDCG@3\": 0.55644, \"NDCG@5\": 0.58778, \"NDCG@10\": 0.61284, \"NDCG@100\": 0.63115, \"NDCG@1000\": 0.63115}, {\"MAP@1\": 0.48539, \"MAP@3\": 0.53606, \"MAP@5\": 0.55509, \"MAP@10\": 0.56734, \"MAP@100\": 0.57159, \"MAP@1000\": 0.57159}, {\"Recall@1\": 0.48539, \"Recall@3\": 0.59606, \"Recall@5\": 0.67156, \"Recall@10\": 0.74406, \"Recall@100\": 0.82322, \"Recall@1000\": 0.82322}, {\"P@1\": 0.5, \"P@3\": 0.21111, \"P@5\": 0.146, \"P@10\": 0.082, \"P@100\": 0.00917, \"P@1000\": 0.00092}], \"fiqa\": [{\"NDCG@1\": 0.3179, \"NDCG@3\": 0.28995, \"NDCG@5\": 0.30022, \"NDCG@10\": 0.32123, \"NDCG@100\": 0.36665, \"NDCG@1000\": 0.36665}, {\"MAP@1\": 0.16094, \"MAP@3\": 0.22094, \"MAP@5\": 0.2391, \"MAP@10\": 0.25312, \"MAP@100\": 0.26484, \"MAP@1000\": 0.26484}, {\"Recall@1\": 0.16094, \"Recall@3\": 0.26366, \"Recall@5\": 0.31289, \"Recall@10\": 0.37909, \"Recall@100\": 0.54654, \"Recall@1000\": 0.54654}, {\"P@1\": 0.3179, \"P@3\": 0.18724, \"P@5\": 0.1392, \"P@10\": 0.08627, \"P@100\": 0.01302, \"P@1000\": 0.0013}], \"dbpedia-entity\": [{\"NDCG@1\": 0.41375, \"NDCG@3\": 0.3504, \"NDCG@5\": 0.33722, \"NDCG@10\": 0.33272, \"NDCG@100\": 0.37147, \"NDCG@1000\": 0.36891}, {\"MAP@1\": 0.07058, \"MAP@3\": 0.11125, \"MAP@5\": 0.13013, \"MAP@10\": 0.15829, \"MAP@100\": 0.22852, \"MAP@1000\": 0.22852}, {\"Recall@1\": 0.07058, \"Recall@3\": 0.12547, \"Recall@5\": 0.16095, \"Recall@10\": 0.21532, \"Recall@100\": 0.43465, \"Recall@1000\": 0.43465}, {\"P@1\": 0.535, \"P@3\": 0.39167, \"P@5\": 0.3365, \"P@10\": 0.281, \"P@100\": 0.08405, \"P@1000\": 0.0084}]}"
     ]
    }
   ],
   "source": [
    "class GPTYesRanker:\n",
    "    def __init__(self, model_path=\"EleutherAI/gpt-neo-1.3B\", use_prompt=True, prompt_doc=\"{}\\n{}\\n\", prompt_doc_start=\"{}\\n{}\\n\",\n",
    "                 continuation=\"Yes\", sub_select_voc=[\"Yes\", \"No\"], fewshots=\"\", debug=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Variation of GPTRanker only allowing the output of specific vocabulary.\n",
    "        Args:\n",
    "            model_path: HuggingFace weight name of a decoder transformer model\n",
    "            use_prompt: Whether to use a prompt\n",
    "            prompt_doc: Prompting scheme to embed document and query \n",
    "                Needs to contain two {} as query is not used for logprobs in this ranker\n",
    "            prompt_doc_start: Prompting scheme specifically used for the first example, e.g. to include description\n",
    "            continuation: Expected continuation to measure logprobs of\n",
    "            sub_select_voc: Vocabulary to use for measuring probability\n",
    "            fewshots: Fewshot example to use [doc, query]\n",
    "            debug: To get information while running\n",
    "        \"\"\"\n",
    "        \n",
    "        self.device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        if self.model.config.model_type == 'gpt2':\n",
    "            self.max_length = self.model.config.n_ctx\n",
    "        elif self.model.config.model_type == 'gpt_neo':\n",
    "            self.max_length = self.model.config.max_position_embeddings\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model of type {self.model.config.model_type}\")\n",
    "\n",
    "        # Truncation will be done from the left in the log likelihood\n",
    "        self.prompt_doc = prompt_doc\n",
    "        self.use_prompt = use_prompt\n",
    "        self.instruction_len = len(self.tokenizer.tokenize(self.prompt_doc[:self.prompt_doc.index(\"{\")]))\n",
    "        \n",
    "        self.continuation = continuation\n",
    "        \n",
    "        self.fewshots = fewshots\n",
    "        if self.fewshots:\n",
    "            # doc, query\n",
    "            self.fewshots = prompt_doc_start.format(self.fewshots[0], self.fewshots[1]) + self.continuation\n",
    "            # Still take overflowing tokens away from the current doc (not the fewshot doc)\n",
    "            self.instruction_len += len(self.tokenizer.tokenize(self.fewshots))\n",
    "            \n",
    "            \n",
    "        self.debug = debug\n",
    "        self.sub_select_idx = self.tokenizer.encode(sub_select_voc, add_special_tokens=False)\n",
    "    \n",
    "    # Write your own score function, which takes in query-document text pairs and returns the similarity scores\n",
    "    def predict(self, sentences: List[Tuple[str,str]], batch_size: int, **kwags) -> List[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          sentences: [query, document]\n",
    "          batch_size: Unused\n",
    "\n",
    "        Returns:\n",
    "          log_probs: float log probability for each query-doc pair\n",
    "        \"\"\"\n",
    "        # TODO: Possibly feed in batched?; Depending on model size?\n",
    "        if self.use_prompt:\n",
    "            # Leave queries as is, as all its tokens will be used to compute the loglikelihoods\n",
    "            sentences = [(self.continuation, self.fewshots + self.prompt_doc.format(doc, query)) for (query, doc) in sentences]\n",
    "\n",
    "        encoded = encode(sentences, self.tokenizer)\n",
    "        # loglikelihood batch_size is not the batch_size fed into this func\n",
    "        log_probs = _loglikelihood_tokens(encoded, self.model, self.max_length, self.device, instruction_len=self.instruction_len, \n",
    "                                          tokenizer=self.tokenizer, sub_select_idx=self.sub_select_idx,debug=self.debug)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "model = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = \"EleutherAI/gpt-neo-2.7B\"\n",
    "\n",
    "prompts_start = {\"L\": 'An intelligent, helpful bot is given. The bot responds \"Yes\" if the document is a fit to the query and \"No\" otherwise.\\n###\\nDocument: {}\\nQuery: {}\\nBot:',\n",
    "                 \"M\": 'An intelligent, helpful bot is given. The bot responds \"Yes\" if the document is a fit to the query and \"No\" otherwise.\\n###\\nDocument: {}\\nQuery: {}\\nBot: ',\n",
    "}\n",
    "\n",
    "prompts_base = [\"\\nDocument: {}\\nQuery: {}\\nBot:\", \"\\nDocument: {}\\nQuery: {}\\nBot: \"]\n",
    "\n",
    "continuations = [\" Yes\", \"Yes\"]\n",
    "sub_select_vocs = [[\" Yes\", \" No\"], [\"Yes\", \"No\"]]\n",
    "  \n",
    "\n",
    "def get_match_len(qid, tokenizer, corpus, queries, qrels, get_corpus_id=False):    \n",
    "    query_len = len(tokenizer.tokenize(queries[qid]))\n",
    "    \n",
    "    corpora = qrels[qid]\n",
    "    corpora_lens = []\n",
    "    for corpus_id, score in corpora.items():\n",
    "        corpus_len = len(tokenizer.tokenize(corpus[corpus_id][\"text\"]))\n",
    "        # Prefer corpora with high score (won't matter for most cases where all scores are 1)\n",
    "        corpora_lens.append((corpus_len + query_len) / (score + 1e-10))\n",
    "    # Optionally get the shortest fitting corpus\n",
    "    if get_corpus_id: return list(corpora.keys())[np.argmin(corpora_lens)]\n",
    "    return min(corpora_lens)\n",
    "\n",
    "\n",
    "def run_reranking(results_bm25_path, results_path, data_path, top_k=100, k_values=[1, 3, 5, 10, 100, 1000], fewshots=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_bm25_path: Path to .json results from bm25 for the dataset\n",
    "        results_path: Path to .json to write rerank results\n",
    "        top_k: How many docs to rerank per query\n",
    "        k_values: For how many docs per query to compute the scores\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
    "    \n",
    "    with open(results_bm25_path, 'r') as fp:\n",
    "        results_bm25 = json.load(fp)\n",
    "    \n",
    "    # Optional, make sure results are correct\n",
    "    ndcg_bm25, _map_bm25, recall_bm25, precision_bm25 = EvaluateRetrieval.evaluate(qrels, results_bm25, k_values)\n",
    "    \n",
    "    \n",
    "    if fewshots:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        min_q_id = min(qrels, key=lambda x: get_match_len(x, tokenizer, corpus, queries, qrels))\n",
    "        min_corp_id = get_match_len(min_q_id, tokenizer, corpus, queries, qrels, get_corpus_id=True)\n",
    "\n",
    "        query_shot = queries[min_q_id]\n",
    "        corp_shot = corpus[min_corp_id][\"text\"]\n",
    "        \n",
    "        fewshots = [corp_shot, query_shot]\n",
    "        \n",
    "    reranker = Rerank(GPTYesRanker(model_path=model, use_prompt=True, prompt_doc=prompt_base, \n",
    "                                   sub_select_voc=sub_select_voc, continuation=continuation, \n",
    "                                   fewshots=fewshots, prompt_doc_start=prompt_start, debug=False), batch_size=128)\n",
    "    \n",
    "\n",
    "    # Rerank top-100 results using the reranker provided\n",
    "    results_rerank = reranker.rerank(corpus, queries, results_bm25, top_k=top_k)\n",
    "    \n",
    "    # Save rerank results\n",
    "    with open(results_path, 'w') as fp:\n",
    "        json.dump(results_rerank, fp)\n",
    "\n",
    "    #### Evaluate retrieval using NDCG@k, MAP@K ...\n",
    "    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, results_rerank, k_values)\n",
    "\n",
    "    return (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision)\n",
    "\n",
    "\n",
    "for (prompt_id, prompt_start), continuation, sub_select_voc, prompt_base in zip(prompts_start.items(), continuations, sub_select_vocs, prompts_base):\n",
    "    \n",
    "    scores_out_path = f\"beir_scores_gpt_{prompt_id}.json\"\n",
    "    if os.path.exists(os.path.join(os.getcwd(), scores_out_path)):\n",
    "        continue\n",
    "\n",
    "    ndcgs_bm25 = {}\n",
    "    ndcgs = {}\n",
    "    \n",
    "    logging.info(f\"\\n{'-' * 20} Running prompt {prompt_id}: {prompt_start} {'-' * 20}\\n\")\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "\n",
    "        logging.info(f\"\\n{'-' * 10} Running {dataset} {'-' * 10}\\n\")\n",
    "        \n",
    "        if not(os.path.exists(os.path.join(os.getcwd(), 'datasets', dataset))):\n",
    "            url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "            out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "            data_path = util.download_and_unzip(url, out_dir)\n",
    "            print(\"Dataset downloaded here: {}\".format(data_path))\n",
    "            \n",
    "        # Load the dataset into BEIR\n",
    "        data_path = f\"datasets/{dataset}\"\n",
    "\n",
    "        # cqadupstack - Contains several sub datasets\n",
    "        if dataset == \"cqadupstack\":\n",
    "            cqa_ndcgs_bm25, cqa_maps_bm25, cqa_recalls_bm25, cqa_precisions_bm25 = [], [], [], []\n",
    "            cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "            for sub_dataset in os.listdir(data_path):\n",
    "                sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "                \n",
    "                results_bm25_path = f\"results_{dataset}_{sub_dataset}.json\"\n",
    "                results_path = f\"results_gpt_prompt{prompt_id}_{dataset}_{sub_dataset}.json\"\n",
    "                # Skip if already computed these results\n",
    "                if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                    continue\n",
    "\n",
    "                (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, sub_data_path)\n",
    "\n",
    "                cqa_ndcgs_bm25.append(ndcg)\n",
    "                cqa_maps_bm25.append(_map)\n",
    "                cqa_recalls_bm25.append(recall)\n",
    "                cqa_precisions_bm25.append(precision)\n",
    "\n",
    "                cqa_ndcgs.append(ndcg)\n",
    "                cqa_maps.append(_map)\n",
    "                cqa_recalls.append(recall)\n",
    "                cqa_precisions.append(precision)\n",
    "\n",
    "            for (metric, group) in [(ndcg_bm25, cqa_ndcgs_bm25), (_map_bm25, cqa_maps_bm25), (recall_bm25, cqa_recalls_bm25), (precision_bm25, cqa_precisions_bm25)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "            for (metric, group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "            logging.info(\"CQA Final BM25\")\n",
    "            logging.info(f\"{ndcg_bm25}\")\n",
    "            logging.info(f\"{_map_bm25}\")\n",
    "            logging.info(f\"{recall_bm25}\")\n",
    "            logging.info(f\"{precision_bm25}\")\n",
    "\n",
    "            logging.info(\"CQA Final\")\n",
    "            logging.info(f\"{ndcg}\")\n",
    "            logging.info(f\"{_map}\")\n",
    "            logging.info(f\"{recall}\")\n",
    "            logging.info(f\"{precision}\")\n",
    "\n",
    "        else:\n",
    "            results_bm25_path = f\"results_{dataset}.json\"\n",
    "            results_path = f\"results_gpt_prompt{prompt_id}_{dataset}.json\"\n",
    "            # Skip if already computed these results\n",
    "            if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                continue\n",
    "            (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, data_path)\n",
    "\n",
    "        ndcgs[dataset] = ndcg\n",
    "        ndcgs_bm25[dataset] = ndcg_bm25\n",
    "\n",
    "        # Optionally clean-up each time to avoid running out of space\n",
    "        # !rm -r datasets\n",
    "\n",
    "    with open(scores_out_path, 'w') as fp:\n",
    "        json.dump(ndcgs, fp)\n",
    "    # Optionally also save the bm25 results tho they should be the same each time    \n",
    "    with open(f\"./beir_scores_bm25_{prompt_id}.json\", 'w') as fp:\n",
    "        json.dump(ndcg_bm25, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c3670",
   "metadata": {},
   "source": [
    "##### Computing max_rerank=10 based on max_rerank=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8464eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:07:19 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff62313c5d94cf79b750baa24d60307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:07:20 - Loaded 171332 TEST Documents.\n",
      "2022-01-07 18:07:20 - Doc Example: {'text': 'OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'title': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia'}\n",
      "2022-01-07 18:07:20 - Loading Queries...\n",
      "2022-01-07 18:07:21 - Loaded 50 TEST Queries.\n",
      "2022-01-07 18:07:21 - Query Example: what is the origin of COVID-19\n",
      "2022-01-07 18:07:21 - \n",
      "\n",
      "2022-01-07 18:07:21 - NDCG@1: 0.7500\n",
      "2022-01-07 18:07:21 - NDCG@10: 0.6947\n",
      "2022-01-07 18:07:21 - \n",
      "\n",
      "2022-01-07 18:07:21 - MAP@1: 0.0023\n",
      "2022-01-07 18:07:21 - MAP@10: 0.0177\n",
      "2022-01-07 18:07:21 - \n",
      "\n",
      "2022-01-07 18:07:21 - Recall@1: 0.0023\n",
      "2022-01-07 18:07:21 - Recall@10: 0.0191\n",
      "2022-01-07 18:07:21 - \n",
      "\n",
      "2022-01-07 18:07:21 - P@1: 0.8400\n",
      "2022-01-07 18:07:21 - P@10: 0.7340\n",
      "2022-01-07 18:07:21 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c580b2be8d4ea4b33aaaf58828b676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:07:25 - Loaded 382545 TEST Documents.\n",
      "2022-01-07 18:07:25 - Doc Example: {'text': 'My opponent forfeited every round. None of my arguments were answered. I don’t like the idea of winning by default, but here we are.Tule: it’s good for students to get involved and address big issues like teen pregnancy. You need to be able to answer arguments like mine and not simply prepare for an abstinence-only type of response. You should also be aware that, in the U.S., condoms may be sold to minors in ANY state. A retailer who says it is illegal to sell you them is, frankly, wrong.', 'title': 'Contraceptive Forms for High School Students'}\n",
      "2022-01-07 18:07:25 - Loading Queries...\n",
      "2022-01-07 18:07:25 - Loaded 49 TEST Queries.\n",
      "2022-01-07 18:07:25 - Query Example: Should teachers get tenure?\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - NDCG@1: 0.3469\n",
      "2022-01-07 18:07:25 - NDCG@10: 0.3396\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - MAP@1: 0.0272\n",
      "2022-01-07 18:07:25 - MAP@10: 0.1295\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - Recall@1: 0.0272\n",
      "2022-01-07 18:07:25 - Recall@10: 0.2122\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - P@1: 0.3674\n",
      "2022-01-07 18:07:25 - P@10: 0.3306\n",
      "2022-01-07 18:07:25 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce9654827224ca0905094c92da3e080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:07:25 - Loaded 3633 TEST Documents.\n",
      "2022-01-07 18:07:25 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
      "2022-01-07 18:07:25 - Loading Queries...\n",
      "2022-01-07 18:07:25 - Loaded 323 TEST Queries.\n",
      "2022-01-07 18:07:25 - Query Example: Do Cholesterol Statin Drugs Cause Breast Cancer?\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - NDCG@1: 0.4470\n",
      "2022-01-07 18:07:25 - NDCG@10: 0.3135\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - MAP@1: 0.0457\n",
      "2022-01-07 18:07:25 - MAP@10: 0.1066\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - Recall@1: 0.0457\n",
      "2022-01-07 18:07:25 - Recall@10: 0.1350\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - P@1: 0.4669\n",
      "2022-01-07 18:07:25 - P@10: 0.2262\n",
      "2022-01-07 18:07:25 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996bb1894b7d432f9b2902d01e5f642d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:07:25 - Loaded 5183 TEST Documents.\n",
      "2022-01-07 18:07:25 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2022-01-07 18:07:25 - Loading Queries...\n",
      "2022-01-07 18:07:25 - Loaded 300 TEST Queries.\n",
      "2022-01-07 18:07:25 - Query Example: 0-dimensional biomaterials show inductive properties.\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - NDCG@1: 0.5167\n",
      "2022-01-07 18:07:25 - NDCG@10: 0.6259\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - MAP@1: 0.5046\n",
      "2022-01-07 18:07:25 - MAP@10: 0.5889\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - Recall@1: 0.5046\n",
      "2022-01-07 18:07:25 - Recall@10: 0.7249\n",
      "2022-01-07 18:07:25 - \n",
      "\n",
      "2022-01-07 18:07:25 - P@1: 0.5167\n",
      "2022-01-07 18:07:25 - P@10: 0.0797\n",
      "2022-01-07 18:07:25 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbea291ca6e454f92edc5a0acb34f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:07:26 - Loaded 57638 TEST Documents.\n",
      "2022-01-07 18:07:26 - Doc Example: {'text': \"I'm not saying I don't like the idea of on-the-job training too, but you can't expect the company to do that. Training workers is not their job - they're building software. Perhaps educational systems in the U.S. (or their students) should worry a little about getting marketable skills in exchange for their massive investment in education, rather than getting out with thousands in student debt and then complaining that they aren't qualified to do anything.\", 'title': ''}\n",
      "2022-01-07 18:07:26 - Loading Queries...\n",
      "2022-01-07 18:07:26 - Loaded 648 TEST Queries.\n",
      "2022-01-07 18:07:26 - Query Example: How to deposit a cheque issued to an associate in my business into my business account?\n",
      "2022-01-07 18:07:26 - \n",
      "\n",
      "2022-01-07 18:07:26 - NDCG@1: 0.2870\n",
      "2022-01-07 18:07:26 - NDCG@10: 0.2804\n",
      "2022-01-07 18:07:26 - \n",
      "\n",
      "2022-01-07 18:07:26 - MAP@1: 0.1472\n",
      "2022-01-07 18:07:26 - MAP@10: 0.2227\n",
      "2022-01-07 18:07:26 - \n",
      "\n",
      "2022-01-07 18:07:26 - Recall@1: 0.1472\n",
      "2022-01-07 18:07:26 - Recall@10: 0.3244\n",
      "2022-01-07 18:07:26 - \n",
      "\n",
      "2022-01-07 18:07:26 - P@1: 0.2870\n",
      "2022-01-07 18:07:26 - P@10: 0.0702\n",
      "2022-01-07 18:07:26 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3a8fc6193d4f3fbabd8c66300b05ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4635922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:07:52 - Loaded 4635922 TEST Documents.\n",
      "2022-01-07 18:07:53 - Doc Example: {'text': \"Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold.   A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.\", 'title': 'Animalia (book)'}\n",
      "2022-01-07 18:07:53 - Loading Queries...\n",
      "2022-01-07 18:07:53 - Loaded 400 TEST Queries.\n",
      "2022-01-07 18:07:53 - Query Example: Szechwan dish food cuisine\n",
      "2022-01-07 18:07:53 - \n",
      "\n",
      "2022-01-07 18:07:53 - NDCG@1: 0.4163\n",
      "2022-01-07 18:07:53 - NDCG@10: 0.3275\n",
      "2022-01-07 18:07:53 - \n",
      "\n",
      "2022-01-07 18:07:53 - MAP@1: 0.0657\n",
      "2022-01-07 18:07:53 - MAP@10: 0.1568\n",
      "2022-01-07 18:07:53 - \n",
      "\n",
      "2022-01-07 18:07:53 - Recall@1: 0.0657\n",
      "2022-01-07 18:07:53 - Recall@10: 0.2089\n",
      "2022-01-07 18:07:53 - \n",
      "\n",
      "2022-01-07 18:07:53 - P@1: 0.5375\n",
      "2022-01-07 18:07:53 - P@10: 0.2747\n",
      "2022-01-07 18:07:53 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f478708ad8e84a0b9a70c6146ca93588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2681468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:08:08 - Loaded 2681468 TEST Documents.\n",
      "2022-01-07 18:08:08 - Doc Example: {'text': \"In accounting, minority interest (or non-controlling interest) is the portion of a subsidiary corporation's stock that is not owned by the parent corporation. The magnitude of the minority interest in the subsidiary company is generally less than 50% of outstanding shares, or the corporation would generally cease to be a subsidiary of the parent.[1]\", 'title': 'Minority interest'}\n",
      "2022-01-07 18:08:08 - Loading Queries...\n",
      "2022-01-07 18:08:08 - Loaded 3452 TEST Queries.\n",
      "2022-01-07 18:08:08 - Query Example: what is non controlling interest on balance sheet\n",
      "2022-01-07 18:08:12 - \n",
      "\n",
      "2022-01-07 18:08:12 - NDCG@1: 0.1860\n",
      "2022-01-07 18:08:12 - NDCG@10: 0.3361\n",
      "2022-01-07 18:08:12 - \n",
      "\n",
      "2022-01-07 18:08:12 - MAP@1: 0.1660\n",
      "2022-01-07 18:08:12 - MAP@10: 0.2742\n",
      "2022-01-07 18:08:12 - \n",
      "\n",
      "2022-01-07 18:08:12 - Recall@1: 0.1660\n",
      "2022-01-07 18:08:12 - Recall@10: 0.5067\n",
      "2022-01-07 18:08:12 - \n",
      "\n",
      "2022-01-07 18:08:12 - P@1: 0.1860\n",
      "2022-01-07 18:08:12 - P@10: 0.0594\n",
      "2022-01-07 18:08:12 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e09037d805467db246f2086b507f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5233329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:08:41 - Loaded 5233329 TEST Documents.\n",
      "2022-01-07 18:08:41 - Doc Example: {'text': 'Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions. These are often described as stateless societies, although several authors have defined them more specifically as institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary and harmful.', 'title': 'Anarchism'}\n",
      "2022-01-07 18:08:41 - Loading Queries...\n",
      "2022-01-07 18:08:41 - Loaded 7405 TEST Queries.\n",
      "2022-01-07 18:08:41 - Query Example: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "2022-01-07 18:08:48 - \n",
      "\n",
      "2022-01-07 18:08:48 - NDCG@1: 0.7772\n",
      "2022-01-07 18:08:48 - NDCG@10: 0.6327\n",
      "2022-01-07 18:08:48 - \n",
      "\n",
      "2022-01-07 18:08:48 - MAP@1: 0.3886\n",
      "2022-01-07 18:08:48 - MAP@10: 0.5462\n",
      "2022-01-07 18:08:48 - \n",
      "\n",
      "2022-01-07 18:08:48 - Recall@1: 0.3886\n",
      "2022-01-07 18:08:48 - Recall@10: 0.6295\n",
      "2022-01-07 18:08:48 - \n",
      "\n",
      "2022-01-07 18:08:48 - P@1: 0.7772\n",
      "2022-01-07 18:08:48 - P@10: 0.1259\n",
      "2022-01-07 18:08:48 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d2b3e8653411cbdfb29c1d2085624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/522931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:08:50 - Loaded 522931 TEST Documents.\n",
      "2022-01-07 18:08:50 - Doc Example: {'text': 'What is the step by step guide to invest in share market in india?', 'title': ''}\n",
      "2022-01-07 18:08:50 - Loading Queries...\n",
      "2022-01-07 18:08:50 - Loaded 10000 TEST Queries.\n",
      "2022-01-07 18:08:50 - Query Example: Which question should I ask on Quora?\n",
      "2022-01-07 18:08:59 - \n",
      "\n",
      "2022-01-07 18:08:59 - NDCG@1: 0.6877\n",
      "2022-01-07 18:08:59 - NDCG@10: 0.7942\n",
      "2022-01-07 18:08:59 - \n",
      "\n",
      "2022-01-07 18:08:59 - MAP@1: 0.6002\n",
      "2022-01-07 18:08:59 - MAP@10: 0.7415\n",
      "2022-01-07 18:08:59 - \n",
      "\n",
      "2022-01-07 18:08:59 - Recall@1: 0.6002\n",
      "2022-01-07 18:08:59 - Recall@10: 0.9026\n",
      "2022-01-07 18:08:59 - \n",
      "\n",
      "2022-01-07 18:08:59 - P@1: 0.6877\n",
      "2022-01-07 18:08:59 - P@10: 0.1220\n",
      "2022-01-07 18:08:59 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d826e8443d44d41b48dcd609d091205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:09:31 - Loaded 5416568 TEST Documents.\n",
      "2022-01-07 18:09:31 - Doc Example: {'text': 'The following are the football ( soccer ) events of the year 1928 throughout the world .', 'title': '1928 in association football'}\n",
      "2022-01-07 18:09:31 - Loading Queries...\n",
      "2022-01-07 18:09:32 - Loaded 6666 TEST Queries.\n",
      "2022-01-07 18:09:32 - Query Example: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n",
      "2022-01-07 18:09:38 - \n",
      "\n",
      "2022-01-07 18:09:38 - NDCG@1: 0.6589\n",
      "2022-01-07 18:09:38 - NDCG@10: 0.7348\n",
      "2022-01-07 18:09:38 - \n",
      "\n",
      "2022-01-07 18:09:38 - MAP@1: 0.6178\n",
      "2022-01-07 18:09:38 - MAP@10: 0.6961\n",
      "2022-01-07 18:09:38 - \n",
      "\n",
      "2022-01-07 18:09:38 - Recall@1: 0.6178\n",
      "2022-01-07 18:09:38 - Recall@10: 0.8141\n",
      "2022-01-07 18:09:38 - \n",
      "\n",
      "2022-01-07 18:09:38 - P@1: 0.6589\n",
      "2022-01-07 18:09:38 - P@10: 0.0888\n",
      "2022-01-07 18:09:38 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cfe5a0f8b44b60bc0f0cced023f4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:10:10 - Loaded 5416593 TEST Documents.\n",
      "2022-01-07 18:10:10 - Doc Example: {'text': 'The following are the football ( soccer ) events of the year 1928 throughout the world .', 'title': '1928 in association football'}\n",
      "2022-01-07 18:10:10 - Loading Queries...\n",
      "2022-01-07 18:10:10 - Loaded 1535 TEST Queries.\n",
      "2022-01-07 18:10:10 - Query Example: Global warming is driving polar bears toward extinction\n",
      "2022-01-07 18:10:13 - \n",
      "\n",
      "2022-01-07 18:10:13 - NDCG@1: 0.1785\n",
      "2022-01-07 18:10:13 - NDCG@10: 0.1942\n",
      "2022-01-07 18:10:13 - \n",
      "\n",
      "2022-01-07 18:10:13 - MAP@1: 0.0803\n",
      "2022-01-07 18:10:13 - MAP@10: 0.1367\n",
      "2022-01-07 18:10:13 - \n",
      "\n",
      "2022-01-07 18:10:13 - Recall@1: 0.0803\n",
      "2022-01-07 18:10:13 - Recall@10: 0.2318\n",
      "2022-01-07 18:10:13 - \n",
      "\n",
      "2022-01-07 18:10:13 - P@1: 0.1785\n",
      "2022-01-07 18:10:13 - P@10: 0.0575\n",
      "2022-01-07 18:10:13 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c9a177c86b49e4bbb8c8fecaa4a673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:10:13 - Loaded 8674 TEST Documents.\n",
      "2022-01-07 18:10:13 - Doc Example: {'text': \"You don’t have to be vegetarian to be green. Many special environments have been created by livestock farming – for example chalk down land in England and mountain pastures in many countries. Ending livestock farming would see these areas go back to woodland with a loss of many unique plants and animals. Growing crops can also be very bad for the planet, with fertilisers and pesticides polluting rivers, lakes and seas. Most tropical forests are now cut down for timber, or to allow oil palm trees to be grown in plantations, not to create space for meat production.  British farmer and former editor Simon Farrell also states: “Many vegans and vegetarians rely on one source from the U.N. calculation that livestock generates 18% of global carbon emissions, but this figure contains basic mistakes. It attributes all deforestation from ranching to cattle, rather than logging or development. It also muddles up one-off emissions from deforestation with on-going pollution.”  He also refutes the statement of meat production inefficiency: “Scientists have calculated that globally the ratio between the amounts of useful plant food used to produce meat is about 5 to 1. If you feed animals only food that humans can eat — which is, indeed, largely the case in the Western world — that may be true. But animals also eat food we can't eat, such as grass. So the real conversion figure is 1.4 to 1.” [1] At the same time eating a vegetarian diet may be no more environmentally friendly than a meat based diet if it is not sustainably sourced or uses perishable fruit and vegetables that are flown in from around the world. Eating locally sourced food can has as big an impact as being vegetarian. [2]  [1] Tara Kelly, Simon Fairlie: How Eating Meat Can Save the World, 12 October 2010  [2] Lucy Siegle, ‘It is time to become a vegetarian?’ The Observer, 18th May 2008\", 'title': 'animals environment general health health general weight philosophy ethics'}\n",
      "2022-01-07 18:10:13 - Loading Queries...\n",
      "2022-01-07 18:10:13 - Loaded 1406 TEST Queries.\n",
      "2022-01-07 18:10:13 - Query Example: Being vegetarian helps the environment  Becoming a vegetarian is an environmentally friendly thing to do. Modern farming is one of the main sources of pollution in our rivers. Beef farming is one of the main causes of deforestation, and as long as people continue to buy fast food in their billions, there will be a financial incentive to continue cutting down trees to make room for cattle. Because of our desire to eat fish, our rivers and seas are being emptied of fish and many species are facing extinction. Energy resources are used up much more greedily by meat farming than my farming cereals, pulses etc. Eating meat and fish not only causes cruelty to animals, it causes serious harm to the environment and to biodiversity. For example consider Meat production related pollution and deforestation  At Toronto’s 1992 Royal Agricultural Winter Fair, Agriculture Canada displayed two contrasting statistics: “it takes four football fields of land (about 1.6 hectares) to feed each Canadian” and “one apple tree produces enough fruit to make 320 pies.” Think about it — a couple of apple trees and a few rows of wheat on a mere fraction of a hectare could produce enough food for one person! [1]  The 2006 U.N. Food and Agriculture Organization (FAO) report concluded that worldwide livestock farming generates 18% of the planet's greenhouse gas emissions — by comparison, all the world's cars, trains, planes and boats account for a combined 13% of greenhouse gas emissions. [2]  As a result of the above point producing meat damages the environment. The demand for meat drives deforestation. Daniel Cesar Avelino of Brazil's Federal Public Prosecution Office says “We know that the single biggest driver of deforestation in the Amazon is cattle.” This clearing of tropical rainforests such as the Amazon for agriculture is estimated to produce 17% of the world's greenhouse gas emissions. [3] Not only this but the production of meat takes a lot more energy than it ultimately gives us chicken meat production consumes energy in a 4:1 ratio to protein output; beef cattle production requires an energy input to protein output ratio of 54:1.  The same is true with water use due to the same phenomenon of meat being inefficient to produce in terms of the amount of grain needed to produce the same weight of meat, production requires a lot of water. Water is another scarce resource that we will soon not have enough of in various areas of the globe. Grain-fed beef production takes 100,000 liters of water for every kilogram of food. Raising broiler chickens takes 3,500 liters of water to make a kilogram of meat. In comparison, soybean production uses 2,000 liters for kilogram of food produced; rice, 1,912; wheat, 900; and potatoes, 500 liters. [4] This is while there are areas of the globe that have severe water shortages. With farming using up to 70 times more water than is used for domestic purposes: cooking and washing. A third of the population of the world is already suffering from a shortage of water. [5] Groundwater levels are falling all over the world and rivers are beginning to dry up. Already some of the biggest rivers such as China’s Yellow river do not reach the sea. [6]  With a rising population becoming vegetarian is the only responsible way to eat.  [1] Stephen Leckie, ‘How Meat-centred Eating Patterns Affect Food Security and the Environment’, International development research center  [2] Bryan Walsh, Meat: Making Global Warming Worse, Time magazine, 10 September 2008 .  [3] David Adam, Supermarket suppliers ‘helping to destroy Amazon rainforest’, The Guardian, 21st June 2009.  [4] Roger Segelken, U.S. could feed 800 million people with grain that livestock eat, Cornell Science News, 7th August 1997.  [5] Fiona Harvey, Water scarcity affects one in three, FT.com, 21st August 2003  [6] Rupert Wingfield-Hayes, Yellow river ‘drying up’, BBC News, 29th July 2004\n",
      "2022-01-07 18:10:15 - \n",
      "\n",
      "2022-01-07 18:10:15 - NDCG@1: 0.1337\n",
      "2022-01-07 18:10:15 - NDCG@10: 0.3935\n",
      "2022-01-07 18:10:15 - \n",
      "\n",
      "2022-01-07 18:10:15 - MAP@1: 0.1337\n",
      "2022-01-07 18:10:15 - MAP@10: 0.2848\n",
      "2022-01-07 18:10:15 - \n",
      "\n",
      "2022-01-07 18:10:15 - Recall@1: 0.1337\n",
      "2022-01-07 18:10:15 - Recall@10: 0.7539\n",
      "2022-01-07 18:10:15 - \n",
      "\n",
      "2022-01-07 18:10:15 - P@1: 0.1337\n",
      "2022-01-07 18:10:15 - P@10: 0.0754\n",
      "2022-01-07 18:10:15 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532d7dd7f04a4fef9096ff2afe1c7443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8841823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:10:58 - Loaded 8841823 DEV Documents.\n",
      "2022-01-07 18:10:59 - Doc Example: {'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.', 'title': ''}\n",
      "2022-01-07 18:10:59 - Loading Queries...\n",
      "2022-01-07 18:11:00 - Loaded 6980 DEV Queries.\n",
      "2022-01-07 18:11:00 - Query Example: how many years did william bradford serve as governor of plymouth colony?\n",
      "2022-01-07 18:11:05 - \n",
      "\n",
      "2022-01-07 18:11:05 - NDCG@1: 0.1125\n",
      "2022-01-07 18:11:05 - NDCG@10: 0.2370\n",
      "2022-01-07 18:11:05 - \n",
      "\n",
      "2022-01-07 18:11:05 - MAP@1: 0.1097\n",
      "2022-01-07 18:11:05 - MAP@10: 0.1902\n",
      "2022-01-07 18:11:05 - \n",
      "\n",
      "2022-01-07 18:11:05 - Recall@1: 0.1097\n",
      "2022-01-07 18:11:05 - Recall@10: 0.3817\n",
      "2022-01-07 18:11:05 - \n",
      "\n",
      "2022-01-07 18:11:05 - P@1: 0.1125\n",
      "2022-01-07 18:11:05 - P@10: 0.0397\n",
      "2022-01-07 18:11:05 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844f07a13ebd476aab3b656d86f477f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:06 - Loaded 25657 TEST Documents.\n",
      "2022-01-07 18:11:06 - Doc Example: {'text': 'An evolutionary recurrent network which automates the design of recurrent neural/fuzzy networks using a new evolutionary learning algorithm is proposed in this paper. This new evolutionary learning algorithm is based on a hybrid of genetic algorithm (GA) and particle swarm optimization (PSO), and is thus called HGAPSO. In HGAPSO, individuals in a new generation are created, not only by crossover and mutation operation as in GA, but also by PSO. The concept of elite strategy is adopted in HGAPSO, where the upper-half of the best-performing individuals in a population are regarded as elites. However, instead of being reproduced directly to the next generation, these elites are first enhanced. The group constituted by the elites is regarded as a swarm, and each elite corresponds to a particle within it. In this regard, the elites are enhanced by PSO, an operation which mimics the maturing phenomenon in nature. These enhanced elites constitute half of the population in the new generation, whereas the other half is generated by performing crossover and mutation operation on these enhanced elites. HGAPSO is applied to recurrent neural/fuzzy network design as follows. For recurrent neural network, a fully connected recurrent neural network is designed and applied to a temporal sequence production problem. For recurrent fuzzy network design, a Takagi-Sugeno-Kang-type recurrent fuzzy network is designed and applied to dynamic plant control. The performance of HGAPSO is compared to both GA and PSO in these recurrent networks design problems, demonstrating its superiority.', 'title': 'A hybrid of genetic algorithm and particle swarm optimization for recurrent network design'}\n",
      "2022-01-07 18:11:06 - Loading Queries...\n",
      "2022-01-07 18:11:06 - Loaded 1000 TEST Queries.\n",
      "2022-01-07 18:11:06 - Query Example: A Direct Search Method to solve Economic Dispatch Problem with Valve-Point Effect\n",
      "2022-01-07 18:11:08 - \n",
      "\n",
      "2022-01-07 18:11:08 - NDCG@1: 0.2200\n",
      "2022-01-07 18:11:08 - NDCG@10: 0.1734\n",
      "2022-01-07 18:11:08 - \n",
      "\n",
      "2022-01-07 18:11:08 - MAP@1: 0.0447\n",
      "2022-01-07 18:11:08 - MAP@10: 0.1039\n",
      "2022-01-07 18:11:08 - \n",
      "\n",
      "2022-01-07 18:11:08 - Recall@1: 0.0447\n",
      "2022-01-07 18:11:08 - Recall@10: 0.1737\n",
      "2022-01-07 18:11:08 - \n",
      "\n",
      "2022-01-07 18:11:08 - P@1: 0.2200\n",
      "2022-01-07 18:11:08 - P@10: 0.0857\n",
      "2022-01-07 18:11:08 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd11df6a73a94fa3bf17a5971950448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:09 - Loaded 47382 TEST Documents.\n",
      "2022-01-07 18:11:09 - Doc Example: {'text': 'Is there a way to avoid ssh printing warning messages like this?               \"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\\\r\",     \"@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\\\\r\",     \"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\\\r\",      Although the remote host identity has changed but I know it is fine and just want to get rid of this warning.', 'title': 'Force ssh to not to print warnings'}\n",
      "2022-01-07 18:11:09 - Loading Queries...\n",
      "2022-01-07 18:11:12 - Loaded 1072 TEST Queries.\n",
      "2022-01-07 18:11:12 - Query Example: Yanked USB Key During Move\n",
      "2022-01-07 18:11:13 - \n",
      "\n",
      "2022-01-07 18:11:13 - NDCG@1: 0.2883\n",
      "2022-01-07 18:11:13 - NDCG@10: 0.3301\n",
      "2022-01-07 18:11:13 - \n",
      "\n",
      "2022-01-07 18:11:13 - MAP@1: 0.2475\n",
      "2022-01-07 18:11:13 - MAP@10: 0.3003\n",
      "2022-01-07 18:11:13 - \n",
      "\n",
      "2022-01-07 18:11:13 - Recall@1: 0.2475\n",
      "2022-01-07 18:11:13 - Recall@10: 0.3841\n",
      "2022-01-07 18:11:13 - \n",
      "\n",
      "2022-01-07 18:11:13 - P@1: 0.2883\n",
      "2022-01-07 18:11:13 - P@10: 0.0477\n",
      "2022-01-07 18:11:13 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d817f99daf684b9f93cc1bc1e9823a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:13 - Loaded 38316 TEST Documents.\n",
      "2022-01-07 18:11:13 - Doc Example: {'text': \"Let's discuss about $SU(3)$. I understand that the most important representations (relevant to physics) are the defining and the adjoint. In the defining representation of $SU(3)$; namely $\\\\mathbf{3}$, the Gell-Mann matrices are used to represent the generators $$ \\\\left[T^{A}\\\\right]_{ij} = \\\\dfrac{1}{2}\\\\lambda^{A}, $$ where $T^A$ are the generators and $\\\\lambda^A$ the Gell-Mann matrices. In adjoint representation, on the other hand, an $\\\\mathbf{8}$, the generators are represented by matrices according to $$ \\\\left[ T_{i} \\\\right]_{jk} = -if_{ijk}, $$ where $f_{ijk}$ are the structure constants. My question is this, how can one represent the generators in the $\\\\mathbf{10}$ of $SU(3)$, which corresponds to a symmetric tensor with 3 upper or lower indices (or for that matter how to represent the $\\\\mathbf{6}$ with two symmetric indices). What is the general procedure to represent the generators in an arbitrary representation?\", 'title': 'Representation of SU(3) generators'}\n",
      "2022-01-07 18:11:13 - Loading Queries...\n",
      "2022-01-07 18:11:16 - Loaded 1039 TEST Queries.\n",
      "2022-01-07 18:11:16 - Query Example: Magnetic field resistance material: are there any?\n",
      "2022-01-07 18:11:17 - \n",
      "\n",
      "2022-01-07 18:11:17 - NDCG@1: 0.3128\n",
      "2022-01-07 18:11:17 - NDCG@10: 0.3709\n",
      "2022-01-07 18:11:17 - \n",
      "\n",
      "2022-01-07 18:11:17 - MAP@1: 0.2565\n",
      "2022-01-07 18:11:17 - MAP@10: 0.3281\n",
      "2022-01-07 18:11:17 - \n",
      "\n",
      "2022-01-07 18:11:17 - Recall@1: 0.2565\n",
      "2022-01-07 18:11:17 - Recall@10: 0.4457\n",
      "2022-01-07 18:11:17 - \n",
      "\n",
      "2022-01-07 18:11:17 - P@1: 0.3128\n",
      "2022-01-07 18:11:17 - P@10: 0.0610\n",
      "2022-01-07 18:11:17 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abe74496b224d0bb9565a9408aafd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:17 - Loaded 48605 TEST Documents.\n",
      "2022-01-07 18:11:17 - Doc Example: {'text': \"In a shortcode context, is there any difference here?               array(             'slideshow' => '',         ),       and               array(             'slideshow' => NULL,         ),       Is there a best practice for that?\", 'title': 'What is the difference between Null vs Empty (Zero Length) string?'}\n",
      "2022-01-07 18:11:17 - Loading Queries...\n",
      "2022-01-07 18:11:18 - Loaded 541 TEST Queries.\n",
      "2022-01-07 18:11:18 - Query Example: How to enqueue script or style in a theme's template file?\n",
      "2022-01-07 18:11:19 - \n",
      "\n",
      "2022-01-07 18:11:19 - NDCG@1: 0.2606\n",
      "2022-01-07 18:11:19 - NDCG@10: 0.3154\n",
      "2022-01-07 18:11:19 - \n",
      "\n",
      "2022-01-07 18:11:19 - MAP@1: 0.2363\n",
      "2022-01-07 18:11:19 - MAP@10: 0.2864\n",
      "2022-01-07 18:11:19 - \n",
      "\n",
      "2022-01-07 18:11:19 - Recall@1: 0.2363\n",
      "2022-01-07 18:11:19 - Recall@10: 0.3816\n",
      "2022-01-07 18:11:19 - \n",
      "\n",
      "2022-01-07 18:11:19 - P@1: 0.2606\n",
      "2022-01-07 18:11:19 - P@10: 0.0442\n",
      "2022-01-07 18:11:19 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26343c2c315c421ebbb1f1c0061861ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37637 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:19 - Loaded 37637 TEST Documents.\n",
      "2022-01-07 18:11:19 - Doc Example: {'text': \"There is a satellite image it's size is 10 GB and I need to display this image using GeoServer and OpenLayers. When user select the Satellite image in the layer switcher need to display image within 10 seconds. I tried geopdf but the image quality loss isn't acceptable to customer. I want to achieve 10 seconds response time using 32 GB satellite image. Please advice me how to achieve this? Thanks in advance.\", 'title': 'Satellite image display with the help of GeoServer and OpenLayers'}\n",
      "2022-01-07 18:11:19 - Loading Queries...\n",
      "2022-01-07 18:11:22 - Loaded 885 TEST Queries.\n",
      "2022-01-07 18:11:22 - Query Example: Calculating mean upslope aspect from each cell in DEM using Python?\n",
      "2022-01-07 18:11:22 - \n",
      "\n",
      "2022-01-07 18:11:22 - NDCG@1: 0.2791\n",
      "2022-01-07 18:11:22 - NDCG@10: 0.3362\n",
      "2022-01-07 18:11:22 - \n",
      "\n",
      "2022-01-07 18:11:22 - MAP@1: 0.2559\n",
      "2022-01-07 18:11:22 - MAP@10: 0.3078\n",
      "2022-01-07 18:11:22 - \n",
      "\n",
      "2022-01-07 18:11:22 - Recall@1: 0.2559\n",
      "2022-01-07 18:11:22 - Recall@10: 0.4039\n",
      "2022-01-07 18:11:22 - \n",
      "\n",
      "2022-01-07 18:11:22 - P@1: 0.2791\n",
      "2022-01-07 18:11:22 - P@10: 0.0460\n",
      "2022-01-07 18:11:22 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121f271c44fa49018739b15aaa2ddb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:22 - Loaded 45301 TEST Documents.\n",
      "2022-01-07 18:11:22 - Doc Example: {'text': 'What\\'s your Supreme Commander 2 build order. I don\\'t just want \"6 mass extractors, 2 power and a factory\". List of building and units out to the second or third factory, please.', 'title': 'Supreme Commander 2 - Build Orders'}\n",
      "2022-01-07 18:11:22 - Loading Queries...\n",
      "2022-01-07 18:11:28 - Loaded 1595 TEST Queries.\n",
      "2022-01-07 18:11:28 - Query Example: Can the trophy system protect me against bullets?\n",
      "2022-01-07 18:11:28 - \n",
      "\n",
      "2022-01-07 18:11:28 - NDCG@1: 0.4357\n",
      "2022-01-07 18:11:28 - NDCG@10: 0.5051\n",
      "2022-01-07 18:11:28 - \n",
      "\n",
      "2022-01-07 18:11:28 - MAP@1: 0.3795\n",
      "2022-01-07 18:11:28 - MAP@10: 0.4648\n",
      "2022-01-07 18:11:28 - \n",
      "\n",
      "2022-01-07 18:11:28 - Recall@1: 0.3795\n",
      "2022-01-07 18:11:28 - Recall@10: 0.5864\n",
      "2022-01-07 18:11:28 - \n",
      "\n",
      "2022-01-07 18:11:28 - P@1: 0.4357\n",
      "2022-01-07 18:11:28 - P@10: 0.0750\n",
      "2022-01-07 18:11:28 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8239d7d14ce4eb09f83cb9b565bc77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:29 - Loaded 42269 TEST Documents.\n",
      "2022-01-07 18:11:29 - Doc Example: {'text': \"I'm a beginner in statistics and R, sorry if this question may seem trivial. I've collected data measuring several different parameters in 40 subjects at two time-points (t1 and t2). There are 3 main parameters in which I'm interested, let's call them ParA, ParB, ParC. ParA is a score of disability. It is on an arbitrary scale (so it is an ordinal scale measure, if my understanding is correct) and values range from 0.0 to 10.0. Note that the increments in this scale are by 0.5 unit, so values like, e.g. 1.5 are possible. I have two measures, at t1 and t2, so I can describe at least three variables from ParA: ParA at t1, ParA at t2, and whether a subject progressed or not (0 or 1). Being a ratio scale measure, I think it would not make much sense to compute a difference (eg. ParA at t2 - ParA at t1), but I'm willing to accept suggestions on this matter. ParB and ParC are meausurements of two anatomical structures; they are continuous; Par B is an area measured in mm2, ParC is a volume measured in mm3. I believe they should be considered ratio scale measure. For each one I can describe at least 4 variables: measuments at t1 and t2 (eg. ParB1, ParB2), absolute difference between the two measurements (ParB2-ParB1), percentual difference between the two. What I want to do: I want to do some sort of regression, to see if ParA at t2 is best predicted by B or C (percentual difference, or maybe ParB or ParC at t1, I've yet to decide which makes most sense), but I don't know how to do this. I tried doing some research and concluded that I want to do an ordinal logistic regression, but now I'm unsure how to interpret the results and I'm questioning my choice. Here's some data in case someone wants to reproduce my situation:               require(MASS)          example.df <- data.frame(ParA1=c(1.5,0.0,0.0,1.0,1.5,1.0,1.0,0.0,0.0,0.0,3.0), ParA2 = c(2.5,1.5,1.0,2.0,2.0,1.5,2.0,0.0,0.0,0.0,6.5), Progressed=c(1,1,1,1,1,1,1,0,0,0,1), ParB1=c(222.76,743.07,559.65,642.93,584.36,565.53,590.88,465.31,570.22,543.91,574.80), ParB2=c(214.5,674.71,538.75,560.72,581.9,566.40,499.72, 434.72,528.33,517.61,516.1), ParBAbsolDiff=c(-8.27,-68.36,-20.90,-82.21,-2.46,0.87,-91.16,-30.59,-41.88,-26.31,-58.71), ParBPercentDiff=c(-3.71,-9.20,-3.73,-12.79,-0.42,0.15,-15.43,-6.57,-7.34,-4.84,-10.21), ParC1=c(1585354,1600993,1818728,1595059,1445126,1599984,1454398,1540987,1567783,1559505,1523271), ParC2=c(1578834,1512068,1800791,1514774,1472185,1548337,1440284,1505046,1586734,1622379,1496734), ParCAbsolutDiff=c(-6520.26,-88925.62,-17937.04,-80285.40,27059.77,-51646.81,-14114.52,-35940.91,18951.04,62873.71,-26536.51), ParCPercentDiff=c(-0.41,-5.55,-0.99,-5.03,1.87,-3.23,-0.97,-2.33,1.21,4.03,-1.74))          > myregression <- polr(ParA2 ~ ParBPercentDiff + ParCPercentDiff,data=example.df, Hess=TRUE)     Error in polr(ParA2 ~ ParBPercentDiff + ParCPercentDiff, data = example.df,  :      response must be a factor          > example.df$ParA2 <- factor(example.df$ParA2)     > myregression <- polr(ParA2 ~ ParBPercentDiff + ParCPercentDiff, data=example.df, Hess=TRUE)          > summary(myregression)     Call:     polr(formula = ParA2 ~ ParBPercentDiff + ParCPercentDiff, data = example.df,      Hess = TRUE)          Coefficients:                    Value Std. Error t value     ParBPercentDiff -0.04825     0.1114 -0.4330     ParCPercentDiff -0.13650     0.2079 -0.6566          Intercepts:         Value   Std. Error t value     0|1     -0.4982  0.9546    -0.5219     1|1.5   -0.0267  0.9367    -0.0285     1.5|2    0.7736  0.9874     0.7835     2|2.5    2.1062  1.1628     1.8113     2.5|6.5  2.8957  1.3531     2.1400          Residual Deviance: 35.89846      AIC: 49.89846      > ci <- confint(myregression)     Waiting for profiling to be done...     > exp(cbind(OR= coef(myregression), ci))                        OR     2.5 %   97.5 %     ParBPercentDiff 0.9528960 0.7596362 1.200121     ParCPercentDiff 0.8724038 0.5670611 1.321134      I searched the internet, but I don't understand what I found. There are several questions in stackoverflow, but the answers were too advanced. I'm a medical student, so I have some basics concept in statistics, but this is advanced stuff for my current level of understanding. I'm trying to study statistics in my free time, but at the moment I'm pressed, and I need to understand what this mean; I think it is best if it is explained in simple terms. Getting back to my question... I'm confused by the results I obtained, probably because I'm confused about some underlying concept. What I conclude from looking at summary(myregression) is that ParBPercentDiff is a worse predictor than ParCPercentDiff (a certain increase in ParBPercentDiff gives a decrease of -0.04 in the expected value of ParA2 on the log odds scale - so a decrease of ParBPercentDiff should give an increase of +0.04; these values are higher for ParCPercentDiff); however the OR seems to tell a different story; Like, the odds of ParA2 increasing are greater with an increase in ParBPercentDiff...? My interpretation is surely flawed. I think I could interpret the results more easily if I could plot this regression, but I didn't get what I expected.               myprof <- profile(myregression)     plot(myprof)      So:   * Is ordinal logistic regression really what I want to do in this case?   * If not, what analysis do you suggest?   * If yes, how can I interpret the result I got (please, in simple terms)?\", 'title': 'Is this a case for an ordinal logistic regression? Problems interpreting output'}\n",
      "2022-01-07 18:11:29 - Loading Queries...\n",
      "2022-01-07 18:11:30 - Loaded 652 TEST Queries.\n",
      "2022-01-07 18:11:30 - Query Example: Tool to confirm Gaussian fit\n",
      "2022-01-07 18:11:31 - \n",
      "\n",
      "2022-01-07 18:11:31 - NDCG@1: 0.2531\n",
      "2022-01-07 18:11:31 - NDCG@10: 0.3138\n",
      "2022-01-07 18:11:31 - \n",
      "\n",
      "2022-01-07 18:11:31 - MAP@1: 0.2273\n",
      "2022-01-07 18:11:31 - MAP@10: 0.2820\n",
      "2022-01-07 18:11:31 - \n",
      "\n",
      "2022-01-07 18:11:31 - Recall@1: 0.2273\n",
      "2022-01-07 18:11:31 - Recall@10: 0.3849\n",
      "2022-01-07 18:11:31 - \n",
      "\n",
      "2022-01-07 18:11:31 - P@1: 0.2531\n",
      "2022-01-07 18:11:31 - P@10: 0.0456\n",
      "2022-01-07 18:11:31 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e0d2878cb4462396b701296e4011d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:31 - Loaded 17405 TEST Documents.\n",
      "2022-01-07 18:11:31 - Doc Example: {'text': 'I\\'m making a website for a small hotel in php. The hotel owners want a reservation system that uses paypal. They want people to see a calendar and choose a date to make a reservation. If the day has vacancy, they want the user to request booking a room. This would then require the hotel owner to accept the purchase. I have not worked on a project that has this \"request to purchase\" method of buying with paypal. Is this possible? Does anyone know of an open php system that handles this?', 'title': 'Hotel Reservation Request Booking Paypal PHP'}\n",
      "2022-01-07 18:11:31 - Loading Queries...\n",
      "2022-01-07 18:11:32 - Loaded 506 TEST Queries.\n",
      "2022-01-07 18:11:32 - Query Example: Someone else is using our Google Analytics Tracking code number. What do we do?\n",
      "2022-01-07 18:11:32 - \n",
      "\n",
      "2022-01-07 18:11:32 - NDCG@1: 0.3123\n",
      "2022-01-07 18:11:32 - NDCG@10: 0.3562\n",
      "2022-01-07 18:11:32 - \n",
      "\n",
      "2022-01-07 18:11:32 - MAP@1: 0.2649\n",
      "2022-01-07 18:11:32 - MAP@10: 0.3214\n",
      "2022-01-07 18:11:32 - \n",
      "\n",
      "2022-01-07 18:11:32 - Recall@1: 0.2649\n",
      "2022-01-07 18:11:32 - Recall@10: 0.4084\n",
      "2022-01-07 18:11:32 - \n",
      "\n",
      "2022-01-07 18:11:32 - P@1: 0.3123\n",
      "2022-01-07 18:11:32 - P@10: 0.0583\n",
      "2022-01-07 18:11:32 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92067e7e4d749929336de2da2146cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:32 - Loaded 16705 TEST Documents.\n",
      "2022-01-07 18:11:32 - Doc Example: {'text': \"I'm trying to use `Get` to load some pretty substantial packages from a custom menu in the _Mathematica_ toolbar (added via MenuSetup.tr).   The problem is, the standard 5-second evaluation timeout seems to apply to commands executed with `KernelExecute`, so only a fraction of my `Get` is evaluated before the command times out. I'm wondering whether there's an option that can be passed to `KernelExecute` (or to `Item` / `MenuItem`) that will remove that time constraint so that my command can be executed completely.\", 'title': 'Time constraints on KernelExecute commands or MenuItems?'}\n",
      "2022-01-07 18:11:32 - Loading Queries...\n",
      "2022-01-07 18:11:35 - Loaded 804 TEST Queries.\n",
      "2022-01-07 18:11:35 - Query Example: How to use Automorphisms[] on a graph?\n",
      "2022-01-07 18:11:35 - \n",
      "\n",
      "2022-01-07 18:11:35 - NDCG@1: 0.1866\n",
      "2022-01-07 18:11:35 - NDCG@10: 0.2422\n",
      "2022-01-07 18:11:35 - \n",
      "\n",
      "2022-01-07 18:11:35 - MAP@1: 0.1522\n",
      "2022-01-07 18:11:35 - MAP@10: 0.2065\n",
      "2022-01-07 18:11:35 - \n",
      "\n",
      "2022-01-07 18:11:35 - Recall@1: 0.1522\n",
      "2022-01-07 18:11:35 - Recall@10: 0.3160\n",
      "2022-01-07 18:11:35 - \n",
      "\n",
      "2022-01-07 18:11:35 - P@1: 0.1866\n",
      "2022-01-07 18:11:35 - P@10: 0.0412\n",
      "2022-01-07 18:11:35 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264669d3f9c247d5843be07af8340b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:35 - Loaded 22998 TEST Documents.\n",
      "2022-01-07 18:11:35 - Doc Example: {'text': \"I want to send files to android tablet with a application from PC. - I can send files directly to tablet (2.3 android OS) PC see it as a external usb drive. - But i can't send files to tablet (4.2 android OS), because PC see it as a portable media player.(MTP) - How can i fix this problem ? - How can show my device as a external drive? my application that sent files written via Delphi.\", 'title': 'How can show android tablet as a external storage to PC?'}\n",
      "2022-01-07 18:11:35 - Loading Queries...\n",
      "2022-01-07 18:11:36 - Loaded 699 TEST Queries.\n",
      "2022-01-07 18:11:36 - Query Example: Android chroot ubuntu - is it possible to get ubuntu to recognise usb devices\n",
      "2022-01-07 18:11:37 - \n",
      "\n",
      "2022-01-07 18:11:37 - NDCG@1: 0.3619\n",
      "2022-01-07 18:11:37 - NDCG@10: 0.4204\n",
      "2022-01-07 18:11:37 - \n",
      "\n",
      "2022-01-07 18:11:37 - MAP@1: 0.2879\n",
      "2022-01-07 18:11:37 - MAP@10: 0.3735\n",
      "2022-01-07 18:11:37 - \n",
      "\n",
      "2022-01-07 18:11:37 - Recall@1: 0.2879\n",
      "2022-01-07 18:11:37 - Recall@10: 0.4989\n",
      "2022-01-07 18:11:37 - \n",
      "\n",
      "2022-01-07 18:11:37 - P@1: 0.3619\n",
      "2022-01-07 18:11:37 - P@10: 0.0754\n",
      "2022-01-07 18:11:37 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfb0402f18343218a983b97d59c825b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:37 - Loaded 32176 TEST Documents.\n",
      "2022-01-07 18:11:37 - Doc Example: {'text': \"I am in the midst of writing a web application for work. Everything is from scratch. I have been a PHP programmer for about 13 years, Node.js programmer for the past 2 years, and have no shortage of experience with JavaScript. I love Node.js, and recently rebuilt the company's API in it... So, in planning this web application, the approach I'm considering is, have the Node.js API for getting data from the server, but render everything in the browser. Use AJAX for retrieving data, History API for loading pages, and a MVC-like pattern for the different components. I have read articles detailing twitters rebuild a few years ago. It was more or less a client-side JavaScript app, but a couple years after launching it, they started moving a lot of processing/rendering back to the server, claiming the app improved dramatically in terms of speed. So, my question is as the title asks, is a client-side centric app substantially slower?\", 'title': 'Are (mostly) client-side JavaScript web apps slower or less efficient?'}\n",
      "2022-01-07 18:11:37 - Loading Queries...\n",
      "2022-01-07 18:11:40 - Loaded 876 TEST Queries.\n",
      "2022-01-07 18:11:40 - Query Example: Why is closure important for JavaScript?\n",
      "2022-01-07 18:11:40 - \n",
      "\n",
      "2022-01-07 18:11:40 - NDCG@1: 0.2968\n",
      "2022-01-07 18:11:40 - NDCG@10: 0.3415\n",
      "2022-01-07 18:11:40 - \n",
      "\n",
      "2022-01-07 18:11:40 - MAP@1: 0.2365\n",
      "2022-01-07 18:11:40 - MAP@10: 0.3029\n",
      "2022-01-07 18:11:40 - \n",
      "\n",
      "2022-01-07 18:11:40 - Recall@1: 0.2365\n",
      "2022-01-07 18:11:40 - Recall@10: 0.4059\n",
      "2022-01-07 18:11:40 - \n",
      "\n",
      "2022-01-07 18:11:40 - P@1: 0.2968\n",
      "2022-01-07 18:11:40 - P@10: 0.0567\n",
      "2022-01-07 18:11:40 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7965059bd9412d8fc6c82623cd5d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:40 - Loaded 40221 TEST Documents.\n",
      "2022-01-07 18:11:40 - Doc Example: {'text': 'An eponym is one way to eternal (if posthumous) fame. But is there a word meaning an eponym someone would sooner not have? (One would presume that Captain Charles _Boycott_ , Mr Justice _Lynch_ , and Patrick _Hooligan_ would not appreciate their undying notoriety.)', 'title': 'Is there a word meaning \"an unwanted eponym\"?'}\n",
      "2022-01-07 18:11:40 - Loading Queries...\n",
      "2022-01-07 18:11:47 - Loaded 1570 TEST Queries.\n",
      "2022-01-07 18:11:47 - Query Example: Is \"a wide range of features\" singular or plural?\n",
      "2022-01-07 18:11:48 - \n",
      "\n",
      "2022-01-07 18:11:48 - NDCG@1: 0.3401\n",
      "2022-01-07 18:11:48 - NDCG@10: 0.3781\n",
      "2022-01-07 18:11:48 - \n",
      "\n",
      "2022-01-07 18:11:48 - MAP@1: 0.2710\n",
      "2022-01-07 18:11:48 - MAP@10: 0.3393\n",
      "2022-01-07 18:11:48 - \n",
      "\n",
      "2022-01-07 18:11:48 - Recall@1: 0.2710\n",
      "2022-01-07 18:11:48 - Recall@10: 0.4299\n",
      "2022-01-07 18:11:48 - \n",
      "\n",
      "2022-01-07 18:11:48 - P@1: 0.3401\n",
      "2022-01-07 18:11:48 - P@10: 0.0643\n",
      "2022-01-07 18:11:48 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044a6d767711495fb483f875b3800ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:11:49 - Loaded 68184 TEST Documents.\n",
      "2022-01-07 18:11:49 - Doc Example: {'text': \"I am using a pgfplots stacked bar to display the aggregated energy demand of a houshold and the associated price. When the energy demand exceeds a certain threshold, than a higher price has to be paid. This is visualized by the color red and blue of the bars. The threshold is displayed by the thick red horizontal line. My problem is, that I want this red line to exceed the width of the bar, so that it's width is circa 120 percent of the width of the bar. Is there any possibility to achieve this? Thanks ![enter image description here](http://i.stack.imgur.com/3qeEi.jpg)               \\\\documentclass[tikz]{standalone}     \\\\usepackage{pgfplots}     \\\\pgfplotsset{compat=1.10}     \\\\begin{document}     \\\\begin{tikzpicture}     \\\\begin{axis}[       ymin=0,ymax=4,       samples=3,       enlarge x limits={abs=0.5},       bar width=0.6,       ybar stacked,       legend pos=south east,         every axis/.append style={font=\\\\footnotesize},     ]          \\\\draw[red, very thick] (axis cs:0.7,2) -- (axis cs:1.3,2);     \\\\draw[red, very thick] (axis cs:1.7,2.5) -- (axis cs:2.3,2.5);     \\\\draw[red, very thick] (axis cs:2.7,2.5) -- (axis cs:3.3,2.5);          \\\\addplot     coordinates          {(1,1) (2,2.5) (3,1.5)};          \\\\addplot     coordinates          {(1,0) (2,1) (3,0)};               \\\\legend{low price, high price}     \\\\end{axis}     \\\\end{tikzpicture}     \\\\end{document}\", 'title': 'Adding horizontal lines to pgfplots bar'}\n",
      "2022-01-07 18:11:49 - Loading Queries...\n",
      "2022-01-07 18:12:39 - Loaded 2906 TEST Queries.\n",
      "2022-01-07 18:12:39 - Query Example: How can I learn to make my own packages?\n",
      "2022-01-07 18:12:41 - \n",
      "\n",
      "2022-01-07 18:12:41 - NDCG@1: 0.2199\n",
      "2022-01-07 18:12:41 - NDCG@10: 0.2625\n",
      "2022-01-07 18:12:41 - \n",
      "\n",
      "2022-01-07 18:12:41 - MAP@1: 0.1849\n",
      "2022-01-07 18:12:41 - MAP@10: 0.2332\n",
      "2022-01-07 18:12:41 - \n",
      "\n",
      "2022-01-07 18:12:41 - Recall@1: 0.1849\n",
      "2022-01-07 18:12:41 - Recall@10: 0.3167\n",
      "2022-01-07 18:12:41 - \n",
      "\n",
      "2022-01-07 18:12:41 - P@1: 0.2199\n",
      "2022-01-07 18:12:41 - P@10: 0.0416\n",
      "2022-01-07 18:12:41 - CQA Final\n",
      "2022-01-07 18:12:41 - {'NDCG@1': 0.29559833333333335, 'NDCG@10': 0.34770499999999993}\n",
      "2022-01-07 18:12:41 - {'MAP@1': 0.2500375, 'MAP@10': 0.31216416666666663}\n",
      "2022-01-07 18:12:41 - {'Recall@1': 0.2500375, 'Recall@10': 0.4135408333333333}\n",
      "2022-01-07 18:12:41 - {'P@1': 0.29559833333333335, 'P@10': 0.05474333333333333}\n",
      "2022-01-07 18:12:41 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac6465081a54c09b70deb5f544ae8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2866316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:12:52 - Loaded 2866316 TEST Documents.\n",
      "2022-01-07 18:12:52 - Doc Example: {'text': 'This Boston college professor who lives in #NH is on leave after being arrested for child pornography, endangerment:', 'title': ''}\n",
      "2022-01-07 18:12:52 - Loading Queries...\n",
      "2022-01-07 18:12:52 - Loaded 97 TEST Queries.\n",
      "2022-01-07 18:12:52 - Query Example: VIDEO:Good Samaritans Stop Alleged Hit-and-Run Driver in Miami\n",
      "2022-01-07 18:12:52 - \n",
      "\n",
      "2022-01-07 18:12:52 - NDCG@1: 0.4691\n",
      "2022-01-07 18:12:52 - NDCG@10: 0.3388\n",
      "2022-01-07 18:12:52 - \n",
      "\n",
      "2022-01-07 18:12:52 - MAP@1: 0.0368\n",
      "2022-01-07 18:12:52 - MAP@10: 0.1286\n",
      "2022-01-07 18:12:52 - \n",
      "\n",
      "2022-01-07 18:12:52 - Recall@1: 0.0368\n",
      "2022-01-07 18:12:52 - Recall@10: 0.1607\n",
      "2022-01-07 18:12:52 - \n",
      "\n",
      "2022-01-07 18:12:52 - P@1: 0.5464\n",
      "2022-01-07 18:12:52 - P@10: 0.3083\n",
      "2022-01-07 18:12:52 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b877cbaf490d4a53a78a954fdb12cd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594977 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:13:06 - Loaded 594977 TEST Documents.\n",
      "2022-01-07 18:13:06 - Doc Example: {'text': 'NEW ORLEANS — Whenever a Virginia Tech offensive coach is asked how the most prolific receiving duo in school history came to be, inevitably the first road game in 2008 against North Carolina comes up. Midway through the first quarter, Virginia Tech had to call two timeouts in a row because then-freshmen Jarrett Boykin and Danny Coale couldn’t seem to line up right, and “they had those big eyes out there looking around,” Kevin Sherman, their position coach, said recently. Now that Boykin and Coale have only Tuesday’s Sugar Bowl remaining before leaving Virginia Tech with every major school record for a wide receiver, they’ve taken a different stance. “I still don’t think that was on us. Macho [Harris] was in the game and he lined up wrong,” said Boykin, as Coale sat next to him nodding in agreement. Just add that to the list of slights these seniors have had to overcome. Boykin has been the team’s leading receiver the past three seasons, using hands that need size XXXL gloves and a knack for out-maneuvering opposing cornerbacks in the air to set a single-season school record for receptions this year (57). He will end his career with more catches (180) and yards (2,854) than any other Hokies receiver. Coale, an Episcopal High graduate, leads Virginia Tech with 785 receiving yards this year. He is right behind Boykin in the school record books and became the team’s starting punter by the end of this season. Coach Frank Beamer has frequently marveled how “Danny just always seems to be open.” And yet neither warranted even honorable mention all-ACC status this year, a snub that quarterback Logan Thomas said made him “extremely upset” and left Beamer wondering about the media members who participated in the voting. In retrospect, Boykin said he recognizes the lack of notoriety is partly due to Virginia Tech’s offensive philosophy. The Hokies have always been known for their rushing attack, and this year was no different. Running back David Wilson earned ACC player of the year honors during a year when Thomas set multiple records for a first-year quarterback. “There’s just some things that we were held back from being able to show,” Boykin said, “that we’re just as good as [South Carolina wide receiver] Alshon Jeffrey and [Oklahoma State wide receiver] Justin Blackmon. I feel like they’re great athletes, but at the same time we’re right up there with them. “It’s great playing wide receiver here because once we throw the ball, you have opportunities to get big chunks of yardage. What we can’t do is we’re not going to catch 100 balls for 1,500 yards and 22 touchdowns.” The other issue is that neither has the sort of attention-grabbing personality or pedigree associated with big-time wide receivers these days. Coale has graduated with a degree in finance and was named the ACC’s top scholar-athlete this year. He speaks in measured tones reminiscent of a CEO and has yet to join Facebook or Twitter. Boykin is so quiet around the team facility that Beamer said he sometimes doesn’t notice him until he’s making catches on the practice field or in games. Coming out of high school, Coale was barely recruited. Before showing up to a camp in Blacksburg one summer, his only scholarship offer was from VMI, where his father is the head of strength and conditioning. Coale still jokes that when he spent his redshirt year (2007) on the scout team, former Virginia Tech wide receivers and future NFL wideouts Eddie Royal, Andre Davis and Josh Morgan “must have thought I was a walk-on. I prefer to just fly under the radar.” But their accomplishments haven’t gone unnoticed now that the clock is ticking on their careers. Quarterbacks coach Mike O’Cain said Thomas’s comfort level during his record-setting first year under center is a direct reflection of Boykin and Coale. “Not only are they gonna run the right route with the right timing, you know they’re gonna catch the ball,” he said. Years of lining up together has also created a special bond between the two, and it played out before the ACC championship game this year. Boykin was supposed to deliver the pregame speech, but always reticent about public speaking, he was afraid he might stutter and not be taken seriously. He asked Coale to take his place. “I’ve been through his struggles, he’s been through mine,” Coale said. “He’s a guy that I know I can count on, whether it’s five years from now, I just know I can count on him and he’ll be there. I know when I look back, part of my Tech experience is going to be him.”', 'title': 'Danny Coale, Jarrett Boykin are a perfect 1-2 punch for Virginia Tech'}\n",
      "2022-01-07 18:13:06 - Loading Queries...\n",
      "2022-01-07 18:13:06 - Loaded 57 TEST Queries.\n",
      "2022-01-07 18:13:06 - Query Example: Baseball’s minor leaguers pursue their dreams below the poverty line\n",
      "2022-01-07 18:13:06 - \n",
      "\n",
      "2022-01-07 18:13:06 - NDCG@1: 0.4532\n",
      "2022-01-07 18:13:06 - NDCG@10: 0.3997\n",
      "2022-01-07 18:13:06 - \n",
      "\n",
      "2022-01-07 18:13:06 - MAP@1: 0.0217\n",
      "2022-01-07 18:13:06 - MAP@10: 0.1160\n",
      "2022-01-07 18:13:06 - \n",
      "\n",
      "2022-01-07 18:13:06 - Recall@1: 0.0217\n",
      "2022-01-07 18:13:06 - Recall@10: 0.1424\n",
      "2022-01-07 18:13:06 - \n",
      "\n",
      "2022-01-07 18:13:06 - P@1: 0.6667\n",
      "2022-01-07 18:13:06 - P@10: 0.4965\n",
      "2022-01-07 18:13:06 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a42234d12b2446ebb401dbe6edefbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14914714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:15:14 - Loaded 14914604 TEST Documents.\n",
      "2022-01-07 18:15:15 - Doc Example: {'text': 'Depressive disorder is one of the most widespread forms of mental disorders which lead to a significant public health concern, such as disability, suicide, and so on. Its etiology remains vague but it is believed that depressive disorder is a multifactorial disease which is induced by the interaction of social, psychological, and biological factors. Thus, there is no clear and definite pathological theory could illustrate its mechanism independently until now, involving genetics, neuroimaging, neuroinflammation, neuroendocrine, and others. Comprehensive assessment to patients with depression is the starting point for a right diagnosis. History-taking of physical condition is as important as psychiatric interview and rational usage of scales would be beneficial for screening. There are many kinds of therapeutic measures for depressive patients nowadays, including general intervention, pharmacotherapy, psychotherapy, and physical therapy. For now, anti-depressants used in clinical practice is almost monoamine-based drugs while much more progress have been made in developing new antidepressant medications, like prototypical N-methyl-D-aspartate (NMDA) receptor antagonists, opioid agonists, gamma-aminobutyric acid (GABAA) receptors, and psychedelics. Once these novel drugs are proved to be practicable, it will create a historical evolution in the field of psychiatry. In addition, we advocate that measurement-based care (MBC) should run through the whole duration of treatment and goals of MBC in every stage are different. As brain projects in many countries are conducting in inspiring ways, we believe that our understanding about depressive disorder, of course, and other neuropsychiatric disorders will be better in the future.', 'title': 'Introduction'}\n",
      "2022-01-07 18:15:15 - Loading Queries...\n",
      "2022-01-07 18:15:15 - Loaded 500 TEST Queries.\n",
      "2022-01-07 18:15:15 - Query Example: Is poliosis circumscripta another term for a white or unpigmented patch of hair or skin?\n",
      "2022-01-07 18:15:16 - \n",
      "\n",
      "2022-01-07 18:15:16 - NDCG@1: 0.5400\n",
      "2022-01-07 18:15:16 - NDCG@10: 0.5068\n",
      "2022-01-07 18:15:16 - \n",
      "\n",
      "2022-01-07 18:15:16 - MAP@1: 0.2654\n",
      "2022-01-07 18:15:16 - MAP@10: 0.4131\n",
      "2022-01-07 18:15:16 - \n",
      "\n",
      "2022-01-07 18:15:16 - Recall@1: 0.2654\n",
      "2022-01-07 18:15:16 - Recall@10: 0.5127\n",
      "2022-01-07 18:15:16 - \n",
      "\n",
      "2022-01-07 18:15:16 - P@1: 0.5400\n",
      "2022-01-07 18:15:16 - P@10: 0.1894\n",
      "2022-01-07 18:15:16 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3510705c67645a187e1aa65367b4acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 18:15:26 - Loaded 528155 TEST Documents.\n",
      "2022-01-07 18:15:26 - Doc Example: {'text': '\\n\\nPOLITICIANS,  PARTY PREFERENCES \\n\\n   Summary:  Newspapers in the Former Yugoslav Republic of \\n   Macedonia have published the results of opinion polls, \\n   indicating the relative popularity of politicians, \\n   political parties, and attitudes toward the political system. \\n\\n   The 22-23 January edition of the Skopje newspaper VECER in \\nMacedonian published on pages 6-7 the results of an opinion poll \\nconducted by the \"BriMa\" agency in November 1993. According to \\nVECER, 1,036 respondents were classified by age and residence, but \\nthe paper did not explain the methodology or give the margin of \\nerror.  For the purpose of comparison, the paper cited the results \\nof an unidentified poll made in May 1993. The approval/disapproval \\nratings, in percent, for ten Macedonian politicians were: \\n\\n                                           November 1993    May 1993 \\n\\nKiro Gligorov, President of the Republic      76/15           78/13 \\n\\nVasil Tupurkovski, former Macedonian          50/36           43/37 \\n   official in Federal Yugoslavia \\n\\nLjubomir Frckovski, Interior Minister         50/42           42/43 \\n\\nStojan Andov, Parliamentary Chairman          48/41           48/39 \\n\\nBranko Crvenkovski, Prime Minister            46/41           44/38 \\n\\nVlado Popovski, Defense Minister              41/41           36/37 \\n\\nStevo Crvenkovski, Foreign Minister           40/43   No Data Given \\n\\nPetar Gosev, Democratic Party leader          34/53           40/42 \\n\\nTodor Petrov, Independent parliamentarian     32/53   No Data Given \\n\\nNikola Popovski, Social Democratic            29/46           32/42 \\n   Party parliamentarian \\n\\n   VECER noted that President Gligorov\\'s very high approval rating \\nof 90 percent among those over age 65 fell off to a still high 70 \\npercent among respondents between 18 and 24.  Residents of Skopje \\nranked the politicians in a slightly different order from the \\nranking given by the whole sample: Gligorov, Tupurkovski, Frckovski, \\nAndov, Gosev, Branko Crvenkovski, Vlado Popovski, Petrov, Nikola \\nPopovski, and Stevo Crvenkovski. \\n\\n   The results of a series of opinion polls conducted by the Agency \\nfor Public Opinion Research and published \"exclusively\" by the \\nSkopje weekly PULS newspaper, confirmed Gligorov\\'s substantial lead \\nin popularity among political figures.  According to the 31 December \\n1993 issue of PULS (pages 16-18), the agency gathered the data by \\nmeans of telephone interviews with 300 residents in the Republic of \\nMacedonia during 20-24 December. PULS also provided data from \\nsurveys made in March, June, and September for comparison.  Some of \\nthe following percentages are approximate values that were derived \\nfrom the graph published by the paper: \\n\\n                         March       June      September    December \\n\\nKiro Gligorov             87          82.33      89.33           89 \\nStevo Crvenkovski         54          65         49              63 \\nStojan Andov              61          62         60              61 \\nBranko Crvenkovski        56          60         54 \\n53.5 \\nLjubomir Frckovski        35          45         48              50 \\nPetar Gosev               50          31         52 \\n49.53 \\nJovan Andonov, \\n Deputy Prime Minister    39          39         50              37 \\nVlado Popovski            18          25         36              35 \\nKiro Popovski, Deputy \\n Chairman, Parliament     26          27         33              32 \\nAnte Popovski, leader of \\n MAAK (Movement for All- \\n Macedonian Action)       29          32         32 \\nindistinct \\nJane Miljovski, Minister \\n without Portfolio        --          23         31              24 \\nVladimir Golubovski \\n VMRO-DP (Internal \\n Macedonian Revolutionary \\n Organization-Democratic \\n Party) leader            --          30         25              23 \\nNevzat Halili \\n Party for Democratic \\n Prosperity official      38.33       38         18              18 \\n\\nLj upco Georgievski \\nVMRO-DPMNE (Internal \\nMacedonian Revolutionary \\nOrganization-Democratic \\nParty for Macedonian \\nNational Unity) \\nofficial                  18          10         16              17 \\nDosta Dimovska \\nVMRO-DPMNE \\nofficial                  --          11         17              16 \\n\\n   On pages 6 and 7 of its 15-16 January issue, VECER also published \\nthe results of a November 1993 survey on party preferences. \\n\"BriMa,\" working with the Gallup organization, interviewed 1,036 \\npeople. \\n\\n   Question: \"If elections were held today, for which party would \\nyou vote?\" (all numbers are percentages) \\n\\nSDSM (Social Democratic Alliance of Macedonia)  22.8 \\nVMRO-DPMNE                                      11.2 \\nDemocratic Party (DP, led by Petar Gosev)        6.3 \\nSocialist Party                                  3.3 \\nLiberal Party (LP)                               3.2 \\nWorkers Party                                    2.9 \\nPCERM (Party for the Full Emancipation of \\n    Romanies in Macedonia)                       1.8 \\nDemocratic Party of Turks in Macedonia           0.8 \\nMAAK                                             0.3 \\nAnother party                                    4.0 \\nUndecided                                       18.6 \\nWould not vote                                   6.6 \\n\\n   VECER noted that some parties fared better in certain cities than \\ntheir overall scores indicate.  For example, the DP was about twice \\nas popular in Skopje as elsewhere, getting 12.1 percent in the \\ncapital; the VMRO-DPMNE was more popular in Bitola, getting 15.7 \\npercent, than in the remainder of the country; and the LP in the \\nBregalnica area got the support of 10.6 percent, substantially \\nhigher than the 3.2 percent support it received overall. \\n\\n   Question: \"Do you have confidence in the following parties?\" (all \\nnumbers are percentages) \\n\\n              Yes           No       Do Not Know \\n\\nSDSM           28           51          21 \\nVMRO-DPMNE     15           72          14 \\nLP             19           59          22 \\nPDP-NDP*       20           73           7 \\n\\n*Party for Democratic Prosperity-People\\'s Democratic Party \\n\\n   The poll clearly indicated that Macedonians have little \\nconfidence in any of the parties currently active in the country. \\nRespondents were also asked whether it would be good for the country \\nto have elections sooner than scheduled; 62 percent agreed, 20 \\npercent disagreed, and 18 percent did not know. These findings were \\ncorrelated with party preferences, producing the following results: \\nOf those who would vote for the SDSM, 54 percent wanted elections \\nsoon, while 34 percent were against early elections. However, 80 \\npercent of VMRO-DPMNE supporters favored elections soon, as did 79 \\npercent of LP supporters and 71 percent of DP supporters. While 80 \\npercent of those surveyed thought that a person should vote (14 \\npercent did not agree), only 40 percent thought that it was very \\nimportant which party won the elections and 27 percent thought it \\nwas somewhat significant. \\n\\n   (AUTHOR:  GALYEAN.  QUESTIONS AND/OR COMMENTS, PLEASE CALL CHIEF, \\nBALKANS BRANCH AT (703) 733-6481) \\n\\nELAG/25 February/POLCHF/EED/DEW 28/2023Z FEB \\n\\n\\n', 'title': None}\n",
      "2022-01-07 18:15:26 - Loading Queries...\n",
      "2022-01-07 18:15:26 - Loaded 249 TEST Queries.\n",
      "2022-01-07 18:15:26 - Query Example: Identify organizations that participate in international criminal\n",
      "activity, the activity, and, if possible, collaborating organizations\n",
      "and the countries involved.\n",
      "2022-01-07 18:15:31 - \n",
      "\n",
      "2022-01-07 18:15:31 - NDCG@1: 0.4739\n",
      "2022-01-07 18:15:31 - NDCG@10: 0.4187\n",
      "2022-01-07 18:15:31 - \n",
      "\n",
      "2022-01-07 18:15:31 - MAP@1: 0.0199\n",
      "2022-01-07 18:15:31 - MAP@10: 0.1049\n",
      "2022-01-07 18:15:31 - \n",
      "\n",
      "2022-01-07 18:15:31 - Recall@1: 0.0199\n",
      "2022-01-07 18:15:31 - Recall@10: 0.1418\n",
      "2022-01-07 18:15:31 - \n",
      "\n",
      "2022-01-07 18:15:31 - P@1: 0.5261\n",
      "2022-01-07 18:15:31 - P@10: 0.4024\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "# Subselecting all available ones\n",
    "datasets = [\"trec-covid\", \"webis-touche2020\", \"nfcorpus\", \"scifact\", \"fiqa\", \"dbpedia-entity\",\n",
    "            \"nq\", \"hotpotqa\", \"quora\", \"fever\", \"climate-fever\", \"arguana\", \"msmarco\", \"scidocs\", \"cqadupstack\",\n",
    "            \"signal1m\", \"trec-news\", \"bioasq\", \"robust04\"]\n",
    "\n",
    "k_values = [1, 10]\n",
    "max_rerank = 10 # In BEIR 100 documents are reranked for their rerank encoder benchmark\n",
    "\n",
    "# Need to rerank min 100 for @100;\n",
    "assert max_rerank >= max(k_values), \"Max Rerank is too small for the sample scores to compute\"\n",
    "\n",
    "results_prefix = \"\"\n",
    "model_name = \"gptneo01\"\n",
    "ndcgs = {}\n",
    "\n",
    "def simulate_rerank(results_bm25, results_rerank, new_max_rerank):\n",
    "    \"\"\"\n",
    "    results: Dict[str, Dict[str, float]]\n",
    "    \"\"\"\n",
    "    simulate_rerank_results = {}\n",
    "    for qid in results_bm25:\n",
    "        # Topk items that would have been fed into the model\n",
    "        topk = sorted(results_bm25[qid].items(), key=lambda item: item[1], reverse=True)[:new_max_rerank]\n",
    "        topk = [key_val_pair[0] for key_val_pair in topk]\n",
    "\n",
    "        # Scores the model would have given to these topk items\n",
    "        simulate_rerank_results[qid] = {k: results_rerank[qid][k] for k in topk}\n",
    "\n",
    "    return simulate_rerank_results\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    \n",
    "    results_out_path = f\"{results_prefix}results_{model_name}_promptG_{max_rerank}_{dataset}.json\"\n",
    "    \n",
    "    if os.path.exists(results_out_path):\n",
    "        continue\n",
    "\n",
    "    if not(os.path.exists(os.path.join(os.getcwd(), 'datasets', dataset))):\n",
    "        url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "        out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "        data_path = util.download_and_unzip(url, out_dir)\n",
    "        print(\"Dataset downloaded here: {}\".format(data_path))\n",
    "    # Load the dataset into BEIR\n",
    "    data_path = f\"datasets/{dataset}\"\n",
    "    # In the paper it says, BEIR used the dev set for msmarco\n",
    "    split = \"dev\" if dataset == \"msmarco\" else \"test\"\n",
    "    \n",
    "\n",
    "    # cqadupstack - Contains several sub datasets\n",
    "    if dataset == \"cqadupstack\":\n",
    "        cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "        for sub_dataset in os.listdir(data_path):\n",
    "            \n",
    "            sub_results_out_path = f\"{results_prefix}results_{model_name}_promptG_{max_rerank}_{dataset}_{sub_dataset}.json\"\n",
    "            if os.path.exists(sub_results_out_path):\n",
    "                continue\n",
    "        \n",
    "            sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "            corpus, queries, qrels = GenericDataLoader(sub_data_path).load(split=split)\n",
    "            with open(f\"./results_{dataset}_{sub_dataset}.json\", 'r') as fp:\n",
    "                results_loaded = json.load(fp)\n",
    "            with open(f\"{results_prefix}results_{model_name}_promptG_{dataset}_{sub_dataset}.json\", 'r') as fp:\n",
    "                results_gpt_loaded = json.load(fp)\n",
    "            \n",
    "            simulate_rerank_results = simulate_rerank(results_loaded, results_gpt_loaded, max_rerank)\n",
    "\n",
    "            with open(sub_results_out_path, 'w') as fp:\n",
    "                json.dump(simulate_rerank_results, fp)\n",
    "\n",
    "            ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, simulate_rerank_results, k_values)\n",
    "\n",
    "            cqa_ndcgs.append(ndcg)\n",
    "            cqa_maps.append(_map)\n",
    "            cqa_recalls.append(recall)\n",
    "            cqa_precisions.append(precision)\n",
    "        \n",
    "        if cqa_ndcgs:\n",
    "            for (metric, group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "            logging.info(\"CQA Final\")\n",
    "            logging.info(f\"{ndcg}\")\n",
    "            logging.info(f\"{_map}\")\n",
    "            logging.info(f\"{recall}\")\n",
    "            logging.info(f\"{precision}\")\n",
    "\n",
    "    else:\n",
    "        corpus, queries, qrels = GenericDataLoader(data_path).load(split=split)\n",
    "        with open(f\"./results_{dataset}.json\", 'r') as fp:\n",
    "            results_loaded = json.load(fp)\n",
    "        with open(f\"{results_prefix}results_{model_name}_promptG_{dataset}.json\", 'r') as fp:\n",
    "            results_gpt_loaded = json.load(fp)\n",
    "\n",
    "\n",
    "        simulate_rerank_results = simulate_rerank(results_loaded, results_gpt_loaded, max_rerank)\n",
    "        ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, simulate_rerank_results, k_values)\n",
    "        with open(results_out_path, 'w') as fp:\n",
    "            json.dump(simulate_rerank_results, fp)\n",
    "\n",
    "    ndcgs[dataset] = ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73620635",
   "metadata": {},
   "source": [
    "##### API - 13B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Aleph Alpha Client ###\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from aleph_alpha_client import AlephAlphaClient\n",
    "\n",
    "from beir.reranking import Rerank\n",
    "\n",
    "import time\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# tokenizers==0.10.3\n",
    "from tokenizers import Tokenizer as HF_Tokenizer\n",
    "\n",
    "\n",
    "AA_TOKEN = \"API_TOKEN\"\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Wrapper around HF tokenizer to be able to exchange easily at a later point\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, filename):\n",
    "        tokenizer = HF_Tokenizer.from_file(filename)\n",
    "        return cls(tokenizer=tokenizer)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the vocab size of the tokenizer\n",
    "        \"\"\"\n",
    "        return self.tokenizer.get_vocab_size(with_added_tokens=False)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        converts a string into token ids\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(text).ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]):\n",
    "        \"\"\"\n",
    "        converts a list of token ids to a string\n",
    "        \"\"\"\n",
    "        return self.tokenizer.decode(token_ids)\n",
    "\n",
    "\n",
    "class AARanker:\n",
    "    def __init__(self, model=\"EUTran13B\", tokenizer_file=\"alpha-03-128k.json\", use_prompt=True, prompt_doc=\"{}\", prompt_doc_start=\"{}\\n{}\\n\",\n",
    "                 fewshots=\"\", **kwargs):\n",
    "        \"\"\"\n",
    "        GPTRanker producing log-probabilities for reranking doc & query with a GPT-like model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.client = AlephAlphaClient(host=\"https://api.aleph-alpha.de\", token=AA_TOKEN)\n",
    "        self.model = model\n",
    "        self.tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "            \n",
    "        # Truncation will be done from the left in the log likelihood\n",
    "        self.prompt_doc = prompt_doc\n",
    "        self.use_prompt = use_prompt\n",
    "        \n",
    "        self.instruction_len = len(self.tokenizer.encode(self.prompt_doc[:self.prompt_doc.index(\"{\")]))\n",
    "        \n",
    "        self.fewshots = fewshots\n",
    "        if self.fewshots:\n",
    "            # doc, query\n",
    "            self.fewshots = prompt_doc_start.format(self.fewshots[0], self.fewshots[1])\n",
    "            # Still take overflowing tokens away from the current doc (not the fewshot doc)\n",
    "            self.instruction_len += len(self.tokenizer.encode(self.fewshots))\n",
    "            \n",
    "    # Write your own score function, which takes in query-document text pairs and returns the similarity scores\n",
    "    def predict(self, sentences: List[Tuple[str,str]], batch_size: int, **kwags) -> List[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          sentences: [query, document]\n",
    "          batch_size: Unused\n",
    "\n",
    "        Returns:\n",
    "          log_probs: float log probability for each query-doc pair\n",
    "        \"\"\"\n",
    "        \n",
    "        log_probs = []\n",
    "\n",
    "        for query, doc in sentences:\n",
    "            \n",
    "            doc_with_prompt = self.prompt_doc.format(doc)\n",
    "            \n",
    "            doc_with_prompt_tokens = self.tokenizer.encode(doc_with_prompt)\n",
    "            query_tokens = self.tokenizer.encode(query)\n",
    "            \n",
    "            token_len = len(doc_with_prompt_tokens) + len(query_tokens)\n",
    "            \n",
    "            # AA does not accept token len > 2048\n",
    "            max_len = 2048\n",
    "            if token_len > max_len:\n",
    "                # Truncate from the left of the doc only without truncating from the prompt\n",
    "                # Truncate as much until there is enough space for instruction & query (& rest of the doc)\n",
    "                # Removing one more token here, as there seem to be some discrepencies btw this tokenizer and the one of the API\n",
    "                doc_with_prompt_tokens_trunc = (\n",
    "                    doc_with_prompt_tokens[:self.instruction_len] + doc_with_prompt_tokens[-(max_len-self.instruction_len-len(query_tokens)):]\n",
    "                )\n",
    "                \n",
    "                logging.info(f\"Truncated by {len(doc_with_prompt_tokens) - len(doc_with_prompt_tokens_trunc)} tokens\")\n",
    "                \n",
    "                doc_with_prompt = self.tokenizer.decode(doc_with_prompt_tokens_trunc)\n",
    "                \n",
    "                # Tokenizer sometimes prepends a whitespace after decoding - We remove as we want to maintain the same input just with some toks removed\n",
    "                if self.prompt_doc[0] != \" \" and doc_with_prompt[0] == \" \":\n",
    "                    doc_with_prompt = doc_with_prompt[1:]\n",
    "            \n",
    "            for i in range(1,20,2):\n",
    "                try:\n",
    "                    result = self.client.evaluate(self.model, prompt=doc_with_prompt, completion_expected=query)\n",
    "                except Exception as e:\n",
    "                    logging.info(f\"Retrying after {i} seconds due to {e}\")\n",
    "                    time.sleep(i)\n",
    "                    \n",
    "                    \n",
    "                    if token_len > max_len:\n",
    "                        ### Truncate by one more element ###\n",
    "                        # Copy of above truncation except for additional \"- 1\"\n",
    "                        doc_with_prompt_tokens_trunc = (\n",
    "                            doc_with_prompt_tokens[:self.instruction_len] + doc_with_prompt_tokens[-(max_len-self.instruction_len-len(query_tokens)- 1):]\n",
    "                        )\n",
    "                        logging.info(f\"Truncated by {len(doc_with_prompt_tokens) - len(doc_with_prompt_tokens_trunc)} tokens\")\n",
    "                        doc_with_prompt = self.tokenizer.decode(doc_with_prompt_tokens_trunc)\n",
    "                        # Tokenizer sometimes prepends a whitespace after decoding - We remove as we want to maintain the same input just with some toks removed\n",
    "                        if self.prompt_doc[0] != \" \" and doc_with_prompt[0] == \" \":\n",
    "                            doc_with_prompt = doc_with_prompt[1:]\n",
    "                \n",
    "                    # For other errors just try again\n",
    "                    \n",
    "                    continue\n",
    "                    \n",
    "                break\n",
    "                \n",
    "            log_probs.append(result[\"result\"][\"log_probability\"])\n",
    "        \n",
    "        assert len(log_probs) == len(sentences), \"Only produced {len(log_probs)} results for {len(sentences)} sentences\"\n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b7dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-13 05:09:09 - \n",
      "-------------------- Running prompt G: Documents are searched to find matches with the same content.\n",
      "The document \"{}\" is a good search result for \" --------------------\n",
      "\n",
      "2021-12-13 05:09:10 - \n",
      "---------- Running robust04 ----------\n",
      "\n",
      "2021-12-13 05:09:10 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65128f25ea3745979f863ea927b2d751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-13 05:09:21 - Loaded 528155 TEST Documents.\n",
      "2021-12-13 05:09:21 - Doc Example: {'text': '\\n\\nPOLITICIANS,  PARTY PREFERENCES \\n\\n   Summary:  Newspapers in the Former Yugoslav Republic of \\n   Macedonia have published the results of opinion polls, \\n   indicating the relative popularity of politicians, \\n   political parties, and attitudes toward the political system. \\n\\n   The 22-23 January edition of the Skopje newspaper VECER in \\nMacedonian published on pages 6-7 the results of an opinion poll \\nconducted by the \"BriMa\" agency in November 1993. According to \\nVECER, 1,036 respondents were classified by age and residence, but \\nthe paper did not explain the methodology or give the margin of \\nerror.  For the purpose of comparison, the paper cited the results \\nof an unidentified poll made in May 1993. The approval/disapproval \\nratings, in percent, for ten Macedonian politicians were: \\n\\n                                           November 1993    May 1993 \\n\\nKiro Gligorov, President of the Republic      76/15           78/13 \\n\\nVasil Tupurkovski, former Macedonian          50/36           43/37 \\n   official in Federal Yugoslavia \\n\\nLjubomir Frckovski, Interior Minister         50/42           42/43 \\n\\nStojan Andov, Parliamentary Chairman          48/41           48/39 \\n\\nBranko Crvenkovski, Prime Minister            46/41           44/38 \\n\\nVlado Popovski, Defense Minister              41/41           36/37 \\n\\nStevo Crvenkovski, Foreign Minister           40/43   No Data Given \\n\\nPetar Gosev, Democratic Party leader          34/53           40/42 \\n\\nTodor Petrov, Independent parliamentarian     32/53   No Data Given \\n\\nNikola Popovski, Social Democratic            29/46           32/42 \\n   Party parliamentarian \\n\\n   VECER noted that President Gligorov\\'s very high approval rating \\nof 90 percent among those over age 65 fell off to a still high 70 \\npercent among respondents between 18 and 24.  Residents of Skopje \\nranked the politicians in a slightly different order from the \\nranking given by the whole sample: Gligorov, Tupurkovski, Frckovski, \\nAndov, Gosev, Branko Crvenkovski, Vlado Popovski, Petrov, Nikola \\nPopovski, and Stevo Crvenkovski. \\n\\n   The results of a series of opinion polls conducted by the Agency \\nfor Public Opinion Research and published \"exclusively\" by the \\nSkopje weekly PULS newspaper, confirmed Gligorov\\'s substantial lead \\nin popularity among political figures.  According to the 31 December \\n1993 issue of PULS (pages 16-18), the agency gathered the data by \\nmeans of telephone interviews with 300 residents in the Republic of \\nMacedonia during 20-24 December. PULS also provided data from \\nsurveys made in March, June, and September for comparison.  Some of \\nthe following percentages are approximate values that were derived \\nfrom the graph published by the paper: \\n\\n                         March       June      September    December \\n\\nKiro Gligorov             87          82.33      89.33           89 \\nStevo Crvenkovski         54          65         49              63 \\nStojan Andov              61          62         60              61 \\nBranko Crvenkovski        56          60         54 \\n53.5 \\nLjubomir Frckovski        35          45         48              50 \\nPetar Gosev               50          31         52 \\n49.53 \\nJovan Andonov, \\n Deputy Prime Minister    39          39         50              37 \\nVlado Popovski            18          25         36              35 \\nKiro Popovski, Deputy \\n Chairman, Parliament     26          27         33              32 \\nAnte Popovski, leader of \\n MAAK (Movement for All- \\n Macedonian Action)       29          32         32 \\nindistinct \\nJane Miljovski, Minister \\n without Portfolio        --          23         31              24 \\nVladimir Golubovski \\n VMRO-DP (Internal \\n Macedonian Revolutionary \\n Organization-Democratic \\n Party) leader            --          30         25              23 \\nNevzat Halili \\n Party for Democratic \\n Prosperity official      38.33       38         18              18 \\n\\nLj upco Georgievski \\nVMRO-DPMNE (Internal \\nMacedonian Revolutionary \\nOrganization-Democratic \\nParty for Macedonian \\nNational Unity) \\nofficial                  18          10         16              17 \\nDosta Dimovska \\nVMRO-DPMNE \\nofficial                  --          11         17              16 \\n\\n   On pages 6 and 7 of its 15-16 January issue, VECER also published \\nthe results of a November 1993 survey on party preferences. \\n\"BriMa,\" working with the Gallup organization, interviewed 1,036 \\npeople. \\n\\n   Question: \"If elections were held today, for which party would \\nyou vote?\" (all numbers are percentages) \\n\\nSDSM (Social Democratic Alliance of Macedonia)  22.8 \\nVMRO-DPMNE                                      11.2 \\nDemocratic Party (DP, led by Petar Gosev)        6.3 \\nSocialist Party                                  3.3 \\nLiberal Party (LP)                               3.2 \\nWorkers Party                                    2.9 \\nPCERM (Party for the Full Emancipation of \\n    Romanies in Macedonia)                       1.8 \\nDemocratic Party of Turks in Macedonia           0.8 \\nMAAK                                             0.3 \\nAnother party                                    4.0 \\nUndecided                                       18.6 \\nWould not vote                                   6.6 \\n\\n   VECER noted that some parties fared better in certain cities than \\ntheir overall scores indicate.  For example, the DP was about twice \\nas popular in Skopje as elsewhere, getting 12.1 percent in the \\ncapital; the VMRO-DPMNE was more popular in Bitola, getting 15.7 \\npercent, than in the remainder of the country; and the LP in the \\nBregalnica area got the support of 10.6 percent, substantially \\nhigher than the 3.2 percent support it received overall. \\n\\n   Question: \"Do you have confidence in the following parties?\" (all \\nnumbers are percentages) \\n\\n              Yes           No       Do Not Know \\n\\nSDSM           28           51          21 \\nVMRO-DPMNE     15           72          14 \\nLP             19           59          22 \\nPDP-NDP*       20           73           7 \\n\\n*Party for Democratic Prosperity-People\\'s Democratic Party \\n\\n   The poll clearly indicated that Macedonians have little \\nconfidence in any of the parties currently active in the country. \\nRespondents were also asked whether it would be good for the country \\nto have elections sooner than scheduled; 62 percent agreed, 20 \\npercent disagreed, and 18 percent did not know. These findings were \\ncorrelated with party preferences, producing the following results: \\nOf those who would vote for the SDSM, 54 percent wanted elections \\nsoon, while 34 percent were against early elections. However, 80 \\npercent of VMRO-DPMNE supporters favored elections soon, as did 79 \\npercent of LP supporters and 71 percent of DP supporters. While 80 \\npercent of those surveyed thought that a person should vote (14 \\npercent did not agree), only 40 percent thought that it was very \\nimportant which party won the elections and 27 percent thought it \\nwas somewhat significant. \\n\\n   (AUTHOR:  GALYEAN.  QUESTIONS AND/OR COMMENTS, PLEASE CALL CHIEF, \\nBALKANS BRANCH AT (703) 733-6481) \\n\\nELAG/25 February/POLCHF/EED/DEW 28/2023Z FEB \\n\\n\\n', 'title': None}\n",
      "2021-12-13 05:09:21 - Loading Queries...\n",
      "2021-12-13 05:09:21 - Loaded 249 TEST Queries.\n",
      "2021-12-13 05:09:21 - Query Example: Identify organizations that participate in international criminal\n",
      "activity, the activity, and, if possible, collaborating organizations\n",
      "and the countries involved.\n",
      "2021-12-13 05:09:21 - \n",
      "\n",
      "2021-12-13 05:09:21 - NDCG@1: 0.5221\n",
      "2021-12-13 05:09:21 - NDCG@3: 0.4931\n",
      "2021-12-13 05:09:21 - NDCG@5: 0.4597\n",
      "2021-12-13 05:09:21 - NDCG@10: 0.4251\n",
      "2021-12-13 05:09:21 - NDCG@100: 0.3885\n",
      "2021-12-13 05:09:21 - NDCG@1000: 0.4941\n",
      "2021-12-13 05:09:21 - \n",
      "\n",
      "2021-12-13 05:09:21 - MAP@1: 0.0221\n",
      "2021-12-13 05:09:21 - MAP@3: 0.0569\n",
      "2021-12-13 05:09:21 - MAP@5: 0.0779\n",
      "2021-12-13 05:09:21 - MAP@10: 0.1100\n",
      "2021-12-13 05:09:21 - MAP@100: 0.1989\n",
      "2021-12-13 05:09:21 - MAP@1000: 0.2288\n",
      "2021-12-13 05:09:21 - \n",
      "\n",
      "2021-12-13 05:09:21 - Recall@1: 0.0221\n",
      "2021-12-13 05:09:21 - Recall@3: 0.0631\n",
      "2021-12-13 05:09:21 - Recall@5: 0.0917\n",
      "2021-12-13 05:09:21 - Recall@10: 0.1417\n",
      "2021-12-13 05:09:21 - Recall@100: 0.3854\n",
      "2021-12-13 05:09:21 - Recall@1000: 0.6556\n",
      "2021-12-13 05:09:21 - \n",
      "\n",
      "2021-12-13 05:09:21 - P@1: 0.5823\n",
      "2021-12-13 05:09:21 - P@3: 0.5301\n",
      "2021-12-13 05:09:21 - P@5: 0.4731\n",
      "2021-12-13 05:09:21 - P@10: 0.4020\n",
      "2021-12-13 05:09:21 - P@100: 0.1606\n",
      "2021-12-13 05:09:21 - P@1000: 0.0361\n",
      "2021-12-13 05:09:22 - Starting To Rerank Top-100....\n",
      "2021-12-13 05:09:24 - Truncated by 1480 tokens\n",
      "2021-12-13 05:09:25 - Truncated by 1274 tokens\n",
      "2021-12-13 05:09:26 - Truncated by 266 tokens\n",
      "2021-12-13 05:09:27 - Truncated by 9804 tokens\n",
      "2021-12-13 05:09:28 - Truncated by 1224 tokens\n",
      "2021-12-13 05:09:34 - Truncated by 2555 tokens\n",
      "2021-12-13 05:09:36 - Truncated by 4303 tokens\n",
      "2021-12-13 05:09:38 - Truncated by 3161 tokens\n",
      "2021-12-13 05:09:39 - Truncated by 6661 tokens\n",
      "2021-12-13 05:09:45 - Truncated by 8647 tokens\n",
      "2021-12-13 05:09:48 - Truncated by 10634 tokens\n",
      "2021-12-13 05:09:53 - Truncated by 179 tokens\n",
      "2021-12-13 05:09:54 - Truncated by 180 tokens\n",
      "2021-12-13 05:09:54 - Truncated by 576 tokens\n",
      "2021-12-13 05:09:56 - Truncated by 1139 tokens\n",
      "2021-12-13 05:09:57 - Truncated by 1923 tokens\n",
      "2021-12-13 05:09:57 - Truncated by 1978 tokens\n",
      "2021-12-13 05:09:59 - Truncated by 5141 tokens\n",
      "2021-12-13 05:09:59 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:10:00 - Truncated by 5142 tokens\n",
      "2021-12-13 05:10:18 - Truncated by 536 tokens\n",
      "2021-12-13 05:10:20 - Truncated by 1277 tokens\n",
      "2021-12-13 05:10:35 - Truncated by 609 tokens\n",
      "2021-12-13 05:10:35 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:10:36 - Truncated by 610 tokens\n",
      "2021-12-13 05:10:42 - Truncated by 531 tokens\n",
      "2021-12-13 05:10:42 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:10:43 - Truncated by 532 tokens\n",
      "2021-12-13 05:10:48 - Truncated by 477 tokens\n",
      "2021-12-13 05:10:48 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:10:49 - Truncated by 478 tokens\n",
      "2021-12-13 05:10:54 - Truncated by 925 tokens\n",
      "2021-12-13 05:10:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:10:56 - Truncated by 926 tokens\n",
      "2021-12-13 05:10:59 - Truncated by 466 tokens\n",
      "2021-12-13 05:10:59 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:11:00 - Truncated by 467 tokens\n",
      "2021-12-13 05:11:11 - Truncated by 164 tokens\n",
      "2021-12-13 05:12:09 - Truncated by 1831 tokens\n",
      "2021-12-13 05:12:17 - Truncated by 155 tokens\n",
      "2021-12-13 05:12:18 - Truncated by 70 tokens\n",
      "2021-12-13 05:12:24 - Truncated by 482 tokens\n",
      "2021-12-13 05:12:25 - Truncated by 1019 tokens\n",
      "2021-12-13 05:12:38 - Truncated by 765 tokens\n",
      "2021-12-13 05:12:56 - Truncated by 9 tokens\n",
      "2021-12-13 05:12:56 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2050 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:12:57 - Truncated by 10 tokens\n",
      "2021-12-13 05:13:07 - Truncated by 132 tokens\n",
      "2021-12-13 05:13:07 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:13:08 - Truncated by 133 tokens\n",
      "2021-12-13 05:13:08 - Retrying after 3 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:13:11 - Truncated by 133 tokens\n",
      "2021-12-13 05:13:11 - Retrying after 5 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:13:16 - Truncated by 133 tokens\n",
      "2021-12-13 05:13:16 - Retrying after 7 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:13:23 - Truncated by 133 tokens\n",
      "2021-12-13 05:13:23 - Retrying after 9 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:13:32 - Truncated by 133 tokens\n",
      "2021-12-13 05:13:32 - Retrying after 11 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:13:43 - Truncated by 133 tokens\n",
      "2021-12-13 05:13:44 - Retrying after 13 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:13:57 - Truncated by 133 tokens\n",
      "2021-12-13 05:13:57 - Retrying after 15 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:14:12 - Truncated by 133 tokens\n",
      "2021-12-13 05:14:12 - Retrying after 17 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:14:29 - Truncated by 133 tokens\n",
      "2021-12-13 05:14:29 - Retrying after 19 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:14:48 - Truncated by 133 tokens\n",
      "2021-12-13 05:15:00 - Truncated by 116 tokens\n",
      "2021-12-13 05:15:00 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2050 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:15:01 - Truncated by 117 tokens\n",
      "2021-12-13 05:15:01 - Retrying after 3 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:15:04 - Truncated by 117 tokens\n",
      "2021-12-13 05:15:05 - Retrying after 5 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:15:10 - Truncated by 117 tokens\n",
      "2021-12-13 05:15:10 - Retrying after 7 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:15:17 - Truncated by 117 tokens\n",
      "2021-12-13 05:15:17 - Retrying after 9 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:15:26 - Truncated by 117 tokens\n",
      "2021-12-13 05:15:26 - Retrying after 11 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:15:37 - Truncated by 117 tokens\n",
      "2021-12-13 05:15:37 - Retrying after 13 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:15:50 - Truncated by 117 tokens\n",
      "2021-12-13 05:15:50 - Retrying after 15 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:16:05 - Truncated by 117 tokens\n",
      "2021-12-13 05:16:05 - Retrying after 17 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:16:22 - Truncated by 117 tokens\n",
      "2021-12-13 05:16:22 - Retrying after 19 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:16:41 - Truncated by 117 tokens\n",
      "2021-12-13 05:16:46 - Truncated by 1614 tokens\n",
      "2021-12-13 05:16:47 - Truncated by 690 tokens\n",
      "2021-12-13 05:16:47 - Truncated by 1847 tokens\n",
      "2021-12-13 05:16:48 - Truncated by 1665 tokens\n",
      "2021-12-13 05:16:52 - Truncated by 3612 tokens\n",
      "2021-12-13 05:16:52 - Truncated by 756 tokens\n",
      "2021-12-13 05:16:57 - Truncated by 1643 tokens\n",
      "2021-12-13 05:17:01 - Truncated by 2155 tokens\n",
      "2021-12-13 05:17:10 - Truncated by 2873 tokens\n",
      "2021-12-13 05:17:10 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:11 - Truncated by 2874 tokens\n",
      "2021-12-13 05:17:12 - Truncated by 887 tokens\n",
      "2021-12-13 05:17:12 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:13 - Truncated by 888 tokens\n",
      "2021-12-13 05:17:14 - Truncated by 88 tokens\n",
      "2021-12-13 05:17:15 - Truncated by 2009 tokens\n",
      "2021-12-13 05:17:15 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:16 - Truncated by 2010 tokens\n",
      "2021-12-13 05:17:18 - Truncated by 789 tokens\n",
      "2021-12-13 05:17:18 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:19 - Truncated by 790 tokens\n",
      "2021-12-13 05:17:19 - Truncated by 860 tokens\n",
      "2021-12-13 05:17:19 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:20 - Truncated by 861 tokens\n",
      "2021-12-13 05:17:21 - Truncated by 338 tokens\n",
      "2021-12-13 05:17:22 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:23 - Truncated by 339 tokens\n",
      "2021-12-13 05:17:24 - Truncated by 439 tokens\n",
      "2021-12-13 05:17:24 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:25 - Truncated by 440 tokens\n",
      "2021-12-13 05:17:26 - Truncated by 620 tokens\n",
      "2021-12-13 05:17:26 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:27 - Truncated by 621 tokens\n",
      "2021-12-13 05:17:28 - Truncated by 490 tokens\n",
      "2021-12-13 05:17:28 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:29 - Truncated by 491 tokens\n",
      "2021-12-13 05:17:34 - Truncated by 1291 tokens\n",
      "2021-12-13 05:17:35 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:36 - Truncated by 1292 tokens\n",
      "2021-12-13 05:17:37 - Truncated by 1466 tokens\n",
      "2021-12-13 05:17:37 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:38 - Truncated by 1467 tokens\n",
      "2021-12-13 05:17:39 - Truncated by 632 tokens\n",
      "2021-12-13 05:17:39 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:40 - Truncated by 633 tokens\n",
      "2021-12-13 05:17:41 - Truncated by 3223 tokens\n",
      "2021-12-13 05:17:41 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:42 - Truncated by 3224 tokens\n",
      "2021-12-13 05:17:49 - Truncated by 1636 tokens\n",
      "2021-12-13 05:17:49 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:50 - Truncated by 1637 tokens\n",
      "2021-12-13 05:17:52 - Truncated by 1865 tokens\n",
      "2021-12-13 05:17:52 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:53 - Truncated by 1866 tokens\n",
      "2021-12-13 05:17:55 - Truncated by 873 tokens\n",
      "2021-12-13 05:17:56 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:17:57 - Truncated by 874 tokens\n",
      "2021-12-13 05:18:03 - Truncated by 1909 tokens\n",
      "2021-12-13 05:18:03 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:18:04 - Truncated by 1910 tokens\n",
      "2021-12-13 05:18:09 - Truncated by 945 tokens\n",
      "2021-12-13 05:18:09 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:18:10 - Truncated by 946 tokens\n",
      "2021-12-13 05:18:16 - Truncated by 712 tokens\n",
      "2021-12-13 05:18:16 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:18:17 - Truncated by 713 tokens\n",
      "2021-12-13 05:18:26 - Truncated by 965 tokens\n",
      "2021-12-13 05:18:26 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:18:27 - Truncated by 966 tokens\n",
      "2021-12-13 05:18:32 - Truncated by 1743 tokens\n",
      "2021-12-13 05:18:32 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:18:33 - Truncated by 1744 tokens\n",
      "2021-12-13 05:18:35 - Truncated by 1647 tokens\n",
      "2021-12-13 05:18:35 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:18:36 - Truncated by 1648 tokens\n",
      "2021-12-13 05:18:37 - Truncated by 1060 tokens\n",
      "2021-12-13 05:18:38 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:18:39 - Truncated by 1061 tokens\n",
      "2021-12-13 05:18:39 - Truncated by 1762 tokens\n",
      "2021-12-13 05:18:45 - Truncated by 1388 tokens\n",
      "2021-12-13 05:18:45 - Truncated by 174 tokens\n",
      "2021-12-13 05:21:45 - Retrying after 1 seconds due to [Errno 408] {'error': 'Timeout waiting for task to finish', 'code': 'TIMEOUT_TASK'}\n",
      "2021-12-13 05:21:46 - Truncated by 175 tokens\n",
      "2021-12-13 05:21:50 - Truncated by 166 tokens\n",
      "2021-12-13 05:21:56 - Truncated by 2279 tokens\n",
      "2021-12-13 05:21:58 - Truncated by 2982 tokens\n",
      "2021-12-13 05:21:59 - Truncated by 1632 tokens\n",
      "2021-12-13 05:22:00 - Truncated by 2258 tokens\n",
      "2021-12-13 05:22:03 - Truncated by 234 tokens\n",
      "2021-12-13 05:22:05 - Truncated by 12 tokens\n",
      "2021-12-13 05:22:08 - Truncated by 2048 tokens\n",
      "2021-12-13 05:22:11 - Truncated by 259 tokens\n",
      "2021-12-13 05:22:25 - Truncated by 1765 tokens\n",
      "2021-12-13 05:22:28 - Truncated by 241 tokens\n",
      "2021-12-13 05:22:30 - Truncated by 159 tokens\n",
      "2021-12-13 05:22:32 - Truncated by 140 tokens\n",
      "2021-12-13 05:22:34 - Truncated by 478 tokens\n",
      "2021-12-13 05:22:35 - Truncated by 1231 tokens\n",
      "2021-12-13 05:22:37 - Truncated by 265 tokens\n",
      "2021-12-13 05:22:37 - Truncated by 6964 tokens\n",
      "2021-12-13 05:22:44 - Truncated by 1310 tokens\n",
      "2021-12-13 05:22:48 - Truncated by 209 tokens\n",
      "2021-12-13 05:22:52 - Truncated by 261 tokens\n",
      "2021-12-13 05:22:53 - Truncated by 1438 tokens\n",
      "2021-12-13 05:22:53 - Truncated by 243 tokens\n",
      "2021-12-13 05:22:54 - Truncated by 5360 tokens\n",
      "2021-12-13 05:22:58 - Truncated by 942 tokens\n",
      "2021-12-13 05:23:04 - Truncated by 663 tokens\n",
      "2021-12-13 05:23:11 - Truncated by 2553 tokens\n",
      "2021-12-13 05:23:22 - Truncated by 2379 tokens\n",
      "2021-12-13 05:23:27 - Truncated by 254 tokens\n",
      "2021-12-13 05:23:29 - Truncated by 485 tokens\n",
      "2021-12-13 05:23:31 - Truncated by 430 tokens\n",
      "2021-12-13 05:23:47 - Truncated by 982 tokens\n",
      "2021-12-13 05:23:51 - Truncated by 2550 tokens\n",
      "2021-12-13 05:23:55 - Truncated by 670 tokens\n",
      "2021-12-13 05:23:56 - Truncated by 992 tokens\n",
      "2021-12-13 05:24:06 - Truncated by 589 tokens\n",
      "2021-12-13 05:24:09 - Truncated by 1918 tokens\n",
      "2021-12-13 05:24:12 - Truncated by 253 tokens\n",
      "2021-12-13 05:24:14 - Truncated by 63 tokens\n",
      "2021-12-13 05:24:22 - Truncated by 86 tokens\n",
      "2021-12-13 05:24:25 - Truncated by 25 tokens\n",
      "2021-12-13 05:24:26 - Truncated by 334 tokens\n",
      "2021-12-13 05:24:48 - Truncated by 100 tokens\n",
      "2021-12-13 05:24:53 - Truncated by 2014 tokens\n",
      "2021-12-13 05:24:59 - Truncated by 10 tokens\n",
      "2021-12-13 05:25:08 - Truncated by 110 tokens\n",
      "2021-12-13 05:25:09 - Truncated by 611 tokens\n",
      "2021-12-13 05:25:14 - Truncated by 1112 tokens\n",
      "2021-12-13 05:25:21 - Truncated by 1583 tokens\n",
      "2021-12-13 05:25:34 - Truncated by 193 tokens\n",
      "2021-12-13 05:25:36 - Truncated by 1029 tokens\n",
      "2021-12-13 05:25:38 - Truncated by 485 tokens\n",
      "2021-12-13 05:25:41 - Truncated by 192 tokens\n",
      "2021-12-13 05:25:41 - Truncated by 2267 tokens\n",
      "2021-12-13 05:25:51 - Truncated by 379 tokens\n",
      "2021-12-13 05:25:55 - Truncated by 896 tokens\n",
      "2021-12-13 05:25:57 - Truncated by 1163 tokens\n",
      "2021-12-13 05:25:58 - Truncated by 1163 tokens\n",
      "2021-12-13 05:25:59 - Truncated by 1163 tokens\n",
      "2021-12-13 05:26:02 - Truncated by 696 tokens\n",
      "2021-12-13 05:26:03 - Truncated by 3186 tokens\n",
      "2021-12-13 05:26:05 - Truncated by 107 tokens\n",
      "2021-12-13 05:26:05 - Truncated by 518 tokens\n",
      "2021-12-13 05:26:09 - Truncated by 6 tokens\n",
      "2021-12-13 05:26:13 - Truncated by 325 tokens\n",
      "2021-12-13 05:26:17 - Truncated by 809 tokens\n",
      "2021-12-13 05:26:19 - Truncated by 1493 tokens\n",
      "2021-12-13 05:26:20 - Truncated by 8020 tokens\n",
      "2021-12-13 05:26:24 - Truncated by 1784 tokens\n",
      "2021-12-13 05:26:30 - Truncated by 3591 tokens\n",
      "2021-12-13 05:26:39 - Truncated by 4171 tokens\n",
      "2021-12-13 05:26:42 - Truncated by 18413 tokens\n",
      "2021-12-13 05:26:42 - Truncated by 4222 tokens\n",
      "2021-12-13 05:26:43 - Truncated by 28 tokens\n",
      "2021-12-13 05:26:47 - Truncated by 2630 tokens\n",
      "2021-12-13 05:26:53 - Truncated by 9 tokens\n",
      "2021-12-13 05:26:54 - Truncated by 236 tokens\n",
      "2021-12-13 05:27:01 - Truncated by 166 tokens\n",
      "2021-12-13 05:27:05 - Truncated by 5748 tokens\n",
      "2021-12-13 05:27:06 - Truncated by 1749 tokens\n",
      "2021-12-13 05:27:08 - Truncated by 3800 tokens\n",
      "2021-12-13 05:27:09 - Truncated by 10 tokens\n",
      "2021-12-13 05:27:10 - Truncated by 10 tokens\n",
      "2021-12-13 05:27:12 - Truncated by 812 tokens\n",
      "2021-12-13 05:27:24 - Truncated by 135 tokens\n",
      "2021-12-13 05:27:25 - Truncated by 769 tokens\n",
      "2021-12-13 05:27:27 - Truncated by 1961 tokens\n",
      "2021-12-13 05:27:31 - Truncated by 238 tokens\n",
      "2021-12-13 05:27:31 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:27:32 - Truncated by 239 tokens\n",
      "2021-12-13 05:27:42 - Truncated by 253 tokens\n",
      "2021-12-13 05:27:43 - Truncated by 579 tokens\n",
      "2021-12-13 05:27:44 - Truncated by 589 tokens\n",
      "2021-12-13 05:27:46 - Truncated by 346 tokens\n",
      "2021-12-13 05:27:47 - Truncated by 601 tokens\n",
      "2021-12-13 05:27:49 - Truncated by 1619 tokens\n",
      "2021-12-13 05:27:51 - Truncated by 25 tokens\n",
      "2021-12-13 05:27:52 - Truncated by 929 tokens\n",
      "2021-12-13 05:27:55 - Truncated by 1344 tokens\n",
      "2021-12-13 05:27:55 - Truncated by 1143 tokens\n",
      "2021-12-13 05:27:58 - Truncated by 280 tokens\n",
      "2021-12-13 05:27:58 - Truncated by 2413 tokens\n",
      "2021-12-13 05:28:01 - Truncated by 171 tokens\n",
      "2021-12-13 05:28:04 - Truncated by 4335 tokens\n",
      "2021-12-13 05:28:04 - Truncated by 3497 tokens\n",
      "2021-12-13 05:28:05 - Truncated by 2793 tokens\n",
      "2021-12-13 05:28:07 - Truncated by 131 tokens\n",
      "2021-12-13 05:28:12 - Truncated by 33 tokens\n",
      "2021-12-13 05:28:13 - Truncated by 1646 tokens\n",
      "2021-12-13 05:28:16 - Truncated by 3636 tokens\n",
      "2021-12-13 05:28:18 - Truncated by 1975 tokens\n",
      "2021-12-13 05:28:20 - Truncated by 259 tokens\n",
      "2021-12-13 05:28:20 - Truncated by 173 tokens\n",
      "2021-12-13 05:28:24 - Truncated by 665 tokens\n",
      "2021-12-13 05:28:30 - Truncated by 413 tokens\n",
      "2021-12-13 05:28:37 - Truncated by 416 tokens\n",
      "2021-12-13 05:28:41 - Truncated by 3691 tokens\n",
      "2021-12-13 05:28:46 - Truncated by 446 tokens\n",
      "2021-12-13 05:28:47 - Truncated by 476 tokens\n",
      "2021-12-13 05:28:49 - Truncated by 10625 tokens\n",
      "2021-12-13 05:28:56 - Truncated by 827 tokens\n",
      "2021-12-13 05:29:04 - Truncated by 4294 tokens\n",
      "2021-12-13 05:29:07 - Truncated by 212 tokens\n",
      "2021-12-13 05:29:09 - Truncated by 5449 tokens\n",
      "2021-12-13 05:29:11 - Truncated by 641 tokens\n",
      "2021-12-13 05:29:17 - Truncated by 129 tokens\n",
      "2021-12-13 05:29:20 - Truncated by 989 tokens\n",
      "2021-12-13 05:29:21 - Truncated by 1178 tokens\n",
      "2021-12-13 05:29:22 - Truncated by 77 tokens\n",
      "2021-12-13 05:29:23 - Truncated by 885 tokens\n",
      "2021-12-13 05:29:39 - Truncated by 296 tokens\n",
      "2021-12-13 05:29:40 - Truncated by 528 tokens\n",
      "2021-12-13 05:29:41 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:29:42 - Truncated by 529 tokens\n",
      "2021-12-13 05:29:59 - Truncated by 1069 tokens\n",
      "2021-12-13 05:29:59 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:30:00 - Truncated by 1070 tokens\n",
      "2021-12-13 05:30:18 - Truncated by 633 tokens\n",
      "2021-12-13 05:30:19 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:30:20 - Truncated by 634 tokens\n",
      "2021-12-13 05:30:20 - Truncated by 583 tokens\n",
      "2021-12-13 05:30:20 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:30:21 - Truncated by 584 tokens\n",
      "2021-12-13 05:30:36 - Truncated by 232 tokens\n",
      "2021-12-13 05:30:37 - Truncated by 14 tokens\n",
      "2021-12-13 05:30:39 - Truncated by 1654 tokens\n",
      "2021-12-13 05:30:44 - Truncated by 33 tokens\n",
      "2021-12-13 05:30:54 - Truncated by 938 tokens\n",
      "2021-12-13 05:30:57 - Truncated by 162 tokens\n",
      "2021-12-13 05:30:58 - Truncated by 1041 tokens\n",
      "2021-12-13 05:31:07 - Truncated by 347 tokens\n",
      "2021-12-13 05:31:09 - Truncated by 2216 tokens\n",
      "2021-12-13 05:31:11 - Truncated by 698 tokens\n",
      "2021-12-13 05:31:11 - Truncated by 1238 tokens\n",
      "2021-12-13 05:31:16 - Truncated by 967 tokens\n",
      "2021-12-13 05:31:17 - Truncated by 769 tokens\n",
      "2021-12-13 05:31:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:31:18 - Truncated by 770 tokens\n",
      "2021-12-13 05:31:20 - Truncated by 2007 tokens\n",
      "2021-12-13 05:31:20 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:31:21 - Truncated by 2008 tokens\n",
      "2021-12-13 05:31:27 - Truncated by 5232 tokens\n",
      "2021-12-13 05:31:29 - Truncated by 837 tokens\n",
      "2021-12-13 05:31:29 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:31:30 - Truncated by 838 tokens\n",
      "2021-12-13 05:31:32 - Truncated by 1007 tokens\n",
      "2021-12-13 05:31:32 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:31:33 - Truncated by 1008 tokens\n",
      "2021-12-13 05:31:38 - Truncated by 89 tokens\n",
      "2021-12-13 05:31:38 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:31:39 - Truncated by 90 tokens\n",
      "2021-12-13 05:32:01 - Truncated by 4617 tokens\n",
      "2021-12-13 05:32:06 - Truncated by 693 tokens\n",
      "2021-12-13 05:32:14 - Truncated by 1315 tokens\n",
      "2021-12-13 05:32:14 - Truncated by 144 tokens\n",
      "2021-12-13 05:32:16 - Truncated by 2449 tokens\n",
      "2021-12-13 05:32:21 - Truncated by 526 tokens\n",
      "2021-12-13 05:32:22 - Truncated by 845 tokens\n",
      "2021-12-13 05:32:27 - Truncated by 377 tokens\n",
      "2021-12-13 05:32:28 - Truncated by 3794 tokens\n",
      "2021-12-13 05:32:28 - Truncated by 5 tokens\n",
      "2021-12-13 05:32:30 - Truncated by 7145 tokens\n",
      "2021-12-13 05:32:31 - Truncated by 247 tokens\n",
      "2021-12-13 05:32:32 - Truncated by 1148 tokens\n",
      "2021-12-13 05:32:34 - Truncated by 360 tokens\n",
      "2021-12-13 05:32:38 - Truncated by 651 tokens\n",
      "2021-12-13 05:32:39 - Truncated by 253 tokens\n",
      "2021-12-13 05:32:46 - Truncated by 437 tokens\n",
      "2021-12-13 05:32:47 - Truncated by 1458 tokens\n",
      "2021-12-13 05:32:51 - Truncated by 1092 tokens\n",
      "2021-12-13 05:32:56 - Truncated by 1016 tokens\n",
      "2021-12-13 05:32:58 - Truncated by 1735 tokens\n",
      "2021-12-13 05:32:58 - Truncated by 21 tokens\n",
      "2021-12-13 05:33:00 - Truncated by 1008 tokens\n",
      "2021-12-13 05:33:02 - Truncated by 1664 tokens\n",
      "2021-12-13 05:33:04 - Truncated by 2282 tokens\n",
      "2021-12-13 05:33:09 - Truncated by 250 tokens\n",
      "2021-12-13 05:33:10 - Truncated by 2819 tokens\n",
      "2021-12-13 05:33:12 - Truncated by 4614 tokens\n",
      "2021-12-13 05:33:13 - Truncated by 4614 tokens\n",
      "2021-12-13 05:33:15 - Truncated by 1084 tokens\n",
      "2021-12-13 05:33:19 - Truncated by 585 tokens\n",
      "2021-12-13 05:33:22 - Truncated by 1857 tokens\n",
      "2021-12-13 05:33:22 - Truncated by 1130 tokens\n",
      "2021-12-13 05:33:24 - Truncated by 1070 tokens\n",
      "2021-12-13 05:33:25 - Truncated by 61 tokens\n",
      "2021-12-13 05:33:26 - Truncated by 294 tokens\n",
      "2021-12-13 05:33:27 - Truncated by 720 tokens\n",
      "2021-12-13 05:33:27 - Truncated by 330 tokens\n",
      "2021-12-13 05:33:28 - Truncated by 353 tokens\n",
      "2021-12-13 05:33:29 - Truncated by 353 tokens\n",
      "2021-12-13 05:33:30 - Truncated by 605 tokens\n",
      "2021-12-13 05:33:33 - Truncated by 239 tokens\n",
      "2021-12-13 05:33:34 - Truncated by 3939 tokens\n",
      "2021-12-13 05:33:38 - Truncated by 817 tokens\n",
      "2021-12-13 05:33:41 - Truncated by 630 tokens\n",
      "2021-12-13 05:33:41 - Truncated by 859 tokens\n",
      "2021-12-13 05:33:44 - Truncated by 588 tokens\n",
      "2021-12-13 05:33:45 - Truncated by 1158 tokens\n",
      "2021-12-13 05:33:46 - Truncated by 3927 tokens\n",
      "2021-12-13 05:33:47 - Truncated by 3152 tokens\n",
      "2021-12-13 05:33:48 - Truncated by 4898 tokens\n",
      "2021-12-13 05:33:49 - Truncated by 1553 tokens\n",
      "2021-12-13 05:33:52 - Truncated by 1022 tokens\n",
      "2021-12-13 05:34:01 - Truncated by 705 tokens\n",
      "2021-12-13 05:34:14 - Truncated by 241 tokens\n",
      "2021-12-13 05:34:22 - Truncated by 220 tokens\n",
      "2021-12-13 05:34:22 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:23 - Truncated by 221 tokens\n",
      "2021-12-13 05:34:24 - Truncated by 176 tokens\n",
      "2021-12-13 05:34:24 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:25 - Truncated by 177 tokens\n",
      "2021-12-13 05:34:34 - Truncated by 686 tokens\n",
      "2021-12-13 05:34:34 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:35 - Truncated by 687 tokens\n",
      "2021-12-13 05:34:36 - Truncated by 129 tokens\n",
      "2021-12-13 05:34:36 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:37 - Truncated by 130 tokens\n",
      "2021-12-13 05:34:42 - Truncated by 224 tokens\n",
      "2021-12-13 05:34:44 - Truncated by 857 tokens\n",
      "2021-12-13 05:34:44 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:45 - Truncated by 858 tokens\n",
      "2021-12-13 05:34:46 - Truncated by 1447 tokens\n",
      "2021-12-13 05:34:46 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:47 - Truncated by 1448 tokens\n",
      "2021-12-13 05:34:49 - Truncated by 2285 tokens\n",
      "2021-12-13 05:34:50 - Truncated by 81 tokens\n",
      "2021-12-13 05:34:50 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:51 - Truncated by 82 tokens\n",
      "2021-12-13 05:34:55 - Truncated by 68 tokens\n",
      "2021-12-13 05:34:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:56 - Truncated by 69 tokens\n",
      "2021-12-13 05:34:58 - Truncated by 275 tokens\n",
      "2021-12-13 05:34:58 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:34:59 - Truncated by 276 tokens\n",
      "2021-12-13 05:35:00 - Truncated by 500 tokens\n",
      "2021-12-13 05:35:00 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:35:01 - Truncated by 501 tokens\n",
      "2021-12-13 05:35:04 - Truncated by 2423 tokens\n",
      "2021-12-13 05:35:11 - Truncated by 8474 tokens\n",
      "2021-12-13 05:35:19 - Truncated by 128 tokens\n",
      "2021-12-13 05:35:20 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:35:21 - Truncated by 129 tokens\n",
      "2021-12-13 05:35:22 - Truncated by 116293 tokens\n",
      "2021-12-13 05:35:28 - Truncated by 22252 tokens\n",
      "2021-12-13 05:35:28 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:35:29 - Truncated by 22253 tokens\n",
      "2021-12-13 05:35:34 - Truncated by 3235 tokens\n",
      "2021-12-13 05:35:48 - Truncated by 14 tokens\n",
      "2021-12-13 05:35:48 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:35:49 - Truncated by 15 tokens\n",
      "2021-12-13 05:35:52 - Truncated by 1670 tokens\n",
      "2021-12-13 05:35:52 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:35:53 - Truncated by 1671 tokens\n",
      "2021-12-13 05:35:54 - Truncated by 82 tokens\n",
      "2021-12-13 05:35:54 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:35:55 - Truncated by 83 tokens\n",
      "2021-12-13 05:35:57 - Truncated by 182 tokens\n",
      "2021-12-13 05:35:58 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:35:59 - Truncated by 183 tokens\n",
      "2021-12-13 05:36:00 - Truncated by 160 tokens\n",
      "2021-12-13 05:36:00 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:01 - Truncated by 161 tokens\n",
      "2021-12-13 05:36:03 - Truncated by 230 tokens\n",
      "2021-12-13 05:36:03 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:04 - Truncated by 231 tokens\n",
      "2021-12-13 05:36:08 - Truncated by 333 tokens\n",
      "2021-12-13 05:36:08 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:09 - Truncated by 334 tokens\n",
      "2021-12-13 05:36:09 - Truncated by 882 tokens\n",
      "2021-12-13 05:36:09 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:10 - Truncated by 883 tokens\n",
      "2021-12-13 05:36:11 - Truncated by 361 tokens\n",
      "2021-12-13 05:36:11 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:12 - Truncated by 362 tokens\n",
      "2021-12-13 05:36:12 - Truncated by 905 tokens\n",
      "2021-12-13 05:36:12 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:13 - Truncated by 906 tokens\n",
      "2021-12-13 05:36:17 - Truncated by 2054 tokens\n",
      "2021-12-13 05:36:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:18 - Truncated by 2055 tokens\n",
      "2021-12-13 05:36:21 - Truncated by 889 tokens\n",
      "2021-12-13 05:36:24 - Truncated by 906 tokens\n",
      "2021-12-13 05:36:25 - Truncated by 974 tokens\n",
      "2021-12-13 05:36:25 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:26 - Truncated by 975 tokens\n",
      "2021-12-13 05:36:31 - Truncated by 1175 tokens\n",
      "2021-12-13 05:36:31 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:32 - Truncated by 1176 tokens\n",
      "2021-12-13 05:36:35 - Truncated by 542 tokens\n",
      "2021-12-13 05:36:35 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:36 - Truncated by 543 tokens\n",
      "2021-12-13 05:36:37 - Truncated by 928 tokens\n",
      "2021-12-13 05:36:37 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:38 - Truncated by 929 tokens\n",
      "2021-12-13 05:36:41 - Truncated by 985 tokens\n",
      "2021-12-13 05:36:41 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:42 - Truncated by 986 tokens\n",
      "2021-12-13 05:36:42 - Truncated by 49 tokens\n",
      "2021-12-13 05:36:43 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:44 - Truncated by 50 tokens\n",
      "2021-12-13 05:36:45 - Truncated by 884 tokens\n",
      "2021-12-13 05:36:45 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:46 - Truncated by 885 tokens\n",
      "2021-12-13 05:36:47 - Truncated by 300 tokens\n",
      "2021-12-13 05:36:47 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:48 - Truncated by 301 tokens\n",
      "2021-12-13 05:36:49 - Truncated by 266 tokens\n",
      "2021-12-13 05:36:49 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:50 - Truncated by 267 tokens\n",
      "2021-12-13 05:36:51 - Truncated by 481 tokens\n",
      "2021-12-13 05:36:52 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:36:53 - Truncated by 482 tokens\n",
      "2021-12-13 05:36:56 - Truncated by 525 tokens\n",
      "2021-12-13 05:37:02 - Truncated by 714 tokens\n",
      "2021-12-13 05:37:13 - Truncated by 526 tokens\n",
      "2021-12-13 05:37:21 - Truncated by 1212 tokens\n",
      "2021-12-13 05:37:22 - Truncated by 262 tokens\n",
      "2021-12-13 05:37:25 - Truncated by 1133 tokens\n",
      "2021-12-13 05:37:25 - Truncated by 2 tokens\n",
      "2021-12-13 05:37:28 - Truncated by 602 tokens\n",
      "2021-12-13 05:37:33 - Truncated by 286 tokens\n",
      "2021-12-13 05:37:39 - Truncated by 5350 tokens\n",
      "2021-12-13 05:37:41 - Truncated by 12044 tokens\n",
      "2021-12-13 05:37:43 - Truncated by 142 tokens\n",
      "2021-12-13 05:37:46 - Truncated by 943 tokens\n",
      "2021-12-13 05:37:47 - Truncated by 2152 tokens\n",
      "2021-12-13 05:37:48 - Truncated by 402 tokens\n",
      "2021-12-13 05:37:49 - Truncated by 235 tokens\n",
      "2021-12-13 05:37:52 - Truncated by 12401 tokens\n",
      "2021-12-13 05:37:53 - Truncated by 131 tokens\n",
      "2021-12-13 05:37:54 - Truncated by 929 tokens\n",
      "2021-12-13 05:37:56 - Truncated by 346 tokens\n",
      "2021-12-13 05:38:00 - Truncated by 323 tokens\n",
      "2021-12-13 05:38:01 - Truncated by 781 tokens\n",
      "2021-12-13 05:38:02 - Truncated by 30771 tokens\n",
      "2021-12-13 05:38:05 - Truncated by 206 tokens\n",
      "2021-12-13 05:38:17 - Truncated by 30 tokens\n",
      "2021-12-13 05:38:18 - Truncated by 1196 tokens\n",
      "2021-12-13 05:38:21 - Truncated by 7254 tokens\n",
      "2021-12-13 05:38:22 - Truncated by 443 tokens\n",
      "2021-12-13 05:38:23 - Truncated by 549 tokens\n",
      "2021-12-13 05:38:25 - Truncated by 3181 tokens\n",
      "2021-12-13 05:38:28 - Truncated by 619 tokens\n",
      "2021-12-13 05:38:33 - Truncated by 119 tokens\n",
      "2021-12-13 05:38:34 - Truncated by 7299 tokens\n",
      "2021-12-13 05:38:34 - Truncated by 1969 tokens\n",
      "2021-12-13 05:38:36 - Truncated by 1161 tokens\n",
      "2021-12-13 05:38:36 - Truncated by 286 tokens\n",
      "2021-12-13 05:38:39 - Truncated by 489 tokens\n",
      "2021-12-13 05:38:43 - Truncated by 63 tokens\n",
      "2021-12-13 05:38:46 - Truncated by 87 tokens\n",
      "2021-12-13 05:38:48 - Truncated by 2675 tokens\n",
      "2021-12-13 05:38:52 - Truncated by 639 tokens\n",
      "2021-12-13 05:38:54 - Truncated by 12044 tokens\n",
      "2021-12-13 05:38:54 - Truncated by 434 tokens\n",
      "2021-12-13 05:39:00 - Truncated by 101 tokens\n",
      "2021-12-13 05:39:01 - Truncated by 1220 tokens\n",
      "2021-12-13 05:39:06 - Truncated by 664 tokens\n",
      "2021-12-13 05:39:06 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:39:07 - Truncated by 665 tokens\n",
      "2021-12-13 05:39:14 - Truncated by 3019 tokens\n",
      "2021-12-13 05:39:17 - Truncated by 1320 tokens\n",
      "2021-12-13 05:39:18 - Truncated by 518 tokens\n",
      "2021-12-13 05:39:18 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:39:19 - Truncated by 519 tokens\n",
      "2021-12-13 05:39:20 - Truncated by 593 tokens\n",
      "2021-12-13 05:39:21 - Truncated by 594 tokens\n",
      "2021-12-13 05:39:22 - Truncated by 584 tokens\n",
      "2021-12-13 05:39:24 - Truncated by 7161 tokens\n",
      "2021-12-13 05:39:25 - Truncated by 990 tokens\n",
      "2021-12-13 05:39:26 - Truncated by 953 tokens\n",
      "2021-12-13 05:39:26 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:39:27 - Truncated by 954 tokens\n",
      "2021-12-13 05:39:29 - Truncated by 4423 tokens\n",
      "2021-12-13 05:39:31 - Truncated by 1795 tokens\n",
      "2021-12-13 05:39:38 - Truncated by 300 tokens\n",
      "2021-12-13 05:39:44 - Truncated by 1164 tokens\n",
      "2021-12-13 05:39:46 - Truncated by 2417 tokens\n",
      "2021-12-13 05:39:52 - Truncated by 1107 tokens\n",
      "2021-12-13 05:39:56 - Truncated by 463 tokens\n",
      "2021-12-13 05:39:57 - Truncated by 872 tokens\n",
      "2021-12-13 05:39:59 - Truncated by 573 tokens\n",
      "2021-12-13 05:40:02 - Truncated by 516 tokens\n",
      "2021-12-13 05:40:04 - Truncated by 138 tokens\n",
      "2021-12-13 05:40:06 - Truncated by 469 tokens\n",
      "2021-12-13 05:40:07 - Truncated by 113 tokens\n",
      "2021-12-13 05:40:14 - Truncated by 251 tokens\n",
      "2021-12-13 05:40:18 - Truncated by 235 tokens\n",
      "2021-12-13 05:40:25 - Truncated by 268 tokens\n",
      "2021-12-13 05:40:26 - Truncated by 2196 tokens\n",
      "2021-12-13 05:40:28 - Truncated by 1192 tokens\n",
      "2021-12-13 05:40:29 - Truncated by 992 tokens\n",
      "2021-12-13 05:40:29 - Truncated by 320 tokens\n",
      "2021-12-13 05:40:32 - Truncated by 7529 tokens\n",
      "2021-12-13 05:40:36 - Truncated by 249 tokens\n",
      "2021-12-13 05:40:38 - Truncated by 4698 tokens\n",
      "2021-12-13 05:40:39 - Truncated by 1555 tokens\n",
      "2021-12-13 05:40:42 - Truncated by 640 tokens\n",
      "2021-12-13 05:40:43 - Truncated by 2560 tokens\n",
      "2021-12-13 05:40:43 - Truncated by 29838 tokens\n",
      "2021-12-13 05:40:45 - Truncated by 7093 tokens\n",
      "2021-12-13 05:40:46 - Truncated by 169 tokens\n",
      "2021-12-13 05:40:47 - Truncated by 347 tokens\n",
      "2021-12-13 05:40:48 - Truncated by 2340 tokens\n",
      "2021-12-13 05:40:49 - Truncated by 598 tokens\n",
      "2021-12-13 05:40:50 - Truncated by 576 tokens\n",
      "2021-12-13 05:40:52 - Truncated by 47 tokens\n",
      "2021-12-13 05:40:54 - Truncated by 2251 tokens\n",
      "2021-12-13 05:40:56 - Truncated by 281 tokens\n",
      "2021-12-13 05:40:56 - Truncated by 3095 tokens\n",
      "2021-12-13 05:40:57 - Truncated by 8368 tokens\n",
      "2021-12-13 05:40:58 - Truncated by 4776 tokens\n",
      "2021-12-13 05:41:00 - Truncated by 486 tokens\n",
      "2021-12-13 05:41:01 - Truncated by 35 tokens\n",
      "2021-12-13 05:41:02 - Truncated by 673 tokens\n",
      "2021-12-13 05:41:03 - Truncated by 430 tokens\n",
      "2021-12-13 05:41:03 - Truncated by 2074 tokens\n",
      "2021-12-13 05:41:04 - Truncated by 703 tokens\n",
      "2021-12-13 05:41:06 - Truncated by 19677 tokens\n",
      "2021-12-13 05:41:08 - Truncated by 5650 tokens\n",
      "2021-12-13 05:41:09 - Truncated by 4506 tokens\n",
      "2021-12-13 05:41:10 - Truncated by 2016 tokens\n",
      "2021-12-13 05:41:11 - Truncated by 1994 tokens\n",
      "2021-12-13 05:41:11 - Truncated by 1464 tokens\n",
      "2021-12-13 05:41:14 - Truncated by 1354 tokens\n",
      "2021-12-13 05:41:14 - Truncated by 8461 tokens\n",
      "2021-12-13 05:41:15 - Truncated by 3630 tokens\n",
      "2021-12-13 05:41:18 - Truncated by 540 tokens\n",
      "2021-12-13 05:41:19 - Truncated by 1466 tokens\n",
      "2021-12-13 05:41:20 - Truncated by 1228 tokens\n",
      "2021-12-13 05:41:20 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:21 - Truncated by 1229 tokens\n",
      "2021-12-13 05:41:25 - Truncated by 1804 tokens\n",
      "2021-12-13 05:41:25 - Truncated by 95 tokens\n",
      "2021-12-13 05:41:26 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:27 - Truncated by 96 tokens\n",
      "2021-12-13 05:41:27 - Truncated by 509 tokens\n",
      "2021-12-13 05:41:27 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:28 - Truncated by 510 tokens\n",
      "2021-12-13 05:41:29 - Truncated by 7 tokens\n",
      "2021-12-13 05:41:29 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:30 - Truncated by 8 tokens\n",
      "2021-12-13 05:41:36 - Truncated by 101 tokens\n",
      "2021-12-13 05:41:36 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:37 - Truncated by 102 tokens\n",
      "2021-12-13 05:41:43 - Truncated by 1279 tokens\n",
      "2021-12-13 05:41:50 - Truncated by 597 tokens\n",
      "2021-12-13 05:41:51 - Truncated by 57 tokens\n",
      "2021-12-13 05:41:52 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:53 - Truncated by 58 tokens\n",
      "2021-12-13 05:41:53 - Truncated by 914 tokens\n",
      "2021-12-13 05:41:53 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:54 - Truncated by 915 tokens\n",
      "2021-12-13 05:41:57 - Truncated by 310 tokens\n",
      "2021-12-13 05:41:57 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:41:58 - Truncated by 311 tokens\n",
      "2021-12-13 05:42:00 - Truncated by 757 tokens\n",
      "2021-12-13 05:42:00 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:42:01 - Truncated by 758 tokens\n",
      "2021-12-13 05:42:02 - Truncated by 2270 tokens\n",
      "2021-12-13 05:42:03 - Truncated by 2259 tokens\n",
      "2021-12-13 05:42:04 - Truncated by 352 tokens\n",
      "2021-12-13 05:42:04 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:42:05 - Truncated by 353 tokens\n",
      "2021-12-13 05:42:10 - Truncated by 970 tokens\n",
      "2021-12-13 05:42:15 - Truncated by 1090 tokens\n",
      "2021-12-13 05:42:16 - Truncated by 19 tokens\n",
      "2021-12-13 05:42:25 - Truncated by 335 tokens\n",
      "2021-12-13 05:42:29 - Truncated by 142 tokens\n",
      "2021-12-13 05:42:30 - Truncated by 409 tokens\n",
      "2021-12-13 05:42:30 - Truncated by 1010 tokens\n",
      "2021-12-13 05:42:31 - Truncated by 1149 tokens\n",
      "2021-12-13 05:42:34 - Truncated by 213 tokens\n",
      "2021-12-13 05:42:37 - Truncated by 294 tokens\n",
      "2021-12-13 05:42:44 - Truncated by 489 tokens\n",
      "2021-12-13 05:42:45 - Truncated by 8808 tokens\n",
      "2021-12-13 05:42:47 - Truncated by 4040 tokens\n",
      "2021-12-13 05:42:48 - Truncated by 80 tokens\n",
      "2021-12-13 05:42:50 - Truncated by 732 tokens\n",
      "2021-12-13 05:42:50 - Truncated by 96 tokens\n",
      "2021-12-13 05:42:51 - Truncated by 1158 tokens\n",
      "2021-12-13 05:43:01 - Truncated by 4307 tokens\n",
      "2021-12-13 05:43:20 - Truncated by 2837 tokens\n",
      "2021-12-13 05:43:20 - Truncated by 649 tokens\n",
      "2021-12-13 05:43:23 - Truncated by 1713 tokens\n",
      "2021-12-13 05:43:33 - Truncated by 1787 tokens\n",
      "2021-12-13 05:43:35 - Truncated by 1263 tokens\n",
      "2021-12-13 05:43:36 - Truncated by 467 tokens\n",
      "2021-12-13 05:43:37 - Truncated by 1421 tokens\n",
      "2021-12-13 05:43:42 - Truncated by 3010 tokens\n",
      "2021-12-13 05:43:43 - Truncated by 476 tokens\n",
      "2021-12-13 05:43:45 - Truncated by 6222 tokens\n",
      "2021-12-13 05:43:45 - Truncated by 394 tokens\n",
      "2021-12-13 05:43:46 - Truncated by 1176 tokens\n",
      "2021-12-13 05:43:47 - Truncated by 661 tokens\n",
      "2021-12-13 05:43:52 - Truncated by 1205 tokens\n",
      "2021-12-13 05:43:54 - Truncated by 260 tokens\n",
      "2021-12-13 05:43:56 - Truncated by 566 tokens\n",
      "2021-12-13 05:43:57 - Truncated by 386 tokens\n",
      "2021-12-13 05:43:58 - Truncated by 232 tokens\n",
      "2021-12-13 05:43:59 - Truncated by 1632 tokens\n",
      "2021-12-13 05:44:01 - Truncated by 32 tokens\n",
      "2021-12-13 05:44:02 - Truncated by 630 tokens\n",
      "2021-12-13 05:44:02 - Truncated by 695 tokens\n",
      "2021-12-13 05:44:03 - Truncated by 5181 tokens\n",
      "2021-12-13 05:44:05 - Truncated by 2692 tokens\n",
      "2021-12-13 05:44:05 - Truncated by 1529 tokens\n",
      "2021-12-13 05:44:07 - Truncated by 7568 tokens\n",
      "2021-12-13 05:44:09 - Truncated by 284 tokens\n",
      "2021-12-13 05:44:12 - Truncated by 133 tokens\n",
      "2021-12-13 05:44:14 - Truncated by 346 tokens\n",
      "2021-12-13 05:44:20 - Truncated by 6 tokens\n",
      "2021-12-13 05:44:21 - Truncated by 108 tokens\n",
      "2021-12-13 05:44:21 - Truncated by 812 tokens\n",
      "2021-12-13 05:44:23 - Truncated by 1001 tokens\n",
      "2021-12-13 05:44:27 - Truncated by 1180 tokens\n",
      "2021-12-13 05:44:34 - Truncated by 284 tokens\n",
      "2021-12-13 05:44:36 - Truncated by 308 tokens\n",
      "2021-12-13 05:44:46 - Truncated by 168 tokens\n",
      "2021-12-13 05:45:01 - Truncated by 234 tokens\n",
      "2021-12-13 05:45:02 - Truncated by 893 tokens\n",
      "2021-12-13 05:45:05 - Truncated by 1703 tokens\n",
      "2021-12-13 05:45:07 - Truncated by 3406 tokens\n",
      "2021-12-13 05:45:08 - Truncated by 3406 tokens\n",
      "2021-12-13 05:45:13 - Truncated by 693 tokens\n",
      "2021-12-13 05:45:14 - Truncated by 1135 tokens\n",
      "2021-12-13 05:45:15 - Truncated by 480 tokens\n",
      "2021-12-13 05:45:22 - Truncated by 1920 tokens\n",
      "2021-12-13 05:45:22 - Truncated by 2263 tokens\n",
      "2021-12-13 05:45:24 - Truncated by 40 tokens\n",
      "2021-12-13 05:45:25 - Truncated by 21 tokens\n",
      "2021-12-13 05:45:27 - Truncated by 1914 tokens\n",
      "2021-12-13 05:45:32 - Truncated by 300 tokens\n",
      "2021-12-13 05:45:32 - Truncated by 5662 tokens\n",
      "2021-12-13 05:45:37 - Truncated by 63 tokens\n",
      "2021-12-13 05:45:38 - Truncated by 340 tokens\n",
      "2021-12-13 05:45:39 - Truncated by 863 tokens\n",
      "2021-12-13 05:45:40 - Truncated by 210 tokens\n",
      "2021-12-13 05:45:43 - Truncated by 4183 tokens\n",
      "2021-12-13 05:45:44 - Truncated by 21 tokens\n",
      "2021-12-13 05:45:56 - Truncated by 817 tokens\n",
      "2021-12-13 05:45:58 - Truncated by 2226 tokens\n",
      "2021-12-13 05:46:07 - Truncated by 1500 tokens\n",
      "2021-12-13 05:46:08 - Truncated by 1454 tokens\n",
      "2021-12-13 05:46:09 - Truncated by 19550 tokens\n",
      "2021-12-13 05:46:13 - Truncated by 77139 tokens\n",
      "2021-12-13 05:46:16 - Truncated by 669 tokens\n",
      "2021-12-13 05:46:18 - Truncated by 3771 tokens\n",
      "2021-12-13 05:46:24 - Truncated by 3146 tokens\n",
      "2021-12-13 05:46:30 - Truncated by 3568 tokens\n",
      "2021-12-13 05:46:31 - Truncated by 234 tokens\n",
      "2021-12-13 05:46:33 - Truncated by 1798 tokens\n",
      "2021-12-13 05:46:34 - Truncated by 19519 tokens\n",
      "2021-12-13 05:46:42 - Truncated by 541 tokens\n",
      "2021-12-13 05:46:50 - Truncated by 444 tokens\n",
      "2021-12-13 05:46:56 - Truncated by 7344 tokens\n",
      "2021-12-13 05:46:57 - Truncated by 309 tokens\n",
      "2021-12-13 05:47:01 - Truncated by 1738 tokens\n",
      "2021-12-13 05:47:14 - Truncated by 27 tokens\n",
      "2021-12-13 05:47:16 - Truncated by 1909 tokens\n",
      "2021-12-13 05:47:21 - Truncated by 860 tokens\n",
      "2021-12-13 05:47:31 - Truncated by 270 tokens\n",
      "2021-12-13 05:47:32 - Truncated by 702 tokens\n",
      "2021-12-13 05:47:34 - Truncated by 141 tokens\n",
      "2021-12-13 05:47:44 - Truncated by 1726 tokens\n",
      "2021-12-13 05:47:50 - Truncated by 285 tokens\n",
      "2021-12-13 05:47:53 - Truncated by 623 tokens\n",
      "2021-12-13 05:47:53 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:47:54 - Truncated by 624 tokens\n",
      "2021-12-13 05:47:56 - Truncated by 2569 tokens\n",
      "2021-12-13 05:48:07 - Truncated by 1093 tokens\n",
      "2021-12-13 05:48:12 - Truncated by 431 tokens\n",
      "2021-12-13 05:48:17 - Truncated by 19 tokens\n",
      "2021-12-13 05:48:17 - Truncated by 880 tokens\n",
      "2021-12-13 05:48:18 - Truncated by 342 tokens\n",
      "2021-12-13 05:48:18 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:19 - Truncated by 343 tokens\n",
      "2021-12-13 05:48:21 - Truncated by 336 tokens\n",
      "2021-12-13 05:48:21 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:22 - Truncated by 337 tokens\n",
      "2021-12-13 05:48:29 - Truncated by 436 tokens\n",
      "2021-12-13 05:48:29 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:30 - Truncated by 437 tokens\n",
      "2021-12-13 05:48:32 - Truncated by 394 tokens\n",
      "2021-12-13 05:48:32 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:33 - Truncated by 395 tokens\n",
      "2021-12-13 05:48:33 - Truncated by 400 tokens\n",
      "2021-12-13 05:48:34 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:35 - Truncated by 401 tokens\n",
      "2021-12-13 05:48:40 - Truncated by 454 tokens\n",
      "2021-12-13 05:48:41 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:42 - Truncated by 455 tokens\n",
      "2021-12-13 05:48:43 - Truncated by 697 tokens\n",
      "2021-12-13 05:48:43 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:44 - Truncated by 698 tokens\n",
      "2021-12-13 05:48:46 - Truncated by 2314 tokens\n",
      "2021-12-13 05:48:48 - Truncated by 107 tokens\n",
      "2021-12-13 05:48:48 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:48:49 - Truncated by 108 tokens\n",
      "2021-12-13 05:48:50 - Truncated by 2907 tokens\n",
      "2021-12-13 05:48:52 - Truncated by 1690 tokens\n",
      "2021-12-13 05:48:53 - Truncated by 1415 tokens\n",
      "2021-12-13 05:48:54 - Truncated by 2746 tokens\n",
      "2021-12-13 05:48:57 - Truncated by 19613 tokens\n",
      "2021-12-13 05:49:03 - Truncated by 2407 tokens\n",
      "2021-12-13 05:49:06 - Truncated by 1128 tokens\n",
      "2021-12-13 05:49:29 - Truncated by 817 tokens\n",
      "2021-12-13 05:49:33 - Truncated by 1998 tokens\n",
      "2021-12-13 05:49:33 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:49:34 - Truncated by 1999 tokens\n",
      "2021-12-13 05:49:41 - Truncated by 27 tokens\n",
      "2021-12-13 05:49:41 - Truncated by 198 tokens\n",
      "2021-12-13 05:49:42 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:49:43 - Truncated by 199 tokens\n",
      "2021-12-13 05:50:01 - Truncated by 302 tokens\n",
      "2021-12-13 05:50:09 - Truncated by 5838 tokens\n",
      "2021-12-13 05:50:10 - Truncated by 10520 tokens\n",
      "2021-12-13 05:50:12 - Truncated by 908 tokens\n",
      "2021-12-13 05:50:17 - Truncated by 3060 tokens\n",
      "2021-12-13 05:50:19 - Truncated by 387 tokens\n",
      "2021-12-13 05:50:23 - Truncated by 2898 tokens\n",
      "2021-12-13 05:50:27 - Truncated by 3822 tokens\n",
      "2021-12-13 05:50:28 - Truncated by 7107 tokens\n",
      "2021-12-13 05:50:29 - Truncated by 39057 tokens\n",
      "2021-12-13 05:50:36 - Truncated by 779 tokens\n",
      "2021-12-13 05:50:45 - Truncated by 25 tokens\n",
      "2021-12-13 05:50:46 - Truncated by 682 tokens\n",
      "2021-12-13 05:50:54 - Truncated by 87 tokens\n",
      "2021-12-13 05:50:57 - Truncated by 117 tokens\n",
      "2021-12-13 05:51:11 - Truncated by 804 tokens\n",
      "2021-12-13 05:51:14 - Truncated by 456 tokens\n",
      "2021-12-13 05:51:17 - Truncated by 1259 tokens\n",
      "2021-12-13 05:51:18 - Truncated by 86 tokens\n",
      "2021-12-13 05:51:18 - Truncated by 404 tokens\n",
      "2021-12-13 05:51:22 - Truncated by 148 tokens\n",
      "2021-12-13 05:51:23 - Truncated by 1272 tokens\n",
      "2021-12-13 05:51:25 - Truncated by 762 tokens\n",
      "2021-12-13 05:51:27 - Truncated by 48 tokens\n",
      "2021-12-13 05:51:28 - Truncated by 50 tokens\n",
      "2021-12-13 05:51:28 - Truncated by 4218 tokens\n",
      "2021-12-13 05:51:31 - Truncated by 42 tokens\n",
      "2021-12-13 05:51:32 - Truncated by 2382 tokens\n",
      "2021-12-13 05:51:38 - Truncated by 470 tokens\n",
      "2021-12-13 05:51:38 - Truncated by 534 tokens\n",
      "2021-12-13 05:51:39 - Truncated by 534 tokens\n",
      "2021-12-13 05:51:40 - Truncated by 1329 tokens\n",
      "2021-12-13 05:51:41 - Truncated by 1255 tokens\n",
      "2021-12-13 05:51:41 - Truncated by 256 tokens\n",
      "2021-12-13 05:51:43 - Truncated by 405 tokens\n",
      "2021-12-13 05:51:44 - Truncated by 291 tokens\n",
      "2021-12-13 05:51:45 - Truncated by 1727 tokens\n",
      "2021-12-13 05:51:48 - Truncated by 298 tokens\n",
      "2021-12-13 05:51:49 - Truncated by 298 tokens\n",
      "2021-12-13 05:51:52 - Truncated by 1395 tokens\n",
      "2021-12-13 05:51:56 - Truncated by 4369 tokens\n",
      "2021-12-13 05:51:58 - Truncated by 1941 tokens\n",
      "2021-12-13 05:51:58 - Truncated by 3422 tokens\n",
      "2021-12-13 05:51:59 - Truncated by 3433 tokens\n",
      "2021-12-13 05:52:01 - Truncated by 130 tokens\n",
      "2021-12-13 05:52:04 - Truncated by 7842 tokens\n",
      "2021-12-13 05:52:05 - Truncated by 356 tokens\n",
      "2021-12-13 05:52:07 - Truncated by 846 tokens\n",
      "2021-12-13 05:52:08 - Truncated by 379 tokens\n",
      "2021-12-13 05:52:15 - Truncated by 1894 tokens\n",
      "2021-12-13 05:52:15 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:52:16 - Truncated by 1895 tokens\n",
      "2021-12-13 05:52:17 - Truncated by 3022 tokens\n",
      "2021-12-13 05:52:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:52:18 - Truncated by 3023 tokens\n",
      "2021-12-13 05:52:24 - Truncated by 890 tokens\n",
      "2021-12-13 05:52:24 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:52:25 - Truncated by 891 tokens\n",
      "2021-12-13 05:52:48 - Truncated by 58 tokens\n",
      "2021-12-13 05:52:52 - Truncated by 96 tokens\n",
      "2021-12-13 05:52:56 - Truncated by 1407 tokens\n",
      "2021-12-13 05:53:16 - Truncated by 304 tokens\n",
      "2021-12-13 05:53:19 - Truncated by 1102 tokens\n",
      "2021-12-13 05:53:26 - Truncated by 1596 tokens\n",
      "2021-12-13 05:53:30 - Truncated by 2628 tokens\n",
      "2021-12-13 05:53:33 - Truncated by 188 tokens\n",
      "2021-12-13 05:53:40 - Truncated by 313 tokens\n",
      "2021-12-13 05:53:44 - Truncated by 237 tokens\n",
      "2021-12-13 05:53:47 - Truncated by 581 tokens\n",
      "2021-12-13 05:53:50 - Truncated by 1913 tokens\n",
      "2021-12-13 05:53:52 - Truncated by 433 tokens\n",
      "2021-12-13 05:53:53 - Truncated by 628 tokens\n",
      "2021-12-13 05:53:54 - Truncated by 428 tokens\n",
      "2021-12-13 05:53:55 - Truncated by 396 tokens\n",
      "2021-12-13 05:53:56 - Truncated by 695 tokens\n",
      "2021-12-13 05:53:57 - Truncated by 687 tokens\n",
      "2021-12-13 05:53:58 - Truncated by 23 tokens\n",
      "2021-12-13 05:54:00 - Truncated by 234 tokens\n",
      "2021-12-13 05:54:05 - Truncated by 322 tokens\n",
      "2021-12-13 05:54:08 - Truncated by 1145 tokens\n",
      "2021-12-13 05:54:08 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:54:09 - Truncated by 1146 tokens\n",
      "2021-12-13 05:54:11 - Truncated by 204 tokens\n",
      "2021-12-13 05:54:12 - Truncated by 1968 tokens\n",
      "2021-12-13 05:54:12 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:54:13 - Truncated by 1969 tokens\n",
      "2021-12-13 05:54:13 - Truncated by 1002 tokens\n",
      "2021-12-13 05:54:16 - Truncated by 501 tokens\n",
      "2021-12-13 05:54:16 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:54:17 - Truncated by 502 tokens\n",
      "2021-12-13 05:54:17 - Truncated by 1526 tokens\n",
      "2021-12-13 05:54:21 - Truncated by 3389 tokens\n",
      "2021-12-13 05:54:22 - Truncated by 278 tokens\n",
      "2021-12-13 05:54:23 - Truncated by 2649 tokens\n",
      "2021-12-13 05:54:23 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:54:24 - Truncated by 2650 tokens\n",
      "2021-12-13 05:54:25 - Truncated by 2663 tokens\n",
      "2021-12-13 05:54:26 - Truncated by 3205 tokens\n",
      "2021-12-13 05:54:28 - Truncated by 1187 tokens\n",
      "2021-12-13 05:54:29 - Truncated by 381 tokens\n",
      "2021-12-13 05:54:29 - Truncated by 182 tokens\n",
      "2021-12-13 05:54:29 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:54:30 - Truncated by 183 tokens\n",
      "2021-12-13 05:54:34 - Truncated by 153 tokens\n",
      "2021-12-13 05:54:39 - Truncated by 530 tokens\n",
      "2021-12-13 05:54:40 - Truncated by 530 tokens\n",
      "2021-12-13 05:54:44 - Truncated by 254 tokens\n",
      "2021-12-13 05:54:49 - Truncated by 466 tokens\n",
      "2021-12-13 05:54:52 - Truncated by 514 tokens\n",
      "2021-12-13 05:54:55 - Truncated by 620 tokens\n",
      "2021-12-13 05:54:56 - Truncated by 3350 tokens\n",
      "2021-12-13 05:54:56 - Truncated by 169 tokens\n",
      "2021-12-13 05:54:57 - Truncated by 488 tokens\n",
      "2021-12-13 05:55:02 - Truncated by 632 tokens\n",
      "2021-12-13 05:55:02 - Truncated by 632 tokens\n",
      "2021-12-13 05:55:03 - Truncated by 851 tokens\n",
      "2021-12-13 05:55:08 - Truncated by 762 tokens\n",
      "2021-12-13 05:55:12 - Truncated by 366 tokens\n",
      "2021-12-13 05:55:12 - Truncated by 2084 tokens\n",
      "2021-12-13 05:55:17 - Truncated by 194 tokens\n",
      "2021-12-13 05:55:21 - Truncated by 57 tokens\n",
      "2021-12-13 05:55:30 - Truncated by 922 tokens\n",
      "2021-12-13 05:55:36 - Truncated by 428 tokens\n",
      "2021-12-13 05:55:48 - Truncated by 848 tokens\n",
      "2021-12-13 05:55:49 - Truncated by 1131 tokens\n",
      "2021-12-13 05:55:58 - Truncated by 6 tokens\n",
      "2021-12-13 05:56:04 - Truncated by 132 tokens\n",
      "2021-12-13 05:56:07 - Truncated by 468 tokens\n",
      "2021-12-13 05:56:10 - Truncated by 605 tokens\n",
      "2021-12-13 05:56:11 - Truncated by 69 tokens\n",
      "2021-12-13 05:56:15 - Truncated by 989 tokens\n",
      "2021-12-13 05:56:15 - Truncated by 989 tokens\n",
      "2021-12-13 05:56:16 - Truncated by 732 tokens\n",
      "2021-12-13 05:56:17 - Truncated by 43 tokens\n",
      "2021-12-13 05:56:19 - Truncated by 734 tokens\n",
      "2021-12-13 05:56:25 - Truncated by 5072 tokens\n",
      "2021-12-13 05:56:25 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:56:26 - Truncated by 5073 tokens\n",
      "2021-12-13 05:56:29 - Truncated by 776 tokens\n",
      "2021-12-13 05:56:29 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:56:30 - Truncated by 777 tokens\n",
      "2021-12-13 05:56:32 - Truncated by 1121 tokens\n",
      "2021-12-13 05:56:32 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:56:33 - Truncated by 1122 tokens\n",
      "2021-12-13 05:56:35 - Truncated by 989 tokens\n",
      "2021-12-13 05:56:35 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:56:36 - Truncated by 990 tokens\n",
      "2021-12-13 05:56:42 - Truncated by 196 tokens\n",
      "2021-12-13 05:56:42 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:56:43 - Truncated by 197 tokens\n",
      "2021-12-13 05:56:49 - Truncated by 1197 tokens\n",
      "2021-12-13 05:56:49 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:56:50 - Truncated by 1198 tokens\n",
      "2021-12-13 05:56:52 - Truncated by 10031 tokens\n",
      "2021-12-13 05:56:53 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:56:54 - Truncated by 10032 tokens\n",
      "2021-12-13 05:56:57 - Truncated by 569 tokens\n",
      "2021-12-13 05:57:01 - Truncated by 22 tokens\n",
      "2021-12-13 05:57:04 - Truncated by 544 tokens\n",
      "2021-12-13 05:57:07 - Truncated by 151 tokens\n",
      "2021-12-13 05:57:08 - Truncated by 1124 tokens\n",
      "2021-12-13 05:57:09 - Truncated by 923 tokens\n",
      "2021-12-13 05:57:10 - Truncated by 27 tokens\n",
      "2021-12-13 05:57:14 - Truncated by 2556 tokens\n",
      "2021-12-13 05:57:30 - Truncated by 1545 tokens\n",
      "2021-12-13 05:57:31 - Truncated by 5134 tokens\n",
      "2021-12-13 05:57:32 - Truncated by 1836 tokens\n",
      "2021-12-13 05:57:33 - Truncated by 1930 tokens\n",
      "2021-12-13 05:57:34 - Truncated by 1860 tokens\n",
      "2021-12-13 05:57:39 - Truncated by 5533 tokens\n",
      "2021-12-13 05:57:40 - Truncated by 918 tokens\n",
      "2021-12-13 05:57:40 - Truncated by 626 tokens\n",
      "2021-12-13 05:57:46 - Truncated by 1201 tokens\n",
      "2021-12-13 05:57:48 - Truncated by 162 tokens\n",
      "2021-12-13 05:57:52 - Truncated by 59 tokens\n",
      "2021-12-13 05:57:57 - Truncated by 93 tokens\n",
      "2021-12-13 05:57:58 - Truncated by 2991 tokens\n",
      "2021-12-13 05:57:59 - Truncated by 3855 tokens\n",
      "2021-12-13 05:57:59 - Truncated by 1657 tokens\n",
      "2021-12-13 05:58:03 - Truncated by 199 tokens\n",
      "2021-12-13 05:58:08 - Truncated by 378 tokens\n",
      "2021-12-13 05:58:11 - Truncated by 2237 tokens\n",
      "2021-12-13 05:58:14 - Truncated by 599 tokens\n",
      "2021-12-13 05:58:15 - Truncated by 113 tokens\n",
      "2021-12-13 05:58:20 - Truncated by 61 tokens\n",
      "2021-12-13 05:58:20 - Truncated by 137 tokens\n",
      "2021-12-13 05:58:25 - Truncated by 498 tokens\n",
      "2021-12-13 05:58:27 - Truncated by 19 tokens\n",
      "2021-12-13 05:58:29 - Truncated by 648 tokens\n",
      "2021-12-13 05:58:30 - Truncated by 594 tokens\n",
      "2021-12-13 05:58:34 - Truncated by 361 tokens\n",
      "2021-12-13 05:58:35 - Truncated by 908 tokens\n",
      "2021-12-13 05:58:37 - Truncated by 1359 tokens\n",
      "2021-12-13 05:58:41 - Truncated by 313 tokens\n",
      "2021-12-13 05:58:43 - Truncated by 2660 tokens\n",
      "2021-12-13 05:58:43 - Truncated by 1139 tokens\n",
      "2021-12-13 05:58:48 - Truncated by 5571 tokens\n",
      "2021-12-13 05:58:48 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:58:49 - Truncated by 5572 tokens\n",
      "2021-12-13 05:58:51 - Truncated by 355 tokens\n",
      "2021-12-13 05:58:51 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:58:52 - Truncated by 356 tokens\n",
      "2021-12-13 05:58:55 - Truncated by 60 tokens\n",
      "2021-12-13 05:58:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:58:56 - Truncated by 61 tokens\n",
      "2021-12-13 05:59:05 - Truncated by 1179 tokens\n",
      "2021-12-13 05:59:05 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:59:06 - Truncated by 1180 tokens\n",
      "2021-12-13 05:59:12 - Truncated by 982 tokens\n",
      "2021-12-13 05:59:12 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:59:13 - Truncated by 983 tokens\n",
      "2021-12-13 05:59:14 - Truncated by 586 tokens\n",
      "2021-12-13 05:59:14 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:59:15 - Truncated by 587 tokens\n",
      "2021-12-13 05:59:27 - Truncated by 4102 tokens\n",
      "2021-12-13 05:59:42 - Truncated by 22252 tokens\n",
      "2021-12-13 05:59:42 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 05:59:43 - Truncated by 22253 tokens\n",
      "2021-12-13 05:59:44 - Truncated by 215 tokens\n",
      "2021-12-13 05:59:47 - Truncated by 344 tokens\n",
      "2021-12-13 05:59:50 - Truncated by 378 tokens\n",
      "2021-12-13 06:00:24 - Truncated by 794 tokens\n",
      "2021-12-13 06:00:24 - Truncated by 806 tokens\n",
      "2021-12-13 06:00:26 - Truncated by 602 tokens\n",
      "2021-12-13 06:00:27 - Truncated by 245 tokens\n",
      "2021-12-13 06:00:27 - Truncated by 959 tokens\n",
      "2021-12-13 06:00:28 - Truncated by 5976 tokens\n",
      "2021-12-13 06:00:30 - Truncated by 4661 tokens\n",
      "2021-12-13 06:00:38 - Truncated by 18422 tokens\n",
      "2021-12-13 06:00:39 - Truncated by 18 tokens\n",
      "2021-12-13 06:00:43 - Truncated by 9407 tokens\n",
      "2021-12-13 06:00:50 - Truncated by 3336 tokens\n",
      "2021-12-13 06:00:53 - Truncated by 1788 tokens\n",
      "2021-12-13 06:00:56 - Truncated by 4201 tokens\n",
      "2021-12-13 06:00:57 - Truncated by 698 tokens\n",
      "2021-12-13 06:00:58 - Truncated by 719 tokens\n",
      "2021-12-13 06:01:02 - Truncated by 1565 tokens\n",
      "2021-12-13 06:01:05 - Truncated by 294 tokens\n",
      "2021-12-13 06:01:17 - Truncated by 449 tokens\n",
      "2021-12-13 06:01:18 - Truncated by 195 tokens\n",
      "2021-12-13 06:01:22 - Truncated by 1025 tokens\n",
      "2021-12-13 06:01:22 - Truncated by 602 tokens\n",
      "2021-12-13 06:01:23 - Truncated by 906 tokens\n",
      "2021-12-13 06:01:34 - Truncated by 1873 tokens\n",
      "2021-12-13 06:01:35 - Truncated by 674 tokens\n",
      "2021-12-13 06:01:36 - Truncated by 699 tokens\n",
      "2021-12-13 06:01:36 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:01:37 - Truncated by 700 tokens\n",
      "2021-12-13 06:01:46 - Truncated by 27 tokens\n",
      "2021-12-13 06:01:47 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:01:48 - Truncated by 28 tokens\n",
      "2021-12-13 06:01:50 - Truncated by 2580 tokens\n",
      "2021-12-13 06:01:50 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:01:51 - Truncated by 2581 tokens\n",
      "2021-12-13 06:01:54 - Truncated by 423 tokens\n",
      "2021-12-13 06:01:54 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:01:55 - Truncated by 424 tokens\n",
      "2021-12-13 06:02:16 - Truncated by 616 tokens\n",
      "2021-12-13 06:02:29 - Truncated by 589 tokens\n",
      "2021-12-13 06:02:32 - Truncated by 1370 tokens\n",
      "2021-12-13 06:02:50 - Truncated by 1154 tokens\n",
      "2021-12-13 06:02:50 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:02:51 - Truncated by 1155 tokens\n",
      "2021-12-13 06:02:54 - Truncated by 165 tokens\n",
      "2021-12-13 06:02:54 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:02:55 - Truncated by 166 tokens\n",
      "2021-12-13 06:02:55 - Truncated by 172 tokens\n",
      "2021-12-13 06:02:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:02:56 - Truncated by 173 tokens\n",
      "2021-12-13 06:02:59 - Truncated by 2851 tokens\n",
      "2021-12-13 06:03:00 - Truncated by 2851 tokens\n",
      "2021-12-13 06:03:11 - Truncated by 4529 tokens\n",
      "2021-12-13 06:03:11 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:12 - Truncated by 4530 tokens\n",
      "2021-12-13 06:03:16 - Truncated by 1179 tokens\n",
      "2021-12-13 06:03:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:18 - Truncated by 1180 tokens\n",
      "2021-12-13 06:03:20 - Truncated by 198 tokens\n",
      "2021-12-13 06:03:21 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:22 - Truncated by 199 tokens\n",
      "2021-12-13 06:03:23 - Truncated by 417 tokens\n",
      "2021-12-13 06:03:23 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:24 - Truncated by 418 tokens\n",
      "2021-12-13 06:03:30 - Truncated by 294 tokens\n",
      "2021-12-13 06:03:30 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:31 - Truncated by 295 tokens\n",
      "2021-12-13 06:03:33 - Truncated by 1082 tokens\n",
      "2021-12-13 06:03:33 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:34 - Truncated by 1083 tokens\n",
      "2021-12-13 06:03:41 - Truncated by 340 tokens\n",
      "2021-12-13 06:03:41 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:42 - Truncated by 341 tokens\n",
      "2021-12-13 06:03:44 - Truncated by 294 tokens\n",
      "2021-12-13 06:03:44 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:45 - Truncated by 295 tokens\n",
      "2021-12-13 06:03:57 - Truncated by 711 tokens\n",
      "2021-12-13 06:03:58 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:03:59 - Truncated by 712 tokens\n",
      "2021-12-13 06:04:02 - Truncated by 433 tokens\n",
      "2021-12-13 06:04:04 - Truncated by 134 tokens\n",
      "2021-12-13 06:04:06 - Truncated by 1654 tokens\n",
      "2021-12-13 06:04:08 - Truncated by 1121 tokens\n",
      "2021-12-13 06:04:10 - Truncated by 345 tokens\n",
      "2021-12-13 06:04:11 - Truncated by 1367 tokens\n",
      "2021-12-13 06:04:12 - Truncated by 12047 tokens\n",
      "2021-12-13 06:04:14 - Truncated by 885 tokens\n",
      "2021-12-13 06:04:15 - Truncated by 404 tokens\n",
      "2021-12-13 06:04:19 - Truncated by 552 tokens\n",
      "2021-12-13 06:04:19 - Truncated by 1240 tokens\n",
      "2021-12-13 06:04:21 - Truncated by 2678 tokens\n",
      "2021-12-13 06:04:26 - Truncated by 1916 tokens\n",
      "2021-12-13 06:04:31 - Truncated by 428 tokens\n",
      "2021-12-13 06:04:35 - Truncated by 167 tokens\n",
      "2021-12-13 06:04:38 - Truncated by 4664 tokens\n",
      "2021-12-13 06:04:41 - Truncated by 1164 tokens\n",
      "2021-12-13 06:04:42 - Truncated by 2322 tokens\n",
      "2021-12-13 06:04:42 - Truncated by 2322 tokens\n",
      "2021-12-13 06:04:44 - Truncated by 269 tokens\n",
      "2021-12-13 06:04:48 - Truncated by 9431 tokens\n",
      "2021-12-13 06:04:51 - Truncated by 6000 tokens\n",
      "2021-12-13 06:04:55 - Truncated by 144 tokens\n",
      "2021-12-13 06:04:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:04:56 - Truncated by 145 tokens\n",
      "2021-12-13 06:04:57 - Truncated by 1812 tokens\n",
      "2021-12-13 06:04:59 - Truncated by 19791 tokens\n",
      "2021-12-13 06:05:01 - Truncated by 635 tokens\n",
      "2021-12-13 06:05:07 - Truncated by 18446 tokens\n",
      "2021-12-13 06:05:13 - Truncated by 626 tokens\n",
      "2021-12-13 06:05:14 - Truncated by 158 tokens\n",
      "2021-12-13 06:05:14 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:15 - Truncated by 159 tokens\n",
      "2021-12-13 06:05:15 - Truncated by 381 tokens\n",
      "2021-12-13 06:05:15 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:16 - Truncated by 382 tokens\n",
      "2021-12-13 06:05:20 - Truncated by 1941 tokens\n",
      "2021-12-13 06:05:25 - Truncated by 279 tokens\n",
      "2021-12-13 06:05:25 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:26 - Truncated by 280 tokens\n",
      "2021-12-13 06:05:26 - Truncated by 236 tokens\n",
      "2021-12-13 06:05:27 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:28 - Truncated by 237 tokens\n",
      "2021-12-13 06:05:32 - Truncated by 487 tokens\n",
      "2021-12-13 06:05:36 - Truncated by 524 tokens\n",
      "2021-12-13 06:05:36 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:37 - Truncated by 525 tokens\n",
      "2021-12-13 06:05:40 - Truncated by 12034 tokens\n",
      "2021-12-13 06:05:41 - Truncated by 332 tokens\n",
      "2021-12-13 06:05:43 - Truncated by 1037 tokens\n",
      "2021-12-13 06:05:43 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:44 - Truncated by 1038 tokens\n",
      "2021-12-13 06:05:45 - Truncated by 4651 tokens\n",
      "2021-12-13 06:05:45 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:46 - Truncated by 4652 tokens\n",
      "2021-12-13 06:05:47 - Truncated by 737 tokens\n",
      "2021-12-13 06:05:47 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:05:48 - Truncated by 738 tokens\n",
      "2021-12-13 06:06:07 - Truncated by 2015 tokens\n",
      "2021-12-13 06:06:07 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:08 - Truncated by 2016 tokens\n",
      "2021-12-13 06:06:09 - Truncated by 419 tokens\n",
      "2021-12-13 06:06:09 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:10 - Truncated by 420 tokens\n",
      "2021-12-13 06:06:12 - Truncated by 365 tokens\n",
      "2021-12-13 06:06:12 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:13 - Truncated by 366 tokens\n",
      "2021-12-13 06:06:15 - Truncated by 2319 tokens\n",
      "2021-12-13 06:06:18 - Truncated by 637 tokens\n",
      "2021-12-13 06:06:19 - Truncated by 632 tokens\n",
      "2021-12-13 06:06:20 - Truncated by 1113 tokens\n",
      "2021-12-13 06:06:20 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:21 - Truncated by 1114 tokens\n",
      "2021-12-13 06:06:23 - Truncated by 224 tokens\n",
      "2021-12-13 06:06:23 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:24 - Truncated by 225 tokens\n",
      "2021-12-13 06:06:26 - Truncated by 1744 tokens\n",
      "2021-12-13 06:06:27 - Truncated by 172 tokens\n",
      "2021-12-13 06:06:27 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:28 - Truncated by 173 tokens\n",
      "2021-12-13 06:06:34 - Truncated by 1180 tokens\n",
      "2021-12-13 06:06:34 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:35 - Truncated by 1181 tokens\n",
      "2021-12-13 06:06:36 - Truncated by 1134 tokens\n",
      "2021-12-13 06:06:40 - Truncated by 27 tokens\n",
      "2021-12-13 06:06:40 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:41 - Truncated by 28 tokens\n",
      "2021-12-13 06:06:42 - Truncated by 225 tokens\n",
      "2021-12-13 06:06:42 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:43 - Truncated by 226 tokens\n",
      "2021-12-13 06:06:45 - Truncated by 3811 tokens\n",
      "2021-12-13 06:06:45 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:46 - Truncated by 3812 tokens\n",
      "2021-12-13 06:06:50 - Truncated by 74 tokens\n",
      "2021-12-13 06:06:51 - Truncated by 25 tokens\n",
      "2021-12-13 06:06:51 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:52 - Truncated by 26 tokens\n",
      "2021-12-13 06:06:53 - Truncated by 87 tokens\n",
      "2021-12-13 06:06:53 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:54 - Truncated by 88 tokens\n",
      "2021-12-13 06:06:55 - Truncated by 975 tokens\n",
      "2021-12-13 06:06:56 - Truncated by 142 tokens\n",
      "2021-12-13 06:06:58 - Truncated by 9417 tokens\n",
      "2021-12-13 06:06:58 - Truncated by 130 tokens\n",
      "2021-12-13 06:06:58 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:06:59 - Truncated by 131 tokens\n",
      "2021-12-13 06:07:11 - Truncated by 66497 tokens\n",
      "2021-12-13 06:07:18 - Truncated by 367 tokens\n",
      "2021-12-13 06:07:18 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:07:19 - Truncated by 368 tokens\n",
      "2021-12-13 06:07:25 - Truncated by 556 tokens\n",
      "2021-12-13 06:07:25 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:07:26 - Truncated by 557 tokens\n",
      "2021-12-13 06:07:28 - Truncated by 84 tokens\n",
      "2021-12-13 06:07:30 - Truncated by 5986 tokens\n",
      "2021-12-13 06:07:32 - Truncated by 450 tokens\n",
      "2021-12-13 06:07:32 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:07:33 - Truncated by 451 tokens\n",
      "2021-12-13 06:07:47 - Truncated by 1113 tokens\n",
      "2021-12-13 06:07:52 - Truncated by 1155 tokens\n",
      "2021-12-13 06:08:02 - Truncated by 801 tokens\n",
      "2021-12-13 06:08:12 - Truncated by 62 tokens\n",
      "2021-12-13 06:08:23 - Truncated by 770 tokens\n",
      "2021-12-13 06:08:23 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:08:24 - Truncated by 771 tokens\n",
      "2021-12-13 06:08:25 - Truncated by 1325 tokens\n",
      "2021-12-13 06:08:27 - Truncated by 4233 tokens\n",
      "2021-12-13 06:08:30 - Truncated by 161 tokens\n",
      "2021-12-13 06:08:31 - Truncated by 6210 tokens\n",
      "2021-12-13 06:08:33 - Truncated by 2304 tokens\n",
      "2021-12-13 06:08:33 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:08:34 - Truncated by 2305 tokens\n",
      "2021-12-13 06:08:39 - Truncated by 1747 tokens\n",
      "2021-12-13 06:08:47 - Truncated by 402 tokens\n",
      "2021-12-13 06:08:49 - Truncated by 742 tokens\n",
      "2021-12-13 06:08:52 - Truncated by 805 tokens\n",
      "2021-12-13 06:08:52 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:08:53 - Truncated by 806 tokens\n",
      "2021-12-13 06:09:01 - Truncated by 2817 tokens\n",
      "2021-12-13 06:09:04 - Truncated by 594 tokens\n",
      "2021-12-13 06:09:10 - Truncated by 2030 tokens\n",
      "2021-12-13 06:09:10 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:09:11 - Truncated by 2031 tokens\n",
      "2021-12-13 06:09:14 - Truncated by 377 tokens\n",
      "2021-12-13 06:09:14 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:09:15 - Truncated by 378 tokens\n",
      "2021-12-13 06:09:17 - Truncated by 1243 tokens\n",
      "2021-12-13 06:09:21 - Truncated by 2160 tokens\n",
      "2021-12-13 06:09:23 - Truncated by 2044 tokens\n",
      "2021-12-13 06:09:25 - Truncated by 274 tokens\n",
      "2021-12-13 06:09:25 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:09:26 - Truncated by 275 tokens\n",
      "2021-12-13 06:09:28 - Truncated by 270 tokens\n",
      "2021-12-13 06:09:28 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:09:29 - Truncated by 271 tokens\n",
      "2021-12-13 06:09:34 - Truncated by 6982 tokens\n",
      "2021-12-13 06:09:36 - Truncated by 755 tokens\n",
      "2021-12-13 06:09:43 - Truncated by 215 tokens\n",
      "2021-12-13 06:09:48 - Truncated by 397 tokens\n",
      "2021-12-13 06:10:00 - Truncated by 1042 tokens\n",
      "2021-12-13 06:10:00 - Truncated by 1533 tokens\n",
      "2021-12-13 06:10:09 - Truncated by 2053 tokens\n",
      "2021-12-13 06:10:15 - Truncated by 234 tokens\n",
      "2021-12-13 06:10:21 - Truncated by 771 tokens\n",
      "2021-12-13 06:10:26 - Truncated by 21 tokens\n",
      "2021-12-13 06:10:26 - Truncated by 651 tokens\n",
      "2021-12-13 06:10:27 - Truncated by 275 tokens\n",
      "2021-12-13 06:10:30 - Truncated by 1935 tokens\n",
      "2021-12-13 06:10:34 - Truncated by 181 tokens\n",
      "2021-12-13 06:10:35 - Truncated by 283 tokens\n",
      "2021-12-13 06:10:38 - Truncated by 129 tokens\n",
      "2021-12-13 06:10:39 - Truncated by 731 tokens\n",
      "2021-12-13 06:10:43 - Truncated by 2687 tokens\n",
      "2021-12-13 06:10:43 - Truncated by 11841 tokens\n",
      "2021-12-13 06:10:46 - Truncated by 340 tokens\n",
      "2021-12-13 06:10:47 - Truncated by 113 tokens\n",
      "2021-12-13 06:10:47 - Truncated by 1232 tokens\n",
      "2021-12-13 06:10:51 - Truncated by 12056 tokens\n",
      "2021-12-13 06:10:52 - Truncated by 487 tokens\n",
      "2021-12-13 06:10:53 - Truncated by 1765 tokens\n",
      "2021-12-13 06:10:53 - Truncated by 550 tokens\n",
      "2021-12-13 06:10:56 - Truncated by 1447 tokens\n",
      "2021-12-13 06:10:58 - Truncated by 224 tokens\n",
      "2021-12-13 06:11:00 - Truncated by 588 tokens\n",
      "2021-12-13 06:11:01 - Truncated by 446 tokens\n",
      "2021-12-13 06:11:04 - Truncated by 274 tokens\n",
      "2021-12-13 06:11:04 - Truncated by 489 tokens\n",
      "2021-12-13 06:11:05 - Truncated by 562 tokens\n",
      "2021-12-13 06:11:05 - Truncated by 717 tokens\n",
      "2021-12-13 06:11:09 - Truncated by 50 tokens\n",
      "2021-12-13 06:11:11 - Truncated by 1520 tokens\n",
      "2021-12-13 06:11:13 - Truncated by 132 tokens\n",
      "2021-12-13 06:11:16 - Truncated by 60 tokens\n",
      "2021-12-13 06:11:23 - Truncated by 575 tokens\n",
      "2021-12-13 06:11:23 - Truncated by 494 tokens\n",
      "2021-12-13 06:11:24 - Truncated by 524 tokens\n",
      "2021-12-13 06:11:24 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:11:25 - Truncated by 525 tokens\n",
      "2021-12-13 06:11:25 - Truncated by 120 tokens\n",
      "2021-12-13 06:11:34 - Truncated by 25 tokens\n",
      "2021-12-13 06:11:37 - Truncated by 2340 tokens\n",
      "2021-12-13 06:11:38 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:11:39 - Truncated by 2341 tokens\n",
      "2021-12-13 06:11:40 - Truncated by 230 tokens\n",
      "2021-12-13 06:11:41 - Truncated by 18429 tokens\n",
      "2021-12-13 06:11:45 - Truncated by 254 tokens\n",
      "2021-12-13 06:11:45 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:11:46 - Truncated by 255 tokens\n",
      "2021-12-13 06:11:52 - Truncated by 10687 tokens\n",
      "2021-12-13 06:11:58 - Truncated by 759 tokens\n",
      "2021-12-13 06:11:59 - Truncated by 1556 tokens\n",
      "2021-12-13 06:12:00 - Truncated by 283 tokens\n",
      "2021-12-13 06:12:01 - Truncated by 445 tokens\n",
      "2021-12-13 06:12:02 - Truncated by 1132 tokens\n",
      "2021-12-13 06:12:03 - Truncated by 246 tokens\n",
      "2021-12-13 06:12:09 - Truncated by 1511 tokens\n",
      "2021-12-13 06:12:10 - Truncated by 1164 tokens\n",
      "2021-12-13 06:12:11 - Truncated by 18 tokens\n",
      "2021-12-13 06:12:20 - Truncated by 1514 tokens\n",
      "2021-12-13 06:12:25 - Truncated by 2309 tokens\n",
      "2021-12-13 06:12:50 - Truncated by 282 tokens\n",
      "2021-12-13 06:12:50 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:12:51 - Truncated by 283 tokens\n",
      "2021-12-13 06:12:55 - Truncated by 141 tokens\n",
      "2021-12-13 06:12:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:12:56 - Truncated by 142 tokens\n",
      "2021-12-13 06:12:57 - Truncated by 801 tokens\n",
      "2021-12-13 06:12:57 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:12:58 - Truncated by 802 tokens\n",
      "2021-12-13 06:13:01 - Truncated by 11 tokens\n",
      "2021-12-13 06:13:02 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:13:03 - Truncated by 12 tokens\n",
      "2021-12-13 06:13:06 - Truncated by 256 tokens\n",
      "2021-12-13 06:13:06 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:13:07 - Truncated by 257 tokens\n",
      "2021-12-13 06:13:16 - Truncated by 1155 tokens\n",
      "2021-12-13 06:13:16 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:13:17 - Truncated by 1156 tokens\n",
      "2021-12-13 06:13:27 - Truncated by 210 tokens\n",
      "2021-12-13 06:13:27 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:13:28 - Truncated by 211 tokens\n",
      "2021-12-13 06:13:29 - Truncated by 644 tokens\n",
      "2021-12-13 06:13:29 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:13:30 - Truncated by 645 tokens\n",
      "2021-12-13 06:13:34 - Truncated by 2016 tokens\n",
      "2021-12-13 06:13:34 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:13:35 - Truncated by 2017 tokens\n",
      "2021-12-13 06:13:36 - Truncated by 162 tokens\n",
      "2021-12-13 06:13:36 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:13:37 - Truncated by 163 tokens\n",
      "2021-12-13 06:13:45 - Truncated by 889 tokens\n",
      "2021-12-13 06:13:51 - Truncated by 758 tokens\n",
      "2021-12-13 06:14:02 - Truncated by 2227 tokens\n",
      "2021-12-13 06:14:03 - Truncated by 784 tokens\n",
      "2021-12-13 06:14:04 - Truncated by 1146 tokens\n",
      "2021-12-13 06:14:06 - Truncated by 504 tokens\n",
      "2021-12-13 06:14:06 - Truncated by 28 tokens\n",
      "2021-12-13 06:14:15 - Truncated by 973 tokens\n",
      "2021-12-13 06:14:19 - Truncated by 392 tokens\n",
      "2021-12-13 06:14:25 - Truncated by 1645 tokens\n",
      "2021-12-13 06:14:25 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:14:26 - Truncated by 1646 tokens\n",
      "2021-12-13 06:14:27 - Truncated by 1655 tokens\n",
      "2021-12-13 06:14:27 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:14:28 - Truncated by 1656 tokens\n",
      "2021-12-13 06:14:32 - Truncated by 67 tokens\n",
      "2021-12-13 06:14:32 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:14:33 - Truncated by 68 tokens\n",
      "2021-12-13 06:14:34 - Truncated by 158 tokens\n",
      "2021-12-13 06:14:34 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:14:35 - Truncated by 159 tokens\n",
      "2021-12-13 06:14:40 - Truncated by 134 tokens\n",
      "2021-12-13 06:14:40 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:14:41 - Truncated by 135 tokens\n",
      "2021-12-13 06:14:47 - Truncated by 810 tokens\n",
      "2021-12-13 06:14:47 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:14:48 - Truncated by 811 tokens\n",
      "2021-12-13 06:14:59 - Truncated by 93 tokens\n",
      "2021-12-13 06:14:59 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:00 - Truncated by 94 tokens\n",
      "2021-12-13 06:15:06 - Truncated by 296 tokens\n",
      "2021-12-13 06:15:06 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:07 - Truncated by 297 tokens\n",
      "2021-12-13 06:15:37 - Truncated by 542 tokens\n",
      "2021-12-13 06:15:37 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:38 - Truncated by 543 tokens\n",
      "2021-12-13 06:15:40 - Truncated by 314 tokens\n",
      "2021-12-13 06:15:40 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:41 - Truncated by 315 tokens\n",
      "2021-12-13 06:15:44 - Truncated by 640 tokens\n",
      "2021-12-13 06:15:44 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:45 - Truncated by 641 tokens\n",
      "2021-12-13 06:15:46 - Truncated by 754 tokens\n",
      "2021-12-13 06:15:50 - Truncated by 131 tokens\n",
      "2021-12-13 06:15:50 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:51 - Truncated by 132 tokens\n",
      "2021-12-13 06:15:52 - Truncated by 491 tokens\n",
      "2021-12-13 06:15:52 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:53 - Truncated by 492 tokens\n",
      "2021-12-13 06:15:54 - Truncated by 573 tokens\n",
      "2021-12-13 06:15:55 - Truncated by 237 tokens\n",
      "2021-12-13 06:15:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:15:56 - Truncated by 238 tokens\n",
      "2021-12-13 06:15:58 - Truncated by 10784 tokens\n",
      "2021-12-13 06:16:08 - Truncated by 23 tokens\n",
      "2021-12-13 06:16:08 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:16:09 - Truncated by 24 tokens\n",
      "2021-12-13 06:16:12 - Truncated by 45 tokens\n",
      "2021-12-13 06:16:12 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:16:13 - Truncated by 46 tokens\n",
      "2021-12-13 06:16:13 - Truncated by 82 tokens\n",
      "2021-12-13 06:16:14 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:16:15 - Truncated by 83 tokens\n",
      "2021-12-13 06:16:17 - Truncated by 12 tokens\n",
      "2021-12-13 06:16:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:16:18 - Truncated by 13 tokens\n",
      "2021-12-13 06:16:19 - Truncated by 733 tokens\n",
      "2021-12-13 06:16:20 - Truncated by 2412 tokens\n",
      "2021-12-13 06:16:20 - Truncated by 121 tokens\n",
      "2021-12-13 06:16:21 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:16:22 - Truncated by 122 tokens\n",
      "2021-12-13 06:16:38 - Truncated by 2989 tokens\n",
      "2021-12-13 06:16:42 - Truncated by 511 tokens\n",
      "2021-12-13 06:16:57 - Truncated by 501 tokens\n",
      "2021-12-13 06:16:58 - Truncated by 214 tokens\n",
      "2021-12-13 06:16:58 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:16:59 - Truncated by 215 tokens\n",
      "2021-12-13 06:17:02 - Truncated by 530 tokens\n",
      "2021-12-13 06:17:02 - Truncated by 453 tokens\n",
      "2021-12-13 06:17:04 - Truncated by 825 tokens\n",
      "2021-12-13 06:17:07 - Truncated by 557 tokens\n",
      "2021-12-13 06:17:09 - Truncated by 2649 tokens\n",
      "2021-12-13 06:17:10 - Truncated by 12401 tokens\n",
      "2021-12-13 06:17:13 - Truncated by 492 tokens\n",
      "2021-12-13 06:17:14 - Truncated by 252 tokens\n",
      "2021-12-13 06:17:18 - Truncated by 397 tokens\n",
      "2021-12-13 06:17:19 - Truncated by 1220 tokens\n",
      "2021-12-13 06:17:21 - Truncated by 791 tokens\n",
      "2021-12-13 06:17:25 - Truncated by 1288 tokens\n",
      "2021-12-13 06:17:27 - Truncated by 366 tokens\n",
      "2021-12-13 06:17:31 - Truncated by 1096 tokens\n",
      "2021-12-13 06:17:34 - Truncated by 480 tokens\n",
      "2021-12-13 06:17:35 - Truncated by 270 tokens\n",
      "2021-12-13 06:17:38 - Truncated by 1347 tokens\n",
      "2021-12-13 06:17:42 - Truncated by 1534 tokens\n",
      "2021-12-13 06:17:47 - Truncated by 199 tokens\n",
      "2021-12-13 06:17:48 - Truncated by 3031 tokens\n",
      "2021-12-13 06:17:53 - Truncated by 443 tokens\n",
      "2021-12-13 06:17:54 - Truncated by 270 tokens\n",
      "2021-12-13 06:17:56 - Truncated by 3097 tokens\n",
      "2021-12-13 06:17:59 - Truncated by 1670 tokens\n",
      "2021-12-13 06:18:01 - Truncated by 117 tokens\n",
      "2021-12-13 06:18:04 - Truncated by 146 tokens\n",
      "2021-12-13 06:18:04 - Truncated by 334 tokens\n",
      "2021-12-13 06:18:06 - Truncated by 3099 tokens\n",
      "2021-12-13 06:18:09 - Truncated by 805 tokens\n",
      "2021-12-13 06:18:11 - Truncated by 1302 tokens\n",
      "2021-12-13 06:18:12 - Truncated by 1067 tokens\n",
      "2021-12-13 06:18:14 - Truncated by 488 tokens\n",
      "2021-12-13 06:18:17 - Truncated by 1576 tokens\n",
      "2021-12-13 06:18:19 - Truncated by 1019 tokens\n",
      "2021-12-13 06:18:21 - Truncated by 4425 tokens\n",
      "2021-12-13 06:18:22 - Truncated by 4425 tokens\n",
      "2021-12-13 06:18:26 - Truncated by 83 tokens\n",
      "2021-12-13 06:18:29 - Truncated by 437 tokens\n",
      "2021-12-13 06:18:30 - Truncated by 1956 tokens\n",
      "2021-12-13 06:18:32 - Truncated by 354 tokens\n",
      "2021-12-13 06:18:34 - Truncated by 471 tokens\n",
      "2021-12-13 06:18:35 - Truncated by 6978 tokens\n",
      "2021-12-13 06:18:40 - Truncated by 298 tokens\n",
      "2021-12-13 06:18:44 - Truncated by 1588 tokens\n",
      "2021-12-13 06:18:47 - Truncated by 1280 tokens\n",
      "2021-12-13 06:18:48 - Truncated by 1347 tokens\n",
      "2021-12-13 06:18:49 - Truncated by 217 tokens\n",
      "2021-12-13 06:18:51 - Truncated by 7336 tokens\n",
      "2021-12-13 06:18:56 - Truncated by 2021 tokens\n",
      "2021-12-13 06:18:56 - Truncated by 998 tokens\n",
      "2021-12-13 06:19:00 - Truncated by 132 tokens\n",
      "2021-12-13 06:19:00 - Truncated by 396 tokens\n",
      "2021-12-13 06:19:01 - Truncated by 1658 tokens\n",
      "2021-12-13 06:19:04 - Truncated by 137 tokens\n",
      "2021-12-13 06:19:05 - Truncated by 352 tokens\n",
      "2021-12-13 06:19:05 - Truncated by 3677 tokens\n",
      "2021-12-13 06:19:07 - Truncated by 30405 tokens\n",
      "2021-12-13 06:19:08 - Truncated by 7147 tokens\n",
      "2021-12-13 06:19:08 - Truncated by 840 tokens\n",
      "2021-12-13 06:19:10 - Truncated by 570 tokens\n",
      "2021-12-13 06:19:14 - Truncated by 2332 tokens\n",
      "2021-12-13 06:19:16 - Truncated by 329 tokens\n",
      "2021-12-13 06:19:20 - Truncated by 50 tokens\n",
      "2021-12-13 06:19:21 - Truncated by 46153 tokens\n",
      "2021-12-13 06:19:22 - Truncated by 50 tokens\n",
      "2021-12-13 06:19:24 - Truncated by 112 tokens\n",
      "2021-12-13 06:19:25 - Truncated by 24101 tokens\n",
      "2021-12-13 06:19:27 - Truncated by 333 tokens\n",
      "2021-12-13 06:19:35 - Truncated by 753 tokens\n",
      "2021-12-13 06:19:36 - Truncated by 3664 tokens\n",
      "2021-12-13 06:19:37 - Truncated by 189 tokens\n",
      "2021-12-13 06:19:40 - Truncated by 147 tokens\n",
      "2021-12-13 06:19:54 - Truncated by 700 tokens\n",
      "2021-12-13 06:19:55 - Truncated by 336 tokens\n",
      "2021-12-13 06:19:59 - Truncated by 47 tokens\n",
      "2021-12-13 06:20:09 - Truncated by 548 tokens\n",
      "2021-12-13 06:20:15 - Truncated by 1191 tokens\n",
      "2021-12-13 06:20:17 - Truncated by 1289 tokens\n",
      "2021-12-13 06:20:18 - Truncated by 415 tokens\n",
      "2021-12-13 06:20:18 - Truncated by 2505 tokens\n",
      "2021-12-13 06:20:19 - Truncated by 1746 tokens\n",
      "2021-12-13 06:20:20 - Truncated by 1746 tokens\n",
      "2021-12-13 06:20:21 - Truncated by 2330 tokens\n",
      "2021-12-13 06:20:24 - Truncated by 986 tokens\n",
      "2021-12-13 06:20:27 - Truncated by 512 tokens\n",
      "2021-12-13 06:20:28 - Truncated by 448 tokens\n",
      "2021-12-13 06:20:28 - Truncated by 236 tokens\n",
      "2021-12-13 06:20:31 - Truncated by 4675 tokens\n",
      "2021-12-13 06:20:40 - Truncated by 153 tokens\n",
      "2021-12-13 06:20:40 - Truncated by 40 tokens\n",
      "2021-12-13 06:20:42 - Truncated by 126 tokens\n",
      "2021-12-13 06:20:46 - Truncated by 1227 tokens\n",
      "2021-12-13 06:20:52 - Truncated by 80 tokens\n",
      "2021-12-13 06:20:53 - Truncated by 1371 tokens\n",
      "2021-12-13 06:20:55 - Truncated by 1658 tokens\n",
      "2021-12-13 06:20:59 - Truncated by 1168 tokens\n",
      "2021-12-13 06:21:00 - Truncated by 339 tokens\n",
      "2021-12-13 06:21:02 - Truncated by 5343 tokens\n",
      "2021-12-13 06:21:02 - Truncated by 936 tokens\n",
      "2021-12-13 06:21:03 - Truncated by 108 tokens\n",
      "2021-12-13 06:21:05 - Truncated by 335 tokens\n",
      "2021-12-13 06:21:08 - Truncated by 3524 tokens\n",
      "2021-12-13 06:21:10 - Truncated by 293 tokens\n",
      "2021-12-13 06:21:13 - Truncated by 712 tokens\n",
      "2021-12-13 06:21:16 - Truncated by 628 tokens\n",
      "2021-12-13 06:21:17 - Truncated by 426 tokens\n",
      "2021-12-13 06:21:21 - Truncated by 1445 tokens\n",
      "2021-12-13 06:21:25 - Truncated by 85 tokens\n",
      "2021-12-13 06:21:32 - Truncated by 1093 tokens\n",
      "2021-12-13 06:21:33 - Truncated by 767 tokens\n",
      "2021-12-13 06:21:36 - Truncated by 152 tokens\n",
      "2021-12-13 06:21:41 - Truncated by 1238 tokens\n",
      "2021-12-13 06:21:43 - Truncated by 1498 tokens\n",
      "2021-12-13 06:21:45 - Truncated by 1605 tokens\n",
      "2021-12-13 06:21:45 - Truncated by 2833 tokens\n",
      "2021-12-13 06:22:01 - Truncated by 5 tokens\n",
      "2021-12-13 06:22:08 - Truncated by 416 tokens\n",
      "2021-12-13 06:22:10 - Truncated by 364 tokens\n",
      "2021-12-13 06:22:20 - Truncated by 1057 tokens\n",
      "2021-12-13 06:22:31 - Truncated by 1098 tokens\n",
      "2021-12-13 06:22:35 - Truncated by 177 tokens\n",
      "2021-12-13 06:22:41 - Truncated by 3667 tokens\n",
      "2021-12-13 06:22:42 - Truncated by 3695 tokens\n",
      "2021-12-13 06:22:51 - Truncated by 49 tokens\n",
      "2021-12-13 06:22:57 - Truncated by 2114 tokens\n",
      "2021-12-13 06:22:58 - Truncated by 127 tokens\n",
      "2021-12-13 06:23:00 - Truncated by 906 tokens\n",
      "2021-12-13 06:23:05 - Truncated by 774 tokens\n",
      "2021-12-13 06:23:06 - Truncated by 1565 tokens\n",
      "2021-12-13 06:23:07 - Truncated by 1626 tokens\n",
      "2021-12-13 06:23:10 - Truncated by 793 tokens\n",
      "2021-12-13 06:23:12 - Truncated by 2164 tokens\n",
      "2021-12-13 06:23:13 - Truncated by 722 tokens\n",
      "2021-12-13 06:23:14 - Truncated by 466 tokens\n",
      "2021-12-13 06:23:15 - Truncated by 706 tokens\n",
      "2021-12-13 06:23:17 - Truncated by 208 tokens\n",
      "2021-12-13 06:23:21 - Truncated by 880 tokens\n",
      "2021-12-13 06:23:23 - Truncated by 1754 tokens\n",
      "2021-12-13 06:23:24 - Truncated by 259 tokens\n",
      "2021-12-13 06:23:25 - Truncated by 1105 tokens\n",
      "2021-12-13 06:23:31 - Truncated by 240 tokens\n",
      "2021-12-13 06:23:48 - Truncated by 704 tokens\n",
      "2021-12-13 06:23:50 - Truncated by 197 tokens\n",
      "2021-12-13 06:23:54 - Truncated by 137 tokens\n",
      "2021-12-13 06:23:55 - Truncated by 146 tokens\n",
      "2021-12-13 06:24:04 - Truncated by 387 tokens\n",
      "2021-12-13 06:24:11 - Truncated by 249 tokens\n",
      "2021-12-13 06:24:12 - Truncated by 619 tokens\n",
      "2021-12-13 06:24:15 - Truncated by 2195 tokens\n",
      "2021-12-13 06:24:28 - Truncated by 259 tokens\n",
      "2021-12-13 06:24:32 - Truncated by 405 tokens\n",
      "2021-12-13 06:24:35 - Truncated by 2357 tokens\n",
      "2021-12-13 06:24:36 - Truncated by 2357 tokens\n",
      "2021-12-13 06:24:37 - Truncated by 4769 tokens\n",
      "2021-12-13 06:24:38 - Truncated by 4692 tokens\n",
      "2021-12-13 06:24:41 - Truncated by 931 tokens\n",
      "2021-12-13 06:24:43 - Truncated by 5617 tokens\n",
      "2021-12-13 06:24:45 - Truncated by 6444 tokens\n",
      "2021-12-13 06:24:53 - Truncated by 188 tokens\n",
      "2021-12-13 06:25:00 - Truncated by 227 tokens\n",
      "2021-12-13 06:25:03 - Truncated by 7223 tokens\n",
      "2021-12-13 06:25:08 - Truncated by 605 tokens\n",
      "2021-12-13 06:25:10 - Truncated by 1156 tokens\n",
      "2021-12-13 06:25:12 - Truncated by 124 tokens\n",
      "2021-12-13 06:25:12 - Truncated by 584 tokens\n",
      "2021-12-13 06:25:14 - Truncated by 183 tokens\n",
      "2021-12-13 06:25:15 - Truncated by 37 tokens\n",
      "2021-12-13 06:25:17 - Truncated by 2146 tokens\n",
      "2021-12-13 06:25:19 - Truncated by 160 tokens\n",
      "2021-12-13 06:25:19 - Truncated by 8004 tokens\n",
      "2021-12-13 06:25:25 - Truncated by 1855 tokens\n",
      "2021-12-13 06:25:29 - Truncated by 1041 tokens\n",
      "2021-12-13 06:25:32 - Truncated by 2265 tokens\n",
      "2021-12-13 06:25:41 - Truncated by 2716 tokens\n",
      "2021-12-13 06:25:49 - Truncated by 1258 tokens\n",
      "2021-12-13 06:25:50 - Truncated by 1307 tokens\n",
      "2021-12-13 06:25:52 - Truncated by 4345 tokens\n",
      "2021-12-13 06:25:54 - Truncated by 500 tokens\n",
      "2021-12-13 06:25:56 - Truncated by 82 tokens\n",
      "2021-12-13 06:25:57 - Truncated by 90 tokens\n",
      "2021-12-13 06:26:01 - Truncated by 1504 tokens\n",
      "2021-12-13 06:26:04 - Truncated by 233 tokens\n",
      "2021-12-13 06:26:04 - Truncated by 249 tokens\n",
      "2021-12-13 06:26:08 - Truncated by 477 tokens\n",
      "2021-12-13 06:26:10 - Truncated by 2025 tokens\n",
      "2021-12-13 06:26:17 - Truncated by 2090 tokens\n",
      "2021-12-13 06:26:27 - Truncated by 842 tokens\n",
      "2021-12-13 06:26:31 - Truncated by 308 tokens\n",
      "2021-12-13 06:26:33 - Truncated by 1508 tokens\n",
      "2021-12-13 06:26:34 - Truncated by 328 tokens\n",
      "2021-12-13 06:26:34 - Truncated by 154 tokens\n",
      "2021-12-13 06:26:35 - Truncated by 1550 tokens\n",
      "2021-12-13 06:26:40 - Truncated by 1180 tokens\n",
      "2021-12-13 06:26:40 - Truncated by 1180 tokens\n",
      "2021-12-13 06:26:42 - Truncated by 1147 tokens\n",
      "2021-12-13 06:26:51 - Truncated by 779 tokens\n",
      "2021-12-13 06:26:52 - Truncated by 1241 tokens\n",
      "2021-12-13 06:26:54 - Truncated by 15 tokens\n",
      "2021-12-13 06:26:54 - Truncated by 19604 tokens\n",
      "2021-12-13 06:27:00 - Truncated by 145 tokens\n",
      "2021-12-13 06:27:07 - Truncated by 501 tokens\n",
      "2021-12-13 06:27:11 - Truncated by 171 tokens\n",
      "2021-12-13 06:27:23 - Truncated by 428 tokens\n",
      "2021-12-13 06:27:25 - Truncated by 1921 tokens\n",
      "2021-12-13 06:27:27 - Truncated by 84 tokens\n",
      "2021-12-13 06:27:33 - Truncated by 2219 tokens\n",
      "2021-12-13 06:27:34 - Truncated by 601 tokens\n",
      "2021-12-13 06:27:38 - Truncated by 2074 tokens\n",
      "2021-12-13 06:27:41 - Truncated by 58 tokens\n",
      "2021-12-13 06:27:46 - Truncated by 1563 tokens\n",
      "2021-12-13 06:27:47 - Truncated by 71 tokens\n",
      "2021-12-13 06:27:53 - Truncated by 245 tokens\n",
      "2021-12-13 06:27:57 - Truncated by 905 tokens\n",
      "2021-12-13 06:27:58 - Truncated by 191 tokens\n",
      "2021-12-13 06:27:59 - Truncated by 342 tokens\n",
      "2021-12-13 06:28:00 - Truncated by 346 tokens\n",
      "2021-12-13 06:28:01 - Truncated by 900 tokens\n",
      "2021-12-13 06:28:03 - Truncated by 55 tokens\n",
      "2021-12-13 06:28:04 - Truncated by 493 tokens\n",
      "2021-12-13 06:28:07 - Truncated by 195 tokens\n",
      "2021-12-13 06:28:10 - Truncated by 1902 tokens\n",
      "2021-12-13 06:28:12 - Truncated by 9407 tokens\n",
      "2021-12-13 06:28:13 - Truncated by 2827 tokens\n",
      "2021-12-13 06:28:19 - Truncated by 746 tokens\n",
      "2021-12-13 06:28:20 - Truncated by 2012 tokens\n",
      "2021-12-13 06:28:20 - Truncated by 448 tokens\n",
      "2021-12-13 06:28:30 - Truncated by 1376 tokens\n",
      "2021-12-13 06:28:33 - Truncated by 813 tokens\n",
      "2021-12-13 06:28:38 - Truncated by 833 tokens\n",
      "2021-12-13 06:28:44 - Truncated by 45 tokens\n",
      "2021-12-13 06:28:55 - Truncated by 41 tokens\n",
      "2021-12-13 06:28:57 - Truncated by 490 tokens\n",
      "2021-12-13 06:29:02 - Truncated by 626 tokens\n",
      "2021-12-13 06:29:07 - Truncated by 1171 tokens\n",
      "2021-12-13 06:29:08 - Truncated by 1303 tokens\n",
      "2021-12-13 06:29:10 - Truncated by 802 tokens\n",
      "2021-12-13 06:29:12 - Truncated by 492 tokens\n",
      "2021-12-13 06:29:15 - Truncated by 474 tokens\n",
      "2021-12-13 06:29:17 - Truncated by 739 tokens\n",
      "2021-12-13 06:29:27 - Truncated by 638 tokens\n",
      "2021-12-13 06:29:29 - Truncated by 1524 tokens\n",
      "2021-12-13 06:29:34 - Truncated by 335 tokens\n",
      "2021-12-13 06:29:37 - Truncated by 578 tokens\n",
      "2021-12-13 06:29:38 - Truncated by 919 tokens\n",
      "2021-12-13 06:29:44 - Truncated by 671 tokens\n",
      "2021-12-13 06:29:47 - Truncated by 94 tokens\n",
      "2021-12-13 06:29:55 - Truncated by 128 tokens\n",
      "2021-12-13 06:29:57 - Truncated by 2125 tokens\n",
      "2021-12-13 06:29:59 - Truncated by 1303 tokens\n",
      "2021-12-13 06:30:03 - Truncated by 2393 tokens\n",
      "2021-12-13 06:30:05 - Truncated by 1297 tokens\n",
      "2021-12-13 06:30:08 - Truncated by 2860 tokens\n",
      "2021-12-13 06:30:13 - Truncated by 2937 tokens\n",
      "2021-12-13 06:30:13 - Truncated by 283 tokens\n",
      "2021-12-13 06:30:14 - Truncated by 635 tokens\n",
      "2021-12-13 06:30:16 - Truncated by 1490 tokens\n",
      "2021-12-13 06:30:18 - Truncated by 2270 tokens\n",
      "2021-12-13 06:30:30 - Truncated by 49 tokens\n",
      "2021-12-13 06:30:35 - Truncated by 1062 tokens\n",
      "2021-12-13 06:30:38 - Truncated by 292 tokens\n",
      "2021-12-13 06:30:41 - Truncated by 85 tokens\n",
      "2021-12-13 06:30:48 - Truncated by 480 tokens\n",
      "2021-12-13 06:30:51 - Truncated by 1869 tokens\n",
      "2021-12-13 06:30:55 - Truncated by 914 tokens\n",
      "2021-12-13 06:30:56 - Truncated by 364 tokens\n",
      "2021-12-13 06:30:57 - Truncated by 1327 tokens\n",
      "2021-12-13 06:30:59 - Truncated by 918 tokens\n",
      "2021-12-13 06:31:02 - Truncated by 1242 tokens\n",
      "2021-12-13 06:31:02 - Truncated by 1256 tokens\n",
      "2021-12-13 06:31:07 - Truncated by 1682 tokens\n",
      "2021-12-13 06:31:08 - Truncated by 79 tokens\n",
      "2021-12-13 06:31:12 - Truncated by 468 tokens\n",
      "2021-12-13 06:31:13 - Truncated by 991 tokens\n",
      "2021-12-13 06:31:14 - Truncated by 147 tokens\n",
      "2021-12-13 06:31:17 - Truncated by 113 tokens\n",
      "2021-12-13 06:31:18 - Truncated by 100 tokens\n",
      "2021-12-13 06:31:20 - Truncated by 1849 tokens\n",
      "2021-12-13 06:31:22 - Truncated by 2944 tokens\n",
      "2021-12-13 06:31:25 - Truncated by 169 tokens\n",
      "2021-12-13 06:31:36 - Truncated by 29774 tokens\n",
      "2021-12-13 06:31:39 - Truncated by 3357 tokens\n",
      "2021-12-13 06:31:52 - Truncated by 297 tokens\n",
      "2021-12-13 06:31:55 - Truncated by 539 tokens\n",
      "2021-12-13 06:31:57 - Truncated by 3563 tokens\n",
      "2021-12-13 06:32:00 - Truncated by 3766 tokens\n",
      "2021-12-13 06:32:07 - Truncated by 204 tokens\n",
      "2021-12-13 06:32:13 - Truncated by 505 tokens\n",
      "2021-12-13 06:32:20 - Truncated by 536 tokens\n",
      "2021-12-13 06:32:26 - Truncated by 325 tokens\n",
      "2021-12-13 06:32:28 - Truncated by 1384 tokens\n",
      "2021-12-13 06:32:33 - Truncated by 216 tokens\n",
      "2021-12-13 06:32:38 - Truncated by 5132 tokens\n",
      "2021-12-13 06:32:39 - Truncated by 859 tokens\n",
      "2021-12-13 06:32:40 - Truncated by 765 tokens\n",
      "2021-12-13 06:32:43 - Truncated by 1389 tokens\n",
      "2021-12-13 06:32:45 - Truncated by 1476 tokens\n",
      "2021-12-13 06:32:48 - Truncated by 1199 tokens\n",
      "2021-12-13 06:32:51 - Truncated by 1554 tokens\n",
      "2021-12-13 06:32:52 - Truncated by 885 tokens\n",
      "2021-12-13 06:32:55 - Truncated by 3003 tokens\n",
      "2021-12-13 06:32:57 - Truncated by 4599 tokens\n",
      "2021-12-13 06:33:00 - Truncated by 47 tokens\n",
      "2021-12-13 06:33:01 - Truncated by 3031 tokens\n",
      "2021-12-13 06:33:02 - Truncated by 2759 tokens\n",
      "2021-12-13 06:33:04 - Truncated by 424 tokens\n",
      "2021-12-13 06:33:07 - Truncated by 308 tokens\n",
      "2021-12-13 06:33:11 - Truncated by 579 tokens\n",
      "2021-12-13 06:33:12 - Truncated by 5 tokens\n",
      "2021-12-13 06:33:13 - Truncated by 2276 tokens\n",
      "2021-12-13 06:33:16 - Truncated by 5226 tokens\n",
      "2021-12-13 06:33:17 - Truncated by 310 tokens\n",
      "2021-12-13 06:33:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:33:18 - Truncated by 311 tokens\n",
      "2021-12-13 06:33:20 - Truncated by 403 tokens\n",
      "2021-12-13 06:33:26 - Truncated by 50 tokens\n",
      "2021-12-13 06:33:26 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:33:27 - Truncated by 51 tokens\n",
      "2021-12-13 06:33:48 - Truncated by 1485 tokens\n",
      "2021-12-13 06:33:48 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:33:49 - Truncated by 1486 tokens\n",
      "2021-12-13 06:33:59 - Truncated by 567 tokens\n",
      "2021-12-13 06:34:02 - Truncated by 16 tokens\n",
      "2021-12-13 06:34:02 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:34:03 - Truncated by 17 tokens\n",
      "2021-12-13 06:34:03 - Truncated by 1079 tokens\n",
      "2021-12-13 06:34:06 - Truncated by 315 tokens\n",
      "2021-12-13 06:34:06 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:34:07 - Truncated by 316 tokens\n",
      "2021-12-13 06:34:10 - Truncated by 655 tokens\n",
      "2021-12-13 06:34:16 - Truncated by 1554 tokens\n",
      "2021-12-13 06:34:17 - Truncated by 757 tokens\n",
      "2021-12-13 06:34:22 - Truncated by 2307 tokens\n",
      "2021-12-13 06:34:27 - Truncated by 443 tokens\n",
      "2021-12-13 06:34:28 - Truncated by 16 tokens\n",
      "2021-12-13 06:34:34 - Truncated by 2324 tokens\n",
      "2021-12-13 06:34:35 - Truncated by 1130 tokens\n",
      "2021-12-13 06:34:38 - Truncated by 285 tokens\n",
      "2021-12-13 06:34:38 - Truncated by 281 tokens\n",
      "2021-12-13 06:34:39 - Truncated by 675 tokens\n",
      "2021-12-13 06:34:42 - Truncated by 8850 tokens\n",
      "2021-12-13 06:34:45 - Truncated by 1512 tokens\n",
      "2021-12-13 06:35:07 - Truncated by 385 tokens\n",
      "2021-12-13 06:35:09 - Truncated by 1067 tokens\n",
      "2021-12-13 06:35:14 - Truncated by 316 tokens\n",
      "2021-12-13 06:35:17 - Truncated by 207 tokens\n",
      "2021-12-13 06:35:18 - Truncated by 135 tokens\n",
      "2021-12-13 06:35:21 - Truncated by 3234 tokens\n",
      "2021-12-13 06:35:24 - Truncated by 840 tokens\n",
      "2021-12-13 06:35:27 - Truncated by 370 tokens\n",
      "2021-12-13 06:35:29 - Truncated by 957 tokens\n",
      "2021-12-13 06:35:31 - Truncated by 194 tokens\n",
      "2021-12-13 06:35:44 - Truncated by 20 tokens\n",
      "2021-12-13 06:35:49 - Truncated by 863 tokens\n",
      "2021-12-13 06:35:51 - Truncated by 4338 tokens\n",
      "2021-12-13 06:35:57 - Truncated by 1327 tokens\n",
      "2021-12-13 06:35:58 - Truncated by 668 tokens\n",
      "2021-12-13 06:36:03 - Truncated by 1854 tokens\n",
      "2021-12-13 06:36:05 - Truncated by 171 tokens\n",
      "2021-12-13 06:36:08 - Truncated by 1461 tokens\n",
      "2021-12-13 06:36:08 - Truncated by 544 tokens\n",
      "2021-12-13 06:36:09 - Truncated by 1369 tokens\n",
      "2021-12-13 06:36:11 - Truncated by 2692 tokens\n",
      "2021-12-13 06:36:12 - Truncated by 839 tokens\n",
      "2021-12-13 06:36:14 - Truncated by 735 tokens\n",
      "2021-12-13 06:36:16 - Truncated by 156 tokens\n",
      "2021-12-13 06:36:17 - Truncated by 2306 tokens\n",
      "2021-12-13 06:36:26 - Truncated by 1464 tokens\n",
      "2021-12-13 06:36:31 - Truncated by 482 tokens\n",
      "2021-12-13 06:36:37 - Truncated by 2069 tokens\n",
      "2021-12-13 06:36:39 - Truncated by 1392 tokens\n",
      "2021-12-13 06:36:40 - Truncated by 450 tokens\n",
      "2021-12-13 06:36:43 - Truncated by 412 tokens\n",
      "2021-12-13 06:36:44 - Truncated by 355 tokens\n",
      "2021-12-13 06:36:46 - Truncated by 229 tokens\n",
      "2021-12-13 06:36:49 - Truncated by 6744 tokens\n",
      "2021-12-13 06:36:49 - Truncated by 1101 tokens\n",
      "2021-12-13 06:36:50 - Truncated by 1156 tokens\n",
      "2021-12-13 06:36:54 - Truncated by 880 tokens\n",
      "2021-12-13 06:36:55 - Truncated by 484 tokens\n",
      "2021-12-13 06:36:57 - Truncated by 1054 tokens\n",
      "2021-12-13 06:36:58 - Truncated by 13 tokens\n",
      "2021-12-13 06:37:00 - Truncated by 7231 tokens\n",
      "2021-12-13 06:37:04 - Truncated by 1080 tokens\n",
      "2021-12-13 06:37:06 - Truncated by 158 tokens\n",
      "2021-12-13 06:37:07 - Truncated by 1387 tokens\n",
      "2021-12-13 06:37:08 - Truncated by 857 tokens\n",
      "2021-12-13 06:37:09 - Truncated by 352 tokens\n",
      "2021-12-13 06:37:13 - Truncated by 547 tokens\n",
      "2021-12-13 06:37:16 - Truncated by 137 tokens\n",
      "2021-12-13 06:37:20 - Truncated by 995 tokens\n",
      "2021-12-13 06:37:22 - Truncated by 268 tokens\n",
      "2021-12-13 06:37:27 - Truncated by 629 tokens\n",
      "2021-12-13 06:37:28 - Truncated by 3351 tokens\n",
      "2021-12-13 06:37:30 - Truncated by 20 tokens\n",
      "2021-12-13 06:37:53 - Truncated by 9 tokens\n",
      "2021-12-13 06:37:55 - Truncated by 77 tokens\n",
      "2021-12-13 06:37:56 - Truncated by 195 tokens\n",
      "2021-12-13 06:38:04 - Truncated by 371 tokens\n",
      "2021-12-13 06:38:14 - Truncated by 949 tokens\n",
      "2021-12-13 06:38:15 - Truncated by 1431 tokens\n",
      "2021-12-13 06:38:17 - Truncated by 7220 tokens\n",
      "2021-12-13 06:38:19 - Truncated by 581 tokens\n",
      "2021-12-13 06:38:32 - Truncated by 2218 tokens\n",
      "2021-12-13 06:38:35 - Truncated by 2145 tokens\n",
      "2021-12-13 06:38:43 - Truncated by 744 tokens\n",
      "2021-12-13 06:38:53 - Truncated by 852 tokens\n",
      "2021-12-13 06:38:54 - Truncated by 266 tokens\n",
      "2021-12-13 06:38:55 - Truncated by 256 tokens\n",
      "2021-12-13 06:39:02 - Truncated by 310 tokens\n",
      "2021-12-13 06:39:14 - Truncated by 49 tokens\n",
      "2021-12-13 06:39:16 - Truncated by 253 tokens\n",
      "2021-12-13 06:39:19 - Truncated by 464 tokens\n",
      "2021-12-13 06:39:29 - Truncated by 910 tokens\n",
      "2021-12-13 06:39:30 - Truncated by 239 tokens\n",
      "2021-12-13 06:39:31 - Truncated by 157 tokens\n",
      "2021-12-13 06:39:33 - Truncated by 1318 tokens\n",
      "2021-12-13 06:39:36 - Truncated by 419 tokens\n",
      "2021-12-13 06:39:40 - Truncated by 361 tokens\n",
      "2021-12-13 06:39:42 - Truncated by 486 tokens\n",
      "2021-12-13 06:39:43 - Truncated by 248 tokens\n",
      "2021-12-13 06:39:44 - Truncated by 390 tokens\n",
      "2021-12-13 06:39:45 - Truncated by 4134 tokens\n",
      "2021-12-13 06:39:46 - Truncated by 42 tokens\n",
      "2021-12-13 06:39:51 - Truncated by 639 tokens\n",
      "2021-12-13 06:39:52 - Truncated by 142 tokens\n",
      "2021-12-13 06:39:55 - Truncated by 656 tokens\n",
      "2021-12-13 06:39:57 - Truncated by 663 tokens\n",
      "2021-12-13 06:39:58 - Truncated by 62 tokens\n",
      "2021-12-13 06:39:59 - Truncated by 252 tokens\n",
      "2021-12-13 06:40:01 - Truncated by 3506 tokens\n",
      "2021-12-13 06:40:05 - Truncated by 1650 tokens\n",
      "2021-12-13 06:40:08 - Truncated by 1209 tokens\n",
      "2021-12-13 06:40:08 - Truncated by 694 tokens\n",
      "2021-12-13 06:40:12 - Truncated by 932 tokens\n",
      "2021-12-13 06:40:18 - Truncated by 1353 tokens\n",
      "2021-12-13 06:40:19 - Truncated by 478 tokens\n",
      "2021-12-13 06:40:21 - Truncated by 423 tokens\n",
      "2021-12-13 06:40:23 - Truncated by 52 tokens\n",
      "2021-12-13 06:40:25 - Truncated by 12033 tokens\n",
      "2021-12-13 06:40:26 - Truncated by 527 tokens\n",
      "2021-12-13 06:40:28 - Truncated by 474 tokens\n",
      "2021-12-13 06:40:28 - Truncated by 918 tokens\n",
      "2021-12-13 06:40:29 - Truncated by 435 tokens\n",
      "2021-12-13 06:40:29 - Truncated by 302 tokens\n",
      "2021-12-13 06:40:30 - Truncated by 52 tokens\n",
      "2021-12-13 06:40:31 - Truncated by 1122 tokens\n",
      "2021-12-13 06:40:32 - Truncated by 695 tokens\n",
      "2021-12-13 06:40:33 - Truncated by 1770 tokens\n",
      "2021-12-13 06:40:36 - Truncated by 628 tokens\n",
      "2021-12-13 06:40:36 - Truncated by 106 tokens\n",
      "2021-12-13 06:41:17 - Truncated by 2163 tokens\n",
      "2021-12-13 06:41:18 - Truncated by 114 tokens\n",
      "2021-12-13 06:41:20 - Truncated by 2017 tokens\n",
      "2021-12-13 06:41:44 - Truncated by 3906 tokens\n",
      "2021-12-13 06:41:45 - Truncated by 1340 tokens\n",
      "2021-12-13 06:41:48 - Truncated by 197 tokens\n",
      "2021-12-13 06:41:53 - Truncated by 1134 tokens\n",
      "2021-12-13 06:41:56 - Truncated by 456 tokens\n",
      "2021-12-13 06:41:58 - Truncated by 1150 tokens\n",
      "2021-12-13 06:41:59 - Truncated by 666 tokens\n",
      "2021-12-13 06:42:03 - Truncated by 1240 tokens\n",
      "2021-12-13 06:42:04 - Truncated by 39027 tokens\n",
      "2021-12-13 06:42:04 - Truncated by 69 tokens\n",
      "2021-12-13 06:42:10 - Truncated by 2427 tokens\n",
      "2021-12-13 06:42:11 - Truncated by 6962 tokens\n",
      "2021-12-13 06:42:15 - Truncated by 1302 tokens\n",
      "2021-12-13 06:42:19 - Truncated by 2556 tokens\n",
      "2021-12-13 06:42:23 - Truncated by 394 tokens\n",
      "2021-12-13 06:42:26 - Truncated by 186 tokens\n",
      "2021-12-13 06:42:27 - Truncated by 525 tokens\n",
      "2021-12-13 06:42:28 - Truncated by 778 tokens\n",
      "2021-12-13 06:42:30 - Truncated by 253 tokens\n",
      "2021-12-13 06:42:30 - Truncated by 179 tokens\n",
      "2021-12-13 06:42:34 - Truncated by 5226 tokens\n",
      "2021-12-13 06:42:35 - Truncated by 2037 tokens\n",
      "2021-12-13 06:42:41 - Truncated by 2241 tokens\n",
      "2021-12-13 06:42:42 - Truncated by 128 tokens\n",
      "2021-12-13 06:42:43 - Truncated by 920 tokens\n",
      "2021-12-13 06:42:44 - Truncated by 502 tokens\n",
      "2021-12-13 06:42:50 - Truncated by 1079 tokens\n",
      "2021-12-13 06:42:51 - Truncated by 873 tokens\n",
      "2021-12-13 06:42:54 - Truncated by 3135 tokens\n",
      "2021-12-13 06:42:56 - Truncated by 1599 tokens\n",
      "2021-12-13 06:42:57 - Truncated by 541 tokens\n",
      "2021-12-13 06:42:57 - Truncated by 885 tokens\n",
      "2021-12-13 06:42:58 - Truncated by 62 tokens\n",
      "2021-12-13 06:43:00 - Truncated by 1500 tokens\n",
      "2021-12-13 06:43:04 - Truncated by 3999 tokens\n",
      "2021-12-13 06:43:05 - Truncated by 2209 tokens\n",
      "2021-12-13 06:43:05 - Truncated by 2632 tokens\n",
      "2021-12-13 06:43:08 - Truncated by 945 tokens\n",
      "2021-12-13 06:43:11 - Truncated by 3 tokens\n",
      "2021-12-13 06:43:12 - Truncated by 1510 tokens\n",
      "2021-12-13 06:43:13 - Truncated by 86 tokens\n",
      "2021-12-13 06:43:18 - Truncated by 3422 tokens\n",
      "2021-12-13 06:43:27 - Truncated by 552 tokens\n",
      "2021-12-13 06:43:40 - Truncated by 4575 tokens\n",
      "2021-12-13 06:43:41 - Truncated by 72735 tokens\n",
      "2021-12-13 06:43:51 - Truncated by 1258 tokens\n",
      "2021-12-13 06:44:01 - Truncated by 1166 tokens\n",
      "2021-12-13 06:44:10 - Truncated by 165 tokens\n",
      "2021-12-13 06:44:15 - Truncated by 581 tokens\n",
      "2021-12-13 06:44:18 - Truncated by 1837 tokens\n",
      "2021-12-13 06:44:18 - Truncated by 2363 tokens\n",
      "2021-12-13 06:44:19 - Truncated by 945 tokens\n",
      "2021-12-13 06:44:22 - Truncated by 923 tokens\n",
      "2021-12-13 06:44:27 - Truncated by 701 tokens\n",
      "2021-12-13 06:44:28 - Truncated by 966 tokens\n",
      "2021-12-13 06:44:30 - Truncated by 3799 tokens\n",
      "2021-12-13 06:44:33 - Truncated by 1996 tokens\n",
      "2021-12-13 06:44:37 - Truncated by 288 tokens\n",
      "2021-12-13 06:44:37 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:44:38 - Truncated by 289 tokens\n",
      "2021-12-13 06:44:41 - Truncated by 219 tokens\n",
      "2021-12-13 06:44:41 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:44:42 - Truncated by 220 tokens\n",
      "2021-12-13 06:44:45 - Truncated by 2289 tokens\n",
      "2021-12-13 06:44:51 - Truncated by 1440 tokens\n",
      "2021-12-13 06:44:54 - Truncated by 217 tokens\n",
      "2021-12-13 06:44:56 - Truncated by 174 tokens\n",
      "2021-12-13 06:44:59 - Truncated by 122 tokens\n",
      "2021-12-13 06:44:59 - Truncated by 268 tokens\n",
      "2021-12-13 06:44:59 - Truncated by 213 tokens\n",
      "2021-12-13 06:45:05 - Truncated by 173 tokens\n",
      "2021-12-13 06:45:11 - Truncated by 1443 tokens\n",
      "2021-12-13 06:45:12 - Truncated by 850 tokens\n",
      "2021-12-13 06:45:13 - Truncated by 169 tokens\n",
      "2021-12-13 06:45:14 - Truncated by 1669 tokens\n",
      "2021-12-13 06:45:14 - Truncated by 2126 tokens\n",
      "2021-12-13 06:45:17 - Truncated by 136 tokens\n",
      "2021-12-13 06:45:17 - Truncated by 214 tokens\n",
      "2021-12-13 06:45:29 - Truncated by 563 tokens\n",
      "2021-12-13 06:45:38 - Truncated by 762 tokens\n",
      "2021-12-13 06:45:52 - Truncated by 145 tokens\n",
      "2021-12-13 06:45:54 - Truncated by 435 tokens\n",
      "2021-12-13 06:46:03 - Truncated by 422 tokens\n",
      "2021-12-13 06:46:04 - Truncated by 1411 tokens\n",
      "2021-12-13 06:46:08 - Truncated by 268 tokens\n",
      "2021-12-13 06:46:10 - Truncated by 1405 tokens\n",
      "2021-12-13 06:46:14 - Truncated by 349 tokens\n",
      "2021-12-13 06:46:17 - Truncated by 5377 tokens\n",
      "2021-12-13 06:46:18 - Truncated by 1567 tokens\n",
      "2021-12-13 06:46:24 - Truncated by 115 tokens\n",
      "2021-12-13 06:46:29 - Truncated by 386 tokens\n",
      "2021-12-13 06:46:33 - Truncated by 3465 tokens\n",
      "2021-12-13 06:46:36 - Truncated by 692 tokens\n",
      "2021-12-13 06:46:40 - Truncated by 4376 tokens\n",
      "2021-12-13 06:46:57 - Truncated by 92 tokens\n",
      "2021-12-13 06:47:00 - Truncated by 1134 tokens\n",
      "2021-12-13 06:47:00 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:47:01 - Truncated by 1135 tokens\n",
      "2021-12-13 06:47:17 - Truncated by 4283 tokens\n",
      "2021-12-13 06:47:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:47:18 - Truncated by 4284 tokens\n",
      "2021-12-13 06:47:33 - Truncated by 625 tokens\n",
      "2021-12-13 06:47:33 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:47:34 - Truncated by 626 tokens\n",
      "2021-12-13 06:47:34 - Truncated by 668 tokens\n",
      "2021-12-13 06:47:39 - Truncated by 1489 tokens\n",
      "2021-12-13 06:47:40 - Truncated by 1655 tokens\n",
      "2021-12-13 06:47:46 - Truncated by 418 tokens\n",
      "2021-12-13 06:47:48 - Truncated by 699 tokens\n",
      "2021-12-13 06:47:50 - Truncated by 1432 tokens\n",
      "2021-12-13 06:47:51 - Truncated by 1658 tokens\n",
      "2021-12-13 06:47:51 - Truncated by 3062 tokens\n",
      "2021-12-13 06:47:53 - Truncated by 1849 tokens\n",
      "2021-12-13 06:47:55 - Truncated by 1667 tokens\n",
      "2021-12-13 06:47:59 - Truncated by 24 tokens\n",
      "2021-12-13 06:48:00 - Truncated by 67 tokens\n",
      "2021-12-13 06:48:05 - Truncated by 264 tokens\n",
      "2021-12-13 06:48:05 - Truncated by 413 tokens\n",
      "2021-12-13 06:48:10 - Truncated by 124 tokens\n",
      "2021-12-13 06:48:12 - Truncated by 485 tokens\n",
      "2021-12-13 06:48:13 - Truncated by 586 tokens\n",
      "2021-12-13 06:48:17 - Truncated by 2224 tokens\n",
      "2021-12-13 06:48:19 - Truncated by 195 tokens\n",
      "2021-12-13 06:48:23 - Truncated by 6 tokens\n",
      "2021-12-13 06:48:32 - Truncated by 310 tokens\n",
      "2021-12-13 06:48:33 - Truncated by 637 tokens\n",
      "2021-12-13 06:48:37 - Truncated by 539 tokens\n",
      "2021-12-13 06:48:40 - Truncated by 947 tokens\n",
      "2021-12-13 06:48:41 - Truncated by 295 tokens\n",
      "2021-12-13 06:48:48 - Truncated by 107 tokens\n",
      "2021-12-13 06:48:53 - Truncated by 393 tokens\n",
      "2021-12-13 06:49:00 - Truncated by 132 tokens\n",
      "2021-12-13 06:49:01 - Truncated by 73 tokens\n",
      "2021-12-13 06:49:06 - Truncated by 6 tokens\n",
      "2021-12-13 06:49:10 - Truncated by 225 tokens\n",
      "2021-12-13 06:49:13 - Truncated by 860 tokens\n",
      "2021-12-13 06:49:19 - Truncated by 423 tokens\n",
      "2021-12-13 06:49:21 - Truncated by 622 tokens\n",
      "2021-12-13 06:49:22 - Truncated by 69 tokens\n",
      "2021-12-13 06:49:24 - Truncated by 176 tokens\n",
      "2021-12-13 06:49:27 - Truncated by 76 tokens\n",
      "2021-12-13 06:49:38 - Truncated by 1942 tokens\n",
      "2021-12-13 06:49:41 - Truncated by 250 tokens\n",
      "2021-12-13 06:49:46 - Truncated by 5348 tokens\n",
      "2021-12-13 06:49:55 - Truncated by 747 tokens\n",
      "2021-12-13 06:50:06 - Truncated by 1681 tokens\n",
      "2021-12-13 06:50:08 - Truncated by 309 tokens\n",
      "2021-12-13 06:50:09 - Truncated by 309 tokens\n",
      "2021-12-13 06:50:11 - Truncated by 63 tokens\n",
      "2021-12-13 06:50:16 - Truncated by 274 tokens\n",
      "2021-12-13 06:50:17 - Truncated by 2124 tokens\n",
      "2021-12-13 06:50:18 - Truncated by 2126 tokens\n",
      "2021-12-13 06:50:20 - Truncated by 5041 tokens\n",
      "2021-12-13 06:50:21 - Truncated by 2675 tokens\n",
      "2021-12-13 06:50:28 - Truncated by 291 tokens\n",
      "2021-12-13 06:50:33 - Truncated by 353 tokens\n",
      "2021-12-13 06:50:33 - Truncated by 9707 tokens\n",
      "2021-12-13 06:50:34 - Truncated by 753 tokens\n",
      "2021-12-13 06:50:35 - Truncated by 12465 tokens\n",
      "2021-12-13 06:50:38 - Truncated by 3341 tokens\n",
      "2021-12-13 06:50:39 - Truncated by 2199 tokens\n",
      "2021-12-13 06:50:39 - Truncated by 5136 tokens\n",
      "2021-12-13 06:50:42 - Truncated by 723 tokens\n",
      "2021-12-13 06:50:42 - Truncated by 527 tokens\n",
      "2021-12-13 06:50:46 - Truncated by 3048 tokens\n",
      "2021-12-13 06:50:47 - Truncated by 164 tokens\n",
      "2021-12-13 06:50:47 - Truncated by 203 tokens\n",
      "2021-12-13 06:50:48 - Truncated by 616 tokens\n",
      "2021-12-13 06:50:49 - Truncated by 2581 tokens\n",
      "2021-12-13 06:50:50 - Truncated by 13958 tokens\n",
      "2021-12-13 06:50:51 - Truncated by 2787 tokens\n",
      "2021-12-13 06:50:52 - Truncated by 352 tokens\n",
      "2021-12-13 06:50:53 - Truncated by 389 tokens\n",
      "2021-12-13 06:50:55 - Truncated by 6239 tokens\n",
      "2021-12-13 06:50:56 - Truncated by 1838 tokens\n",
      "2021-12-13 06:50:57 - Truncated by 1246 tokens\n",
      "2021-12-13 06:50:59 - Truncated by 828 tokens\n",
      "2021-12-13 06:51:04 - Truncated by 329 tokens\n",
      "2021-12-13 06:51:09 - Truncated by 667 tokens\n",
      "2021-12-13 06:51:16 - Truncated by 1935 tokens\n",
      "2021-12-13 06:51:19 - Truncated by 3445 tokens\n",
      "2021-12-13 06:51:23 - Truncated by 50 tokens\n",
      "2021-12-13 06:51:31 - Truncated by 2072 tokens\n",
      "2021-12-13 06:51:41 - Truncated by 666 tokens\n",
      "2021-12-13 06:51:44 - Truncated by 381 tokens\n",
      "2021-12-13 06:51:45 - Truncated by 409 tokens\n",
      "2021-12-13 06:51:51 - Truncated by 557 tokens\n",
      "2021-12-13 06:51:53 - Truncated by 88 tokens\n",
      "2021-12-13 06:51:56 - Truncated by 574 tokens\n",
      "2021-12-13 06:51:58 - Truncated by 742 tokens\n",
      "2021-12-13 06:52:00 - Truncated by 300 tokens\n",
      "2021-12-13 06:52:03 - Truncated by 63 tokens\n",
      "2021-12-13 06:52:04 - Truncated by 128 tokens\n",
      "2021-12-13 06:52:06 - Truncated by 33 tokens\n",
      "2021-12-13 06:52:09 - Truncated by 187 tokens\n",
      "2021-12-13 06:52:21 - Truncated by 444 tokens\n",
      "2021-12-13 06:52:25 - Truncated by 1079 tokens\n",
      "2021-12-13 06:52:30 - Truncated by 403 tokens\n",
      "2021-12-13 06:52:31 - Truncated by 528 tokens\n",
      "2021-12-13 06:52:31 - Truncated by 425 tokens\n",
      "2021-12-13 06:52:36 - Truncated by 411 tokens\n",
      "2021-12-13 06:52:42 - Truncated by 665 tokens\n",
      "2021-12-13 06:52:43 - Truncated by 293 tokens\n",
      "2021-12-13 06:52:43 - Truncated by 103 tokens\n",
      "2021-12-13 06:53:00 - Truncated by 110 tokens\n",
      "2021-12-13 06:53:07 - Truncated by 56 tokens\n",
      "2021-12-13 06:53:17 - Truncated by 68 tokens\n",
      "2021-12-13 06:53:33 - Truncated by 2221 tokens\n",
      "2021-12-13 06:53:34 - Truncated by 450 tokens\n",
      "2021-12-13 06:53:35 - Truncated by 134 tokens\n",
      "2021-12-13 06:53:37 - Truncated by 35 tokens\n",
      "2021-12-13 06:53:38 - Truncated by 5286 tokens\n",
      "2021-12-13 06:53:40 - Truncated by 1249 tokens\n",
      "2021-12-13 06:53:41 - Truncated by 447 tokens\n",
      "2021-12-13 06:53:44 - Truncated by 490 tokens\n",
      "2021-12-13 06:53:49 - Truncated by 1958 tokens\n",
      "2021-12-13 06:53:52 - Truncated by 416 tokens\n",
      "2021-12-13 06:53:53 - Truncated by 26 tokens\n",
      "2021-12-13 06:53:54 - Truncated by 5364 tokens\n",
      "2021-12-13 06:53:55 - Truncated by 461 tokens\n",
      "2021-12-13 06:53:56 - Truncated by 1241 tokens\n",
      "2021-12-13 06:53:57 - Truncated by 323 tokens\n",
      "2021-12-13 06:54:01 - Truncated by 849 tokens\n",
      "2021-12-13 06:54:02 - Truncated by 4217 tokens\n",
      "2021-12-13 06:54:07 - Truncated by 408 tokens\n",
      "2021-12-13 06:54:08 - Truncated by 346 tokens\n",
      "2021-12-13 06:54:09 - Truncated by 1322 tokens\n",
      "2021-12-13 06:54:14 - Truncated by 553 tokens\n",
      "2021-12-13 06:54:16 - Truncated by 1184 tokens\n",
      "2021-12-13 06:54:18 - Truncated by 590 tokens\n",
      "2021-12-13 06:54:19 - Truncated by 383 tokens\n",
      "2021-12-13 06:54:21 - Truncated by 2214 tokens\n",
      "2021-12-13 06:54:22 - Truncated by 746 tokens\n",
      "2021-12-13 06:54:23 - Truncated by 746 tokens\n",
      "2021-12-13 06:54:25 - Truncated by 779 tokens\n",
      "2021-12-13 06:54:26 - Truncated by 3016 tokens\n",
      "2021-12-13 06:54:35 - Truncated by 1043 tokens\n",
      "2021-12-13 06:54:41 - Truncated by 250 tokens\n",
      "2021-12-13 06:54:53 - Truncated by 778 tokens\n",
      "2021-12-13 06:54:56 - Truncated by 28 tokens\n",
      "2021-12-13 06:54:57 - Truncated by 122 tokens\n",
      "2021-12-13 06:55:02 - Truncated by 1825 tokens\n",
      "2021-12-13 06:55:13 - Truncated by 27 tokens\n",
      "2021-12-13 06:55:14 - Truncated by 223 tokens\n",
      "2021-12-13 06:55:16 - Truncated by 365 tokens\n",
      "2021-12-13 06:55:17 - Truncated by 525 tokens\n",
      "2021-12-13 06:55:18 - Truncated by 584 tokens\n",
      "2021-12-13 06:55:20 - Truncated by 37 tokens\n",
      "2021-12-13 06:55:26 - Truncated by 6762 tokens\n",
      "2021-12-13 06:55:27 - Truncated by 3366 tokens\n",
      "2021-12-13 06:55:31 - Truncated by 3981 tokens\n",
      "2021-12-13 06:55:32 - Truncated by 818 tokens\n",
      "2021-12-13 06:55:34 - Truncated by 760 tokens\n",
      "2021-12-13 06:55:38 - Truncated by 4867 tokens\n",
      "2021-12-13 06:55:38 - Truncated by 1794 tokens\n",
      "2021-12-13 06:55:40 - Truncated by 1187 tokens\n",
      "2021-12-13 06:55:43 - Truncated by 639 tokens\n",
      "2021-12-13 06:55:44 - Truncated by 372 tokens\n",
      "2021-12-13 06:55:54 - Truncated by 150 tokens\n",
      "2021-12-13 06:55:55 - Truncated by 34 tokens\n",
      "2021-12-13 06:56:00 - Truncated by 723 tokens\n",
      "2021-12-13 06:56:00 - Truncated by 1565 tokens\n",
      "2021-12-13 06:56:01 - Truncated by 47 tokens\n",
      "2021-12-13 06:56:04 - Truncated by 47 tokens\n",
      "2021-12-13 06:56:05 - Truncated by 1311 tokens\n",
      "2021-12-13 06:56:14 - Truncated by 715 tokens\n",
      "2021-12-13 06:56:17 - Truncated by 2366 tokens\n",
      "2021-12-13 06:56:17 - Truncated by 2366 tokens\n",
      "2021-12-13 06:56:22 - Truncated by 590 tokens\n",
      "2021-12-13 06:56:29 - Truncated by 4285 tokens\n",
      "2021-12-13 06:56:30 - Truncated by 4754 tokens\n",
      "2021-12-13 06:56:35 - Truncated by 721 tokens\n",
      "2021-12-13 06:56:37 - Truncated by 11123 tokens\n",
      "2021-12-13 06:56:42 - Truncated by 637 tokens\n",
      "2021-12-13 06:56:55 - Truncated by 909 tokens\n",
      "2021-12-13 06:56:56 - Truncated by 3005 tokens\n",
      "2021-12-13 06:56:58 - Truncated by 579 tokens\n",
      "2021-12-13 06:56:59 - Truncated by 580 tokens\n",
      "2021-12-13 06:56:59 - Truncated by 570 tokens\n",
      "2021-12-13 06:57:06 - Truncated by 650 tokens\n",
      "2021-12-13 06:57:09 - Truncated by 944 tokens\n",
      "2021-12-13 06:57:10 - Truncated by 3851 tokens\n",
      "2021-12-13 06:57:13 - Truncated by 59 tokens\n",
      "2021-12-13 06:57:14 - Truncated by 256 tokens\n",
      "2021-12-13 06:57:16 - Truncated by 182 tokens\n",
      "2021-12-13 06:57:16 - Truncated by 271 tokens\n",
      "2021-12-13 06:57:17 - Truncated by 337 tokens\n",
      "2021-12-13 06:57:19 - Truncated by 528 tokens\n",
      "2021-12-13 06:57:20 - Truncated by 2040 tokens\n",
      "2021-12-13 06:57:20 - Truncated by 484 tokens\n",
      "2021-12-13 06:57:25 - Truncated by 1859 tokens\n",
      "2021-12-13 06:57:27 - Truncated by 402 tokens\n",
      "2021-12-13 06:57:31 - Truncated by 100 tokens\n",
      "2021-12-13 06:57:37 - Truncated by 411 tokens\n",
      "2021-12-13 06:57:38 - Truncated by 689 tokens\n",
      "2021-12-13 06:57:39 - Truncated by 4528 tokens\n",
      "2021-12-13 06:57:41 - Truncated by 6002 tokens\n",
      "2021-12-13 06:57:42 - Truncated by 1722 tokens\n",
      "2021-12-13 06:57:46 - Truncated by 437 tokens\n",
      "2021-12-13 06:57:48 - Truncated by 47 tokens\n",
      "2021-12-13 06:57:48 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:57:49 - Truncated by 48 tokens\n",
      "2021-12-13 06:57:51 - Truncated by 213 tokens\n",
      "2021-12-13 06:57:51 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:57:52 - Truncated by 214 tokens\n",
      "2021-12-13 06:57:55 - Truncated by 230 tokens\n",
      "2021-12-13 06:57:55 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:57:56 - Truncated by 231 tokens\n",
      "2021-12-13 06:57:57 - Truncated by 237 tokens\n",
      "2021-12-13 06:57:57 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:57:58 - Truncated by 238 tokens\n",
      "2021-12-13 06:58:03 - Truncated by 73 tokens\n",
      "2021-12-13 06:58:03 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:58:04 - Truncated by 74 tokens\n",
      "2021-12-13 06:58:05 - Truncated by 1327 tokens\n",
      "2021-12-13 06:58:05 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:58:06 - Truncated by 1328 tokens\n",
      "2021-12-13 06:58:07 - Truncated by 265 tokens\n",
      "2021-12-13 06:58:07 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:58:08 - Truncated by 266 tokens\n",
      "2021-12-13 06:58:09 - Truncated by 5498 tokens\n",
      "2021-12-13 06:58:13 - Truncated by 66493 tokens\n",
      "2021-12-13 06:58:17 - Truncated by 3387 tokens\n",
      "2021-12-13 06:58:17 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:58:18 - Truncated by 3388 tokens\n",
      "2021-12-13 06:58:19 - Truncated by 1141 tokens\n",
      "2021-12-13 06:58:23 - Truncated by 4466 tokens\n",
      "2021-12-13 06:58:24 - Truncated by 275 tokens\n",
      "2021-12-13 06:58:26 - Truncated by 1565 tokens\n",
      "2021-12-13 06:58:26 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:58:27 - Truncated by 1566 tokens\n",
      "2021-12-13 06:58:29 - Truncated by 2068 tokens\n",
      "2021-12-13 06:58:31 - Truncated by 161 tokens\n",
      "2021-12-13 06:58:31 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 06:58:32 - Truncated by 162 tokens\n",
      "2021-12-13 06:58:33 - Truncated by 619 tokens\n",
      "2021-12-13 06:58:33 - Truncated by 2258 tokens\n",
      "2021-12-13 06:58:34 - Truncated by 2467 tokens\n",
      "2021-12-13 06:58:35 - Truncated by 2467 tokens\n",
      "2021-12-13 06:58:37 - Truncated by 5370 tokens\n",
      "2021-12-13 06:58:38 - Truncated by 2141 tokens\n",
      "2021-12-13 06:58:38 - Truncated by 1348 tokens\n",
      "2021-12-13 06:58:42 - Truncated by 1486 tokens\n",
      "2021-12-13 06:58:43 - Truncated by 1450 tokens\n",
      "2021-12-13 06:58:43 - Truncated by 6813 tokens\n",
      "2021-12-13 06:58:44 - Truncated by 3141 tokens\n",
      "2021-12-13 06:58:45 - Truncated by 517 tokens\n",
      "2021-12-13 06:58:46 - Truncated by 2117 tokens\n",
      "2021-12-13 06:58:48 - Truncated by 2248 tokens\n",
      "2021-12-13 06:58:49 - Truncated by 246 tokens\n",
      "2021-12-13 06:58:50 - Truncated by 60 tokens\n",
      "2021-12-13 06:58:58 - Truncated by 834 tokens\n",
      "2021-12-13 06:59:01 - Truncated by 2998 tokens\n",
      "2021-12-13 06:59:03 - Truncated by 1566 tokens\n",
      "2021-12-13 06:59:04 - Truncated by 644 tokens\n",
      "2021-12-13 06:59:08 - Truncated by 230 tokens\n",
      "2021-12-13 06:59:08 - Truncated by 1194 tokens\n",
      "2021-12-13 06:59:09 - Truncated by 1191 tokens\n",
      "2021-12-13 06:59:10 - Truncated by 501 tokens\n",
      "2021-12-13 06:59:11 - Truncated by 2568 tokens\n",
      "2021-12-13 06:59:12 - Truncated by 19744 tokens\n",
      "2021-12-13 06:59:13 - Truncated by 4479 tokens\n",
      "2021-12-13 06:59:15 - Truncated by 2820 tokens\n",
      "2021-12-13 06:59:17 - Truncated by 832 tokens\n",
      "2021-12-13 06:59:20 - Truncated by 23 tokens\n",
      "2021-12-13 06:59:27 - Truncated by 276 tokens\n",
      "2021-12-13 06:59:30 - Truncated by 94 tokens\n",
      "2021-12-13 06:59:33 - Truncated by 504 tokens\n",
      "2021-12-13 06:59:43 - Truncated by 849 tokens\n",
      "2021-12-13 06:59:44 - Truncated by 3078 tokens\n",
      "2021-12-13 06:59:47 - Truncated by 1025 tokens\n",
      "2021-12-13 06:59:47 - Truncated by 1027 tokens\n",
      "2021-12-13 06:59:52 - Truncated by 3100 tokens\n",
      "2021-12-13 07:00:11 - Truncated by 670 tokens\n",
      "2021-12-13 07:00:17 - Truncated by 1073 tokens\n",
      "2021-12-13 07:00:33 - Truncated by 655 tokens\n",
      "2021-12-13 07:00:34 - Truncated by 881 tokens\n",
      "2021-12-13 07:00:38 - Truncated by 47 tokens\n",
      "2021-12-13 07:00:38 - Truncated by 857 tokens\n",
      "2021-12-13 07:00:39 - Truncated by 23 tokens\n",
      "2021-12-13 07:00:44 - Truncated by 942 tokens\n",
      "2021-12-13 07:00:49 - Truncated by 5222 tokens\n",
      "2021-12-13 07:00:53 - Truncated by 487 tokens\n",
      "2021-12-13 07:00:57 - Truncated by 235 tokens\n",
      "2021-12-13 07:01:05 - Truncated by 415 tokens\n",
      "2021-12-13 07:01:06 - Truncated by 829 tokens\n",
      "2021-12-13 07:01:09 - Truncated by 622 tokens\n",
      "2021-12-13 07:01:10 - Truncated by 622 tokens\n",
      "2021-12-13 07:01:10 - Truncated by 340 tokens\n",
      "2021-12-13 07:01:12 - Truncated by 212 tokens\n",
      "2021-12-13 07:01:12 - Truncated by 2459 tokens\n",
      "2021-12-13 07:01:16 - Truncated by 37 tokens\n",
      "2021-12-13 07:01:17 - Truncated by 1462 tokens\n",
      "2021-12-13 07:01:20 - Truncated by 112 tokens\n",
      "2021-12-13 07:01:22 - Truncated by 566 tokens\n",
      "2021-12-13 07:01:24 - Truncated by 1313 tokens\n",
      "2021-12-13 07:01:24 - Truncated by 2169 tokens\n",
      "2021-12-13 07:01:26 - Truncated by 2888 tokens\n",
      "2021-12-13 07:01:27 - Truncated by 601 tokens\n",
      "2021-12-13 07:01:27 - Truncated by 614 tokens\n",
      "2021-12-13 07:01:29 - Truncated by 543 tokens\n",
      "2021-12-13 07:01:31 - Truncated by 1227 tokens\n",
      "2021-12-13 07:01:34 - Truncated by 3077 tokens\n",
      "2021-12-13 07:01:35 - Truncated by 1401 tokens\n",
      "2021-12-13 07:01:36 - Truncated by 75 tokens\n",
      "2021-12-13 07:01:48 - Truncated by 2077 tokens\n",
      "2021-12-13 07:01:51 - Truncated by 150 tokens\n",
      "2021-12-13 07:01:52 - Truncated by 73 tokens\n",
      "2021-12-13 07:01:53 - Truncated by 1431 tokens\n",
      "2021-12-13 07:01:59 - Truncated by 297 tokens\n",
      "2021-12-13 07:02:00 - Truncated by 1060 tokens\n",
      "2021-12-13 07:02:01 - Truncated by 301 tokens\n",
      "2021-12-13 07:02:09 - Truncated by 1177 tokens\n",
      "2021-12-13 07:02:10 - Truncated by 331 tokens\n",
      "2021-12-13 07:02:11 - Truncated by 232 tokens\n",
      "2021-12-13 07:02:12 - Truncated by 692 tokens\n",
      "2021-12-13 07:02:12 - Truncated by 330 tokens\n",
      "2021-12-13 07:02:14 - Truncated by 880 tokens\n",
      "2021-12-13 07:02:14 - Truncated by 558 tokens\n",
      "2021-12-13 07:02:17 - Truncated by 210 tokens\n",
      "2021-12-13 07:02:18 - Truncated by 439 tokens\n",
      "2021-12-13 07:02:20 - Truncated by 239 tokens\n",
      "2021-12-13 07:02:29 - Truncated by 108 tokens\n",
      "2021-12-13 07:02:35 - Truncated by 361 tokens\n",
      "2021-12-13 07:02:35 - Truncated by 542 tokens\n",
      "2021-12-13 07:02:36 - Truncated by 631 tokens\n",
      "2021-12-13 07:02:41 - Truncated by 52 tokens\n",
      "2021-12-13 07:03:02 - Truncated by 447 tokens\n",
      "2021-12-13 07:03:02 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:03:03 - Truncated by 448 tokens\n",
      "2021-12-13 07:03:09 - Truncated by 1661 tokens\n",
      "2021-12-13 07:03:09 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:03:10 - Truncated by 1662 tokens\n",
      "2021-12-13 07:03:11 - Truncated by 3357 tokens\n",
      "2021-12-13 07:03:16 - Truncated by 4651 tokens\n",
      "2021-12-13 07:03:16 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:03:17 - Truncated by 4652 tokens\n",
      "2021-12-13 07:03:21 - Truncated by 83 tokens\n",
      "2021-12-13 07:03:21 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:03:22 - Truncated by 84 tokens\n",
      "2021-12-13 07:03:45 - Truncated by 924 tokens\n",
      "2021-12-13 07:03:52 - Truncated by 245 tokens\n",
      "2021-12-13 07:03:54 - Truncated by 894 tokens\n",
      "2021-12-13 07:03:57 - Truncated by 308 tokens\n",
      "2021-12-13 07:03:59 - Truncated by 54 tokens\n",
      "2021-12-13 07:04:04 - Truncated by 2006 tokens\n",
      "2021-12-13 07:04:10 - Truncated by 264 tokens\n",
      "2021-12-13 07:04:19 - Truncated by 89 tokens\n",
      "2021-12-13 07:04:23 - Truncated by 1082 tokens\n",
      "2021-12-13 07:04:24 - Truncated by 1955 tokens\n",
      "2021-12-13 07:04:25 - Truncated by 466 tokens\n",
      "2021-12-13 07:04:33 - Truncated by 26 tokens\n",
      "2021-12-13 07:04:38 - Truncated by 3566 tokens\n",
      "2021-12-13 07:04:43 - Truncated by 1663 tokens\n",
      "2021-12-13 07:04:44 - Truncated by 142 tokens\n",
      "2021-12-13 07:04:45 - Truncated by 873 tokens\n",
      "2021-12-13 07:04:46 - Truncated by 540 tokens\n",
      "2021-12-13 07:04:46 - Truncated by 5107 tokens\n",
      "2021-12-13 07:04:47 - Truncated by 46135 tokens\n",
      "2021-12-13 07:04:48 - Truncated by 24083 tokens\n",
      "2021-12-13 07:04:49 - Truncated by 30387 tokens\n",
      "2021-12-13 07:04:50 - Truncated by 3659 tokens\n",
      "2021-12-13 07:04:50 - Truncated by 334 tokens\n",
      "2021-12-13 07:05:02 - Truncated by 257 tokens\n",
      "2021-12-13 07:05:06 - Truncated by 7129 tokens\n",
      "2021-12-13 07:05:08 - Truncated by 552 tokens\n",
      "2021-12-13 07:05:10 - Truncated by 203 tokens\n",
      "2021-12-13 07:05:14 - Truncated by 2314 tokens\n",
      "2021-12-13 07:05:16 - Truncated by 1762 tokens\n",
      "2021-12-13 07:05:17 - Truncated by 315 tokens\n",
      "2021-12-13 07:05:19 - Truncated by 50 tokens\n",
      "2021-12-13 07:05:20 - Truncated by 1017 tokens\n",
      "2021-12-13 07:05:22 - Truncated by 291 tokens\n",
      "2021-12-13 07:05:31 - Truncated by 1043 tokens\n",
      "2021-12-13 07:05:37 - Truncated by 862 tokens\n",
      "2021-12-13 07:05:45 - Truncated by 405 tokens\n",
      "2021-12-13 07:05:46 - Truncated by 306 tokens\n",
      "2021-12-13 07:05:47 - Truncated by 660 tokens\n",
      "2021-12-13 07:05:49 - Truncated by 510 tokens\n",
      "2021-12-13 07:05:49 - Truncated by 1523 tokens\n",
      "2021-12-13 07:05:54 - Truncated by 168 tokens\n",
      "2021-12-13 07:05:57 - Truncated by 485 tokens\n",
      "2021-12-13 07:05:59 - Truncated by 192 tokens\n",
      "2021-12-13 07:06:06 - Truncated by 1471 tokens\n",
      "2021-12-13 07:06:07 - Truncated by 106 tokens\n",
      "2021-12-13 07:06:19 - Truncated by 24 tokens\n",
      "2021-12-13 07:06:20 - Truncated by 1552 tokens\n",
      "2021-12-13 07:06:24 - Truncated by 1109 tokens\n",
      "2021-12-13 07:06:26 - Truncated by 1997 tokens\n",
      "2021-12-13 07:06:27 - Truncated by 541 tokens\n",
      "2021-12-13 07:06:42 - Truncated by 372 tokens\n",
      "2021-12-13 07:06:47 - Truncated by 104 tokens\n",
      "2021-12-13 07:06:52 - Truncated by 816 tokens\n",
      "2021-12-13 07:06:56 - Truncated by 1567 tokens\n",
      "2021-12-13 07:06:58 - Truncated by 743 tokens\n",
      "2021-12-13 07:06:59 - Truncated by 3652 tokens\n",
      "2021-12-13 07:07:00 - Truncated by 2322 tokens\n",
      "2021-12-13 07:07:05 - Truncated by 91 tokens\n",
      "2021-12-13 07:07:05 - Truncated by 205 tokens\n",
      "2021-12-13 07:07:06 - Truncated by 1151 tokens\n",
      "2021-12-13 07:07:07 - Truncated by 841 tokens\n",
      "2021-12-13 07:07:10 - Truncated by 1883 tokens\n",
      "2021-12-13 07:07:12 - Truncated by 411 tokens\n",
      "2021-12-13 07:07:13 - Truncated by 1289 tokens\n",
      "2021-12-13 07:07:17 - Truncated by 6583 tokens\n",
      "2021-12-13 07:07:18 - Truncated by 682 tokens\n",
      "2021-12-13 07:07:19 - Truncated by 825 tokens\n",
      "2021-12-13 07:07:25 - Truncated by 678 tokens\n",
      "2021-12-13 07:07:26 - Truncated by 2304 tokens\n",
      "2021-12-13 07:07:28 - Truncated by 499 tokens\n",
      "2021-12-13 07:07:32 - Truncated by 427 tokens\n",
      "2021-12-13 07:07:41 - Truncated by 481 tokens\n",
      "2021-12-13 07:07:45 - Truncated by 360 tokens\n",
      "2021-12-13 07:07:46 - Truncated by 40 tokens\n",
      "2021-12-13 07:07:48 - Truncated by 829 tokens\n",
      "2021-12-13 07:07:49 - Truncated by 829 tokens\n",
      "2021-12-13 07:07:57 - Truncated by 263 tokens\n",
      "2021-12-13 07:08:04 - Truncated by 642 tokens\n",
      "2021-12-13 07:08:05 - Truncated by 776 tokens\n",
      "2021-12-13 07:08:08 - Truncated by 3033 tokens\n",
      "2021-12-13 07:08:12 - Truncated by 2267 tokens\n",
      "2021-12-13 07:08:13 - Truncated by 602 tokens\n",
      "2021-12-13 07:08:21 - Truncated by 21 tokens\n",
      "2021-12-13 07:08:25 - Truncated by 1662 tokens\n",
      "2021-12-13 07:08:27 - Truncated by 1545 tokens\n",
      "2021-12-13 07:08:34 - Truncated by 1400 tokens\n",
      "2021-12-13 07:08:37 - Truncated by 196 tokens\n",
      "2021-12-13 07:08:40 - Truncated by 1051 tokens\n",
      "2021-12-13 07:08:46 - Truncated by 1627 tokens\n",
      "2021-12-13 07:08:46 - Truncated by 1366 tokens\n",
      "2021-12-13 07:08:48 - Truncated by 1180 tokens\n",
      "2021-12-13 07:08:48 - Truncated by 430 tokens\n",
      "2021-12-13 07:08:50 - Truncated by 1610 tokens\n",
      "2021-12-13 07:08:53 - Truncated by 10 tokens\n",
      "2021-12-13 07:09:13 - Truncated by 356 tokens\n",
      "2021-12-13 07:09:24 - Truncated by 1080 tokens\n",
      "2021-12-13 07:09:27 - Truncated by 802 tokens\n",
      "2021-12-13 07:09:38 - Truncated by 104 tokens\n",
      "2021-12-13 07:09:48 - Truncated by 102 tokens\n",
      "2021-12-13 07:09:51 - Truncated by 4201 tokens\n",
      "2021-12-13 07:09:59 - Truncated by 66 tokens\n",
      "2021-12-13 07:10:11 - Truncated by 365 tokens\n",
      "2021-12-13 07:10:15 - Truncated by 170 tokens\n",
      "2021-12-13 07:10:15 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:10:16 - Truncated by 171 tokens\n",
      "2021-12-13 07:10:19 - Truncated by 67 tokens\n",
      "2021-12-13 07:10:19 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:10:20 - Truncated by 68 tokens\n",
      "2021-12-13 07:10:35 - Truncated by 774 tokens\n",
      "2021-12-13 07:10:39 - Truncated by 581 tokens\n",
      "2021-12-13 07:10:39 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:10:40 - Truncated by 582 tokens\n",
      "2021-12-13 07:10:45 - Truncated by 194 tokens\n",
      "2021-12-13 07:10:45 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:10:46 - Truncated by 195 tokens\n",
      "2021-12-13 07:10:55 - Truncated by 368 tokens\n",
      "2021-12-13 07:10:56 - Truncated by 987 tokens\n",
      "2021-12-13 07:10:57 - Truncated by 987 tokens\n",
      "2021-12-13 07:11:23 - Truncated by 748 tokens\n",
      "2021-12-13 07:12:01 - Truncated by 2036 tokens\n",
      "2021-12-13 07:12:04 - Truncated by 1682 tokens\n",
      "2021-12-13 07:12:04 - Truncated by 111 tokens\n",
      "2021-12-13 07:12:12 - Truncated by 729 tokens\n",
      "2021-12-13 07:12:12 - Truncated by 944 tokens\n",
      "2021-12-13 07:12:16 - Truncated by 3334 tokens\n",
      "2021-12-13 07:12:17 - Truncated by 4837 tokens\n",
      "2021-12-13 07:12:19 - Truncated by 727 tokens\n",
      "2021-12-13 07:12:21 - Truncated by 20520 tokens\n",
      "2021-12-13 07:12:22 - Truncated by 1007 tokens\n",
      "2021-12-13 07:12:27 - Truncated by 3764 tokens\n",
      "2021-12-13 07:12:27 - Truncated by 1341 tokens\n",
      "2021-12-13 07:12:28 - Truncated by 2878 tokens\n",
      "2021-12-13 07:12:32 - Truncated by 4469 tokens\n",
      "2021-12-13 07:12:35 - Truncated by 72738 tokens\n",
      "2021-12-13 07:12:38 - Truncated by 3365 tokens\n",
      "2021-12-13 07:12:40 - Truncated by 181 tokens\n",
      "2021-12-13 07:12:41 - Truncated by 806 tokens\n",
      "2021-12-13 07:12:44 - Truncated by 4690 tokens\n",
      "2021-12-13 07:12:45 - Truncated by 18420 tokens\n",
      "2021-12-13 07:12:46 - Truncated by 60 tokens\n",
      "2021-12-13 07:12:50 - Truncated by 2489 tokens\n",
      "2021-12-13 07:12:56 - Truncated by 446 tokens\n",
      "2021-12-13 07:12:57 - Truncated by 566 tokens\n",
      "2021-12-13 07:12:59 - Truncated by 5614 tokens\n",
      "2021-12-13 07:12:59 - Truncated by 2587 tokens\n",
      "2021-12-13 07:13:08 - Truncated by 2354 tokens\n",
      "2021-12-13 07:13:09 - Truncated by 801 tokens\n",
      "2021-12-13 07:13:14 - Truncated by 73 tokens\n",
      "2021-12-13 07:13:16 - Truncated by 2664 tokens\n",
      "2021-12-13 07:13:20 - Truncated by 774 tokens\n",
      "2021-12-13 07:13:21 - Truncated by 664 tokens\n",
      "2021-12-13 07:13:24 - Truncated by 1120 tokens\n",
      "2021-12-13 07:13:27 - Truncated by 3702 tokens\n",
      "2021-12-13 07:13:30 - Truncated by 3271 tokens\n",
      "2021-12-13 07:13:37 - Truncated by 55 tokens\n",
      "2021-12-13 07:13:45 - Truncated by 390 tokens\n",
      "2021-12-13 07:13:48 - Truncated by 284 tokens\n",
      "2021-12-13 07:13:50 - Truncated by 359 tokens\n",
      "2021-12-13 07:13:51 - Truncated by 115 tokens\n",
      "2021-12-13 07:13:54 - Truncated by 978 tokens\n",
      "2021-12-13 07:13:55 - Truncated by 978 tokens\n",
      "2021-12-13 07:13:56 - Truncated by 917 tokens\n",
      "2021-12-13 07:13:57 - Truncated by 97 tokens\n",
      "2021-12-13 07:13:59 - Truncated by 239 tokens\n",
      "2021-12-13 07:14:00 - Truncated by 8461 tokens\n",
      "2021-12-13 07:14:07 - Truncated by 331 tokens\n",
      "2021-12-13 07:14:10 - Truncated by 834 tokens\n",
      "2021-12-13 07:14:10 - Truncated by 1039 tokens\n",
      "2021-12-13 07:14:11 - Truncated by 275 tokens\n",
      "2021-12-13 07:14:11 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:12 - Truncated by 276 tokens\n",
      "2021-12-13 07:14:15 - Truncated by 2 tokens\n",
      "2021-12-13 07:14:20 - Truncated by 681 tokens\n",
      "2021-12-13 07:14:20 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:21 - Truncated by 682 tokens\n",
      "2021-12-13 07:14:26 - Truncated by 987 tokens\n",
      "2021-12-13 07:14:26 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:27 - Truncated by 988 tokens\n",
      "2021-12-13 07:14:28 - Truncated by 1920 tokens\n",
      "2021-12-13 07:14:28 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:29 - Truncated by 1921 tokens\n",
      "2021-12-13 07:14:29 - Truncated by 393 tokens\n",
      "2021-12-13 07:14:29 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:30 - Truncated by 394 tokens\n",
      "2021-12-13 07:14:32 - Truncated by 1337 tokens\n",
      "2021-12-13 07:14:32 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:33 - Truncated by 1338 tokens\n",
      "2021-12-13 07:14:37 - Truncated by 433 tokens\n",
      "2021-12-13 07:14:37 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:38 - Truncated by 434 tokens\n",
      "2021-12-13 07:14:40 - Truncated by 1224 tokens\n",
      "2021-12-13 07:14:40 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:41 - Truncated by 1225 tokens\n",
      "2021-12-13 07:14:41 - Truncated by 514 tokens\n",
      "2021-12-13 07:14:41 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:42 - Truncated by 515 tokens\n",
      "2021-12-13 07:14:47 - Truncated by 247 tokens\n",
      "2021-12-13 07:14:48 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:49 - Truncated by 248 tokens\n",
      "2021-12-13 07:14:51 - Truncated by 337 tokens\n",
      "2021-12-13 07:14:51 - Retrying after 1 seconds due to (400, {'error': \"Task contains 2049 tokens. This is longer than the models maximum context length of 2048. Consider using a smaller prompt or smaller 'completion expected'.\", 'code': 'TASK_ERROR'})\n",
      "2021-12-13 07:14:52 - Truncated by 338 tokens\n",
      "2021-12-13 07:14:54 - Truncated by 429 tokens\n",
      "2021-12-13 07:14:55 - Truncated by 1575 tokens\n",
      "2021-12-13 07:14:58 - Truncated by 286 tokens\n",
      "2021-12-13 07:15:00 - Truncated by 1770 tokens\n",
      "2021-12-13 07:15:01 - Truncated by 1437 tokens\n",
      "2021-12-13 07:15:06 - Truncated by 2602 tokens\n",
      "2021-12-13 07:15:08 - Truncated by 576 tokens\n",
      "2021-12-13 07:15:08 - Truncated by 7870 tokens\n",
      "2021-12-13 07:15:16 - Truncated by 152 tokens\n",
      "2021-12-13 07:15:17 - Truncated by 845 tokens\n",
      "2021-12-13 07:15:20 - Truncated by 772 tokens\n",
      "2021-12-13 07:15:21 - Truncated by 2223 tokens\n",
      "2021-12-13 07:15:27 - Truncated by 41 tokens\n",
      "2021-12-13 07:15:47 - Truncated by 217 tokens\n",
      "2021-12-13 07:15:53 - Truncated by 2799 tokens\n",
      "2021-12-13 07:15:54 - Truncated by 3789 tokens\n",
      "2021-12-13 07:15:58 - Truncated by 21 tokens\n",
      "2021-12-13 07:16:09 - Truncated by 47 tokens\n",
      "2021-12-13 07:16:17 - Truncated by 150 tokens\n",
      "2021-12-13 07:16:22 - Truncated by 778 tokens\n",
      "2021-12-13 07:16:28 - Truncated by 567 tokens\n",
      "2021-12-13 07:16:29 - Truncated by 239 tokens\n",
      "2021-12-13 07:16:41 - Truncated by 230 tokens\n",
      "2021-12-13 07:16:46 - Truncated by 363 tokens\n",
      "2021-12-13 07:16:47 - Truncated by 103 tokens\n",
      "2021-12-13 07:16:48 - Truncated by 454 tokens\n",
      "2021-12-13 07:16:52 - Truncated by 155 tokens\n",
      "2021-12-13 07:16:54 - Truncated by 103 tokens\n",
      "2021-12-13 07:16:55 - Truncated by 342 tokens\n",
      "2021-12-13 07:16:56 - Truncated by 359 tokens\n",
      "2021-12-13 07:16:58 - Truncated by 645 tokens\n",
      "2021-12-13 07:17:02 - Truncated by 70 tokens\n",
      "2021-12-13 07:17:03 - Truncated by 161 tokens\n",
      "2021-12-13 07:17:04 - Truncated by 530 tokens\n",
      "2021-12-13 07:17:07 - Truncated by 4531 tokens\n",
      "2021-12-13 07:17:09 - Truncated by 926 tokens\n",
      "2021-12-13 07:17:10 - Truncated by 2344 tokens\n",
      "2021-12-13 07:17:11 - Truncated by 473 tokens\n",
      "2021-12-13 07:17:12 - Truncated by 259 tokens\n",
      "2021-12-13 07:17:16 - Truncated by 3399 tokens\n",
      "2021-12-13 07:17:17 - Truncated by 3399 tokens\n",
      "2021-12-13 07:17:19 - Truncated by 341 tokens\n",
      "2021-12-13 07:17:20 - Truncated by 1561 tokens\n",
      "2021-12-13 07:17:22 - Truncated by 4658 tokens\n",
      "2021-12-13 07:17:23 - Truncated by 522 tokens\n",
      "2021-12-13 07:17:24 - Truncated by 20141 tokens\n",
      "2021-12-13 07:17:24 - Truncated by 339 tokens\n",
      "2021-12-13 07:17:27 - Truncated by 1399 tokens\n",
      "2021-12-13 07:17:29 - Truncated by 599 tokens\n",
      "2021-12-13 07:17:34 - Truncated by 2343 tokens\n",
      "2021-12-13 07:17:36 - Truncated by 2830 tokens\n",
      "2021-12-13 07:17:37 - Truncated by 23169 tokens\n",
      "2021-12-13 07:17:39 - Truncated by 8810 tokens\n",
      "2021-12-13 07:17:42 - Truncated by 44 tokens\n",
      "2021-12-13 07:17:45 - Truncated by 9585 tokens\n",
      "2021-12-13 07:17:52 - Truncated by 1601 tokens\n",
      "2021-12-13 07:17:53 - Truncated by 1911 tokens\n",
      "2021-12-13 07:18:01 - Truncated by 466 tokens\n",
      "2021-12-13 07:18:01 - Truncated by 1400 tokens\n",
      "2021-12-13 07:18:03 - Truncated by 594 tokens\n",
      "2021-12-13 07:18:04 - Truncated by 255 tokens\n",
      "2021-12-13 07:18:06 - Truncated by 2105 tokens\n",
      "2021-12-13 07:18:07 - Truncated by 1345 tokens\n",
      "2021-12-13 07:18:08 - Truncated by 674 tokens\n",
      "2021-12-13 07:18:14 - Truncated by 88 tokens\n",
      "2021-12-13 07:18:18 - Truncated by 93 tokens\n",
      "2021-12-13 07:18:41 - Truncated by 978 tokens\n",
      "2021-12-13 07:18:45 - Truncated by 1172 tokens\n",
      "2021-12-13 07:18:48 - Truncated by 179 tokens\n",
      "2021-12-13 07:18:55 - Truncated by 344 tokens\n",
      "2021-12-13 07:18:57 - Truncated by 345 tokens\n",
      "2021-12-13 07:18:58 - Truncated by 605 tokens\n",
      "2021-12-13 07:18:59 - Truncated by 448 tokens\n",
      "2021-12-13 07:19:03 - Truncated by 3678 tokens\n",
      "2021-12-13 07:19:03 - Truncated by 126 tokens\n",
      "2021-12-13 07:19:12 - Truncated by 6418 tokens\n",
      "2021-12-13 07:19:13 - Truncated by 1151 tokens\n",
      "2021-12-13 07:19:14 - Truncated by 849 tokens\n",
      "2021-12-13 07:19:18 - Truncated by 547 tokens\n",
      "2021-12-13 07:19:18 - Truncated by 547 tokens\n",
      "2021-12-13 07:19:21 - Truncated by 532 tokens\n",
      "2021-12-13 07:19:21 - Truncated by 122 tokens\n",
      "2021-12-13 07:19:22 - Truncated by 1412 tokens\n",
      "2021-12-13 07:19:24 - Truncated by 3085 tokens\n",
      "2021-12-13 07:19:26 - Truncated by 77131 tokens\n",
      "2021-12-13 07:19:28 - Truncated by 271 tokens\n",
      "2021-12-13 07:19:29 - Truncated by 685 tokens\n",
      "2021-12-13 07:19:42 - Truncated by 661 tokens\n",
      "2021-12-13 07:19:48 - Truncated by 471 tokens\n",
      "2021-12-13 07:19:54 - Truncated by 278 tokens\n",
      "2021-12-13 07:19:55 - Truncated by 1062 tokens\n",
      "2021-12-13 07:19:56 - Truncated by 781 tokens\n",
      "2021-12-13 07:19:58 - Truncated by 11 tokens\n",
      "2021-12-13 07:20:02 - Truncated by 950 tokens\n",
      "2021-12-13 07:20:02 - Truncated by 330 tokens\n",
      "2021-12-13 07:20:03 - Truncated by 779 tokens\n",
      "2021-12-13 07:20:05 - Truncated by 197 tokens\n",
      "2021-12-13 07:20:13 - Truncated by 1191 tokens\n",
      "2021-12-13 07:20:14 - Truncated by 619 tokens\n",
      "2021-12-13 07:20:18 - Truncated by 501 tokens\n",
      "2021-12-13 07:20:27 - Truncated by 193 tokens\n",
      "2021-12-13 07:20:32 - Truncated by 1055 tokens\n",
      "2021-12-13 07:20:32 - Truncated by 419 tokens\n",
      "2021-12-13 07:20:35 - Truncated by 3446 tokens\n",
      "2021-12-13 07:20:37 - Truncated by 918 tokens\n",
      "2021-12-13 07:20:56 - Truncated by 1540 tokens\n",
      "2021-12-13 07:21:00 - Truncated by 304 tokens\n",
      "2021-12-13 07:21:03 - Truncated by 597 tokens\n",
      "2021-12-13 07:21:05 - Truncated by 170 tokens\n",
      "2021-12-13 07:21:13 - Truncated by 610 tokens\n",
      "2021-12-13 07:21:14 - Truncated by 2047 tokens\n",
      "2021-12-13 07:21:18 - Truncated by 3381 tokens\n",
      "2021-12-13 07:21:19 - Truncated by 108 tokens\n",
      "2021-12-13 07:21:23 - Truncated by 359 tokens\n",
      "2021-12-13 07:21:28 - Truncated by 790 tokens\n",
      "2021-12-13 07:21:46 - Truncated by 626 tokens\n",
      "2021-12-13 07:21:52 - Truncated by 743 tokens\n",
      "2021-12-13 07:21:54 - Truncated by 4273 tokens\n",
      "2021-12-13 07:21:55 - Truncated by 1561 tokens\n",
      "2021-12-13 07:21:57 - Truncated by 610 tokens\n",
      "2021-12-13 07:22:04 - Truncated by 874 tokens\n",
      "2021-12-13 07:22:05 - Truncated by 812 tokens\n",
      "2021-12-13 07:22:06 - Truncated by 638 tokens\n",
      "2021-12-13 07:22:07 - Truncated by 651 tokens\n",
      "2021-12-13 07:22:08 - Truncated by 1021 tokens\n",
      "2021-12-13 07:22:14 - Truncated by 29 tokens\n",
      "2021-12-13 07:22:15 - Truncated by 91 tokens\n",
      "2021-12-13 07:22:20 - Truncated by 334 tokens\n",
      "2021-12-13 07:22:22 - Truncated by 424 tokens\n",
      "2021-12-13 07:22:23 - Truncated by 543 tokens\n",
      "2021-12-13 07:22:24 - Truncated by 1146 tokens\n",
      "2021-12-13 07:22:25 - Truncated by 42 tokens\n",
      "2021-12-13 07:22:28 - Truncated by 90 tokens\n",
      "2021-12-13 07:22:32 - Truncated by 46 tokens\n",
      "2021-12-13 07:22:32 - Truncated by 96 tokens\n",
      "2021-12-13 07:22:34 - Truncated by 2599 tokens\n",
      "2021-12-13 07:22:36 - Truncated by 714 tokens\n",
      "2021-12-13 07:22:38 - Truncated by 465 tokens\n",
      "2021-12-13 07:22:44 - Truncated by 2005 tokens\n",
      "2021-12-13 07:22:50 - Truncated by 102 tokens\n",
      "2021-12-13 07:23:00 - Truncated by 2151 tokens\n",
      "2021-12-13 07:23:05 - Truncated by 212 tokens\n",
      "2021-12-13 07:23:08 - Truncated by 775 tokens\n",
      "2021-12-13 07:23:09 - Truncated by 346 tokens\n",
      "2021-12-13 07:23:10 - Truncated by 2054 tokens\n",
      "2021-12-13 07:23:15 - Truncated by 2212 tokens\n",
      "2021-12-13 07:23:17 - Truncated by 144 tokens\n",
      "2021-12-13 07:23:19 - Truncated by 1101 tokens\n",
      "2021-12-13 07:23:43 - Truncated by 522 tokens\n",
      "2021-12-13 07:23:44 - Truncated by 380 tokens\n",
      "2021-12-13 07:23:47 - Truncated by 810 tokens\n",
      "2021-12-13 07:23:50 - Truncated by 1776 tokens\n",
      "2021-12-13 07:23:53 - Truncated by 713 tokens\n",
      "2021-12-13 07:23:54 - Truncated by 1862 tokens\n",
      "2021-12-13 07:23:55 - Truncated by 1424 tokens\n",
      "2021-12-13 07:23:56 - Truncated by 50 tokens\n",
      "2021-12-13 07:23:58 - Truncated by 657 tokens\n",
      "2021-12-13 07:24:01 - Truncated by 1462 tokens\n",
      "2021-12-13 07:24:03 - Truncated by 7036 tokens\n",
      "2021-12-13 07:24:08 - Truncated by 2437 tokens\n",
      "2021-12-13 07:24:14 - Truncated by 674 tokens\n",
      "2021-12-13 07:24:25 - Truncated by 197 tokens\n",
      "2021-12-13 07:24:27 - Truncated by 2749 tokens\n",
      "2021-12-13 07:24:30 - Truncated by 1427 tokens\n",
      "2021-12-13 07:24:32 - Truncated by 3506 tokens\n",
      "2021-12-13 07:24:34 - Truncated by 1653 tokens\n",
      "2021-12-13 07:24:34 - Truncated by 419 tokens\n",
      "2021-12-13 07:24:40 - Truncated by 3967 tokens\n",
      "2021-12-13 07:24:45 - Truncated by 108 tokens\n",
      "2021-12-13 07:24:46 - Truncated by 321 tokens\n",
      "2021-12-13 07:24:50 - Truncated by 29 tokens\n",
      "2021-12-13 07:24:52 - Truncated by 3341 tokens\n",
      "2021-12-13 07:24:53 - Truncated by 240 tokens\n",
      "2021-12-13 07:24:54 - Truncated by 240 tokens\n",
      "2021-12-13 07:24:54 - Truncated by 240 tokens\n",
      "2021-12-13 07:24:56 - Truncated by 1723 tokens\n",
      "2021-12-13 07:24:56 - Truncated by 3213 tokens\n",
      "2021-12-13 07:24:58 - Truncated by 1809 tokens\n",
      "2021-12-13 07:25:00 - Truncated by 5606 tokens\n",
      "2021-12-13 07:25:03 - Truncated by 5196 tokens\n",
      "2021-12-13 07:25:04 - Truncated by 674 tokens\n",
      "2021-12-13 07:25:06 - Truncated by 718 tokens\n",
      "2021-12-13 07:25:07 - Truncated by 27980 tokens\n",
      "2021-12-13 07:25:09 - Truncated by 18128 tokens\n",
      "2021-12-13 07:25:11 - Truncated by 290 tokens\n",
      "2021-12-13 07:25:12 - Truncated by 581 tokens\n",
      "2021-12-13 07:25:13 - Truncated by 1042 tokens\n",
      "2021-12-13 07:25:14 - Truncated by 208 tokens\n",
      "2021-12-13 07:25:17 - Truncated by 38107 tokens\n",
      "2021-12-13 07:25:23 - Truncated by 241 tokens\n",
      "2021-12-13 07:25:24 - Truncated by 217 tokens\n",
      "2021-12-13 07:25:26 - Truncated by 1721 tokens\n",
      "2021-12-13 07:25:34 - Truncated by 18 tokens\n",
      "2021-12-13 07:25:38 - Truncated by 38 tokens\n",
      "2021-12-13 07:25:40 - Truncated by 454 tokens\n",
      "2021-12-13 07:25:42 - Truncated by 423 tokens\n",
      "2021-12-13 07:25:51 - Truncated by 70 tokens\n",
      "2021-12-13 07:25:55 - Truncated by 301 tokens\n",
      "2021-12-13 07:25:56 - Truncated by 789 tokens\n",
      "2021-12-13 07:26:07 - Truncated by 449 tokens\n",
      "2021-12-13 07:26:10 - Truncated by 15 tokens\n",
      "2021-12-13 07:26:16 - Truncated by 231 tokens\n",
      "2021-12-13 07:26:27 - Truncated by 301 tokens\n",
      "2021-12-13 07:26:29 - Truncated by 941 tokens\n",
      "2021-12-13 07:26:34 - Truncated by 90 tokens\n",
      "2021-12-13 07:26:39 - Truncated by 3153 tokens\n",
      "2021-12-13 07:26:41 - Truncated by 7 tokens\n",
      "2021-12-13 07:26:51 - Truncated by 616 tokens\n",
      "2021-12-13 07:26:56 - Truncated by 334 tokens\n",
      "2021-12-13 07:26:58 - Truncated by 517 tokens\n",
      "2021-12-13 07:27:11 - Truncated by 187 tokens\n",
      "2021-12-13 07:27:11 - Truncated by 537 tokens\n",
      "2021-12-13 07:27:13 - Truncated by 367 tokens\n",
      "2021-12-13 07:27:15 - Truncated by 301 tokens\n",
      "2021-12-13 07:27:16 - Truncated by 301 tokens\n",
      "2021-12-13 07:27:19 - Truncated by 297 tokens\n",
      "2021-12-13 07:27:25 - Truncated by 486 tokens\n",
      "2021-12-13 07:27:28 - Truncated by 594 tokens\n",
      "2021-12-13 07:27:30 - Truncated by 37 tokens\n",
      "2021-12-13 07:27:31 - Truncated by 146 tokens\n",
      "2021-12-13 07:27:37 - Truncated by 1685 tokens\n",
      "2021-12-13 07:27:41 - Truncated by 152 tokens\n",
      "2021-12-13 07:27:47 - Truncated by 709 tokens\n",
      "2021-12-13 07:27:53 - Truncated by 91 tokens\n",
      "2021-12-13 07:27:57 - Truncated by 2328 tokens\n",
      "2021-12-13 07:27:59 - Truncated by 767 tokens\n",
      "2021-12-13 07:28:00 - Truncated by 75 tokens\n",
      "2021-12-13 07:28:02 - Truncated by 76 tokens\n",
      "2021-12-13 07:28:12 - Truncated by 39 tokens\n",
      "2021-12-13 07:28:12 - Truncated by 561 tokens\n",
      "2021-12-13 07:28:19 - Truncated by 97 tokens\n",
      "2021-12-13 07:28:27 - Truncated by 928 tokens\n",
      "2021-12-13 07:28:28 - Truncated by 929 tokens\n",
      "2021-12-13 07:28:30 - Truncated by 3022 tokens\n",
      "2021-12-13 07:28:32 - Truncated by 1489 tokens\n",
      "2021-12-13 07:28:34 - Truncated by 1255 tokens\n",
      "2021-12-13 07:28:38 - Truncated by 98 tokens\n",
      "2021-12-13 07:28:41 - Truncated by 228 tokens\n",
      "2021-12-13 07:28:43 - Truncated by 881 tokens\n",
      "2021-12-13 07:28:43 - Truncated by 1357 tokens\n",
      "2021-12-13 07:28:44 - Truncated by 2364 tokens\n",
      "2021-12-13 07:28:44 - Truncated by 1206 tokens\n",
      "2021-12-13 07:28:45 - Truncated by 1175 tokens\n",
      "2021-12-13 07:28:51 - Truncated by 684 tokens\n",
      "2021-12-13 07:28:56 - Truncated by 2295 tokens\n",
      "2021-12-13 07:28:58 - Truncated by 297 tokens\n",
      "2021-12-13 07:28:58 - Truncated by 166 tokens\n",
      "2021-12-13 07:29:00 - Truncated by 841 tokens\n",
      "2021-12-13 07:29:01 - Truncated by 1056 tokens\n",
      "2021-12-13 07:29:01 - Truncated by 419 tokens\n",
      "2021-12-13 07:29:04 - Truncated by 452 tokens\n",
      "2021-12-13 07:29:08 - Truncated by 920 tokens\n",
      "2021-12-13 07:29:11 - Truncated by 2821 tokens\n",
      "2021-12-13 07:29:12 - Truncated by 1344 tokens\n",
      "2021-12-13 07:29:21 - Truncated by 80 tokens\n",
      "2021-12-13 07:29:22 - Truncated by 1773 tokens\n",
      "2021-12-13 07:29:29 - Truncated by 9 tokens\n",
      "2021-12-13 07:29:32 - Truncated by 63 tokens\n",
      "2021-12-13 07:29:43 - Truncated by 1560 tokens\n",
      "2021-12-13 07:29:48 - Truncated by 1544 tokens\n",
      "2021-12-13 07:29:49 - Truncated by 197 tokens\n",
      "2021-12-13 07:29:56 - Truncated by 1223 tokens\n",
      "2021-12-13 07:30:05 - Truncated by 124 tokens\n",
      "2021-12-13 07:30:06 - Truncated by 165 tokens\n",
      "2021-12-13 07:30:15 - Truncated by 71 tokens\n",
      "2021-12-13 07:30:16 - Truncated by 1064 tokens\n",
      "2021-12-13 07:30:20 - Truncated by 1362 tokens\n",
      "2021-12-13 07:30:21 - Truncated by 3 tokens\n",
      "2021-12-13 07:30:22 - Truncated by 536 tokens\n",
      "2021-12-13 07:30:25 - Truncated by 2575 tokens\n",
      "2021-12-13 07:30:27 - Truncated by 132 tokens\n",
      "2021-12-13 07:30:30 - Truncated by 400 tokens\n",
      "2021-12-13 07:30:33 - Truncated by 683 tokens\n",
      "2021-12-13 07:30:34 - Truncated by 215 tokens\n",
      "2021-12-13 07:30:36 - Truncated by 481 tokens\n",
      "2021-12-13 07:30:39 - Truncated by 413 tokens\n",
      "2021-12-13 07:30:40 - Truncated by 5923 tokens\n",
      "2021-12-13 07:30:44 - Truncated by 1075 tokens\n",
      "2021-12-13 07:30:45 - Truncated by 404 tokens\n",
      "2021-12-13 07:30:45 - Truncated by 713 tokens\n",
      "2021-12-13 07:30:47 - Truncated by 1315 tokens\n",
      "2021-12-13 07:30:50 - Truncated by 1040 tokens\n",
      "2021-12-13 07:30:51 - Truncated by 1 tokens\n",
      "2021-12-13 07:30:51 - Truncated by 219 tokens\n",
      "2021-12-13 07:30:52 - Truncated by 31 tokens\n",
      "2021-12-13 07:30:53 - Truncated by 194 tokens\n",
      "2021-12-13 07:30:54 - Truncated by 454 tokens\n",
      "2021-12-13 07:30:55 - Truncated by 1507 tokens\n",
      "2021-12-13 07:30:59 - Truncated by 102 tokens\n",
      "2021-12-13 07:31:03 - Truncated by 4828 tokens\n",
      "2021-12-13 07:31:11 - Truncated by 148 tokens\n",
      "2021-12-13 07:31:14 - Truncated by 565 tokens\n",
      "2021-12-13 07:31:14 - Truncated by 2015 tokens\n",
      "2021-12-13 07:31:19 - Truncated by 2086 tokens\n",
      "2021-12-13 07:31:38 - Truncated by 472 tokens\n",
      "2021-12-13 07:31:44 - Truncated by 533 tokens\n",
      "2021-12-13 07:31:47 - Truncated by 1478 tokens\n",
      "2021-12-13 07:31:49 - Truncated by 734 tokens\n",
      "2021-12-13 07:31:52 - Truncated by 734 tokens\n",
      "2021-12-13 07:31:57 - Truncated by 2455 tokens\n",
      "2021-12-13 07:32:00 - Truncated by 658 tokens\n",
      "2021-12-13 07:32:01 - Truncated by 217 tokens\n",
      "2021-12-13 07:32:05 - Truncated by 158 tokens\n",
      "2021-12-13 07:32:09 - Truncated by 627 tokens\n",
      "2021-12-13 07:32:10 - Truncated by 339 tokens\n",
      "2021-12-13 07:32:12 - Truncated by 359 tokens\n",
      "2021-12-13 07:32:14 - Truncated by 1093 tokens\n",
      "2021-12-13 07:32:19 - Truncated by 640 tokens\n",
      "2021-12-13 07:32:24 - Truncated by 391 tokens\n",
      "2021-12-13 07:32:36 - Truncated by 1023 tokens\n",
      "2021-12-13 07:32:36 - Truncated by 1023 tokens\n",
      "2021-12-13 07:32:42 - Truncated by 1688 tokens\n",
      "2021-12-13 07:32:50 - Truncated by 395 tokens\n",
      "2021-12-13 07:32:53 - Truncated by 184 tokens\n",
      "2021-12-13 07:32:58 - Truncated by 1509 tokens\n",
      "2021-12-13 07:32:59 - Truncated by 497 tokens\n",
      "2021-12-13 07:33:00 - Truncated by 366 tokens\n",
      "2021-12-13 07:33:02 - Truncated by 1547 tokens\n",
      "2021-12-13 07:33:04 - Truncated by 2648 tokens\n",
      "2021-12-13 07:33:06 - Truncated by 181 tokens\n",
      "2021-12-13 07:33:08 - Truncated by 12074 tokens\n",
      "2021-12-13 07:33:10 - Truncated by 7040 tokens\n",
      "2021-12-13 07:33:17 - Truncated by 443 tokens\n",
      "2021-12-13 07:33:17 - Truncated by 5109 tokens\n",
      "2021-12-13 07:33:20 - Truncated by 1761 tokens\n",
      "2021-12-13 07:33:22 - Truncated by 19516 tokens\n",
      "2021-12-13 07:33:24 - Truncated by 675 tokens\n",
      "2021-12-13 07:33:33 - Truncated by 20 tokens\n",
      "2021-12-13 07:33:38 - Truncated by 758 tokens\n",
      "2021-12-13 07:33:40 - Truncated by 595 tokens\n",
      "2021-12-13 07:33:45 - Truncated by 732 tokens\n",
      "2021-12-13 07:33:46 - Truncated by 1040 tokens\n",
      "2021-12-13 07:33:49 - Truncated by 78 tokens\n",
      "2021-12-13 07:33:50 - Truncated by 180 tokens\n",
      "2021-12-13 07:34:06 - Truncated by 266 tokens\n",
      "2021-12-13 07:34:08 - Truncated by 1960 tokens\n",
      "2021-12-13 07:34:18 - Truncated by 789 tokens\n",
      "2021-12-13 07:34:19 - Truncated by 158 tokens\n",
      "2021-12-13 07:34:27 - Truncated by 325 tokens\n",
      "2021-12-13 07:34:35 - Truncated by 143 tokens\n",
      "2021-12-13 07:34:44 - Truncated by 3660 tokens\n",
      "2021-12-13 07:34:48 - Truncated by 1113 tokens\n",
      "2021-12-13 07:34:59 - Truncated by 146 tokens\n",
      "2021-12-13 07:35:06 - Truncated by 3987 tokens\n",
      "2021-12-13 07:35:06 - Truncated by 3804 tokens\n",
      "2021-12-13 07:35:14 - Truncated by 2071 tokens\n",
      "2021-12-13 07:35:23 - Truncated by 9269 tokens\n",
      "2021-12-13 07:35:24 - Truncated by 1838 tokens\n",
      "2021-12-13 07:35:25 - Truncated by 1290 tokens\n",
      "2021-12-13 07:35:34 - Truncated by 5056 tokens\n",
      "2021-12-13 07:35:34 - Truncated by 11844 tokens\n",
      "2021-12-13 07:35:38 - Truncated by 6 tokens\n",
      "2021-12-13 07:35:40 - Truncated by 520 tokens\n",
      "2021-12-13 07:35:48 - Truncated by 1286 tokens\n",
      "2021-12-13 07:35:51 - Truncated by 78 tokens\n",
      "2021-12-13 07:35:56 - Truncated by 1145 tokens\n",
      "2021-12-13 07:35:57 - Truncated by 898 tokens\n",
      "2021-12-13 07:36:07 - Truncated by 474 tokens\n",
      "2021-12-13 07:36:13 - Truncated by 932 tokens\n",
      "2021-12-13 07:36:19 - Truncated by 163 tokens\n",
      "2021-12-13 07:36:21 - Truncated by 361 tokens\n",
      "2021-12-13 07:36:24 - Truncated by 423 tokens\n",
      "2021-12-13 07:36:28 - Truncated by 1157 tokens\n",
      "2021-12-13 07:36:29 - Truncated by 438 tokens\n",
      "2021-12-13 07:36:30 - Truncated by 1293 tokens\n",
      "2021-12-13 07:36:32 - Truncated by 1079 tokens\n",
      "2021-12-13 07:36:33 - Truncated by 1749 tokens\n",
      "2021-12-13 07:36:37 - Truncated by 528 tokens\n",
      "2021-12-13 07:36:40 - Truncated by 39 tokens\n",
      "2021-12-13 07:36:40 - Truncated by 1589 tokens\n",
      "2021-12-13 07:36:45 - Truncated by 69 tokens\n",
      "2021-12-13 07:36:47 - Truncated by 1331 tokens\n",
      "2021-12-13 07:36:48 - Truncated by 262 tokens\n",
      "2021-12-13 07:36:49 - Truncated by 828 tokens\n",
      "2021-12-13 07:36:50 - Truncated by 660 tokens\n",
      "2021-12-13 07:36:56 - Truncated by 572 tokens\n",
      "2021-12-13 07:37:01 - Truncated by 529 tokens\n",
      "2021-12-13 07:37:09 - Truncated by 1631 tokens\n",
      "2021-12-13 07:37:37 - Truncated by 3446 tokens\n",
      "2021-12-13 07:37:49 - Truncated by 328 tokens\n",
      "2021-12-13 07:37:53 - Truncated by 5325 tokens\n",
      "2021-12-13 07:37:54 - Truncated by 2301 tokens\n",
      "2021-12-13 07:38:03 - Truncated by 125 tokens\n",
      "2021-12-13 07:38:05 - Truncated by 719 tokens\n",
      "2021-12-13 07:38:08 - Truncated by 5416 tokens\n",
      "2021-12-13 07:38:11 - Truncated by 1547 tokens\n",
      "2021-12-13 07:38:24 - Truncated by 2133 tokens\n",
      "2021-12-13 07:38:25 - Truncated by 3068 tokens\n",
      "2021-12-13 07:38:26 - Truncated by 32 tokens\n",
      "2021-12-13 07:38:33 - Truncated by 246 tokens\n",
      "2021-12-13 07:38:39 - Truncated by 28 tokens\n",
      "2021-12-13 07:38:44 - Truncated by 2002 tokens\n",
      "2021-12-13 07:38:49 - Truncated by 2727 tokens\n",
      "2021-12-13 07:38:49 - Truncated by 1008 tokens\n",
      "2021-12-13 07:39:01 - Truncated by 90 tokens\n",
      "2021-12-13 07:39:05 - Truncated by 1392 tokens\n",
      "2021-12-13 07:39:06 - Truncated by 154 tokens\n",
      "2021-12-13 07:39:09 - Truncated by 1187 tokens\n",
      "2021-12-13 07:39:12 - Truncated by 413 tokens\n",
      "2021-12-13 07:39:12 - Truncated by 317 tokens\n",
      "2021-12-13 07:39:15 - Truncated by 1423 tokens\n",
      "2021-12-13 07:39:16 - Truncated by 188 tokens\n",
      "2021-12-13 07:39:16 - Truncated by 440 tokens\n",
      "2021-12-13 07:39:20 - Truncated by 1051 tokens\n",
      "2021-12-13 07:39:28 - Truncated by 138 tokens\n",
      "2021-12-13 07:39:30 - Truncated by 1481 tokens\n",
      "2021-12-13 07:39:32 - Truncated by 1521 tokens\n",
      "2021-12-13 07:39:33 - Truncated by 78 tokens\n",
      "2021-12-13 07:39:33 - Truncated by 755 tokens\n",
      "2021-12-13 07:39:36 - Truncated by 456 tokens\n",
      "2021-12-13 07:39:37 - Truncated by 270 tokens\n",
      "2021-12-13 07:39:39 - Truncated by 394 tokens\n",
      "2021-12-13 07:39:39 - Truncated by 3865 tokens\n",
      "2021-12-13 07:39:42 - Truncated by 1669 tokens\n",
      "2021-12-13 07:39:43 - Truncated by 1851 tokens\n",
      "2021-12-13 07:39:45 - Truncated by 2215 tokens\n",
      "2021-12-13 07:39:46 - Truncated by 1597 tokens\n",
      "2021-12-13 07:39:49 - Truncated by 366 tokens\n",
      "2021-12-13 07:39:49 - Truncated by 2637 tokens\n",
      "2021-12-13 07:39:52 - Truncated by 2525 tokens\n",
      "2021-12-13 07:39:53 - Truncated by 2520 tokens\n",
      "2021-12-13 07:39:56 - Truncated by 557 tokens\n",
      "2021-12-13 07:39:59 - Truncated by 11825 tokens\n",
      "2021-12-13 07:40:00 - Truncated by 711 tokens\n",
      "2021-12-13 07:40:01 - Truncated by 19546 tokens\n",
      "2021-12-13 07:40:08 - Truncated by 1833 tokens\n",
      "2021-12-13 07:40:21 - Truncated by 1557 tokens\n",
      "2021-12-13 07:40:24 - Truncated by 3354 tokens\n",
      "2021-12-13 07:40:32 - Truncated by 283 tokens\n",
      "2021-12-13 07:40:34 - Truncated by 1584 tokens\n",
      "2021-12-13 07:40:43 - Truncated by 192 tokens\n",
      "2021-12-13 07:40:43 - Truncated by 4401 tokens\n",
      "2021-12-13 07:40:46 - Truncated by 283 tokens\n",
      "2021-12-13 07:40:46 - Truncated by 702 tokens\n",
      "2021-12-13 07:40:52 - Truncated by 18 tokens\n",
      "2021-12-13 07:40:52 - Truncated by 2003 tokens\n",
      "2021-12-13 07:40:53 - Truncated by 1823 tokens\n",
      "2021-12-13 07:40:55 - Truncated by 52 tokens\n",
      "2021-12-13 07:41:03 - Truncated by 2149 tokens\n",
      "2021-12-13 07:41:12 - Truncated by 4327 tokens\n",
      "2021-12-13 07:41:22 - Truncated by 1058 tokens\n",
      "2021-12-13 07:41:22 - Truncated by 952 tokens\n",
      "2021-12-13 07:41:29 - Truncated by 737 tokens\n",
      "2021-12-13 07:41:33 - Truncated by 5031 tokens\n",
      "2021-12-13 07:41:36 - Truncated by 187 tokens\n",
      "2021-12-13 07:41:43 - Truncated by 295 tokens\n",
      "2021-12-13 07:41:50 - Truncated by 247 tokens\n",
      "2021-12-13 07:41:53 - Truncated by 796 tokens\n",
      "2021-12-13 07:42:06 - Truncated by 1117 tokens\n",
      "2021-12-13 07:42:10 - Truncated by 1007 tokens\n",
      "2021-12-13 07:42:11 - Truncated by 860 tokens\n",
      "2021-12-13 07:42:14 - Truncated by 329 tokens\n",
      "2021-12-13 07:42:15 - Truncated by 488 tokens\n",
      "2021-12-13 07:42:16 - Truncated by 32 tokens\n",
      "2021-12-13 07:42:22 - Truncated by 2251 tokens\n",
      "2021-12-13 07:42:23 - Truncated by 2269 tokens\n",
      "2021-12-13 07:42:24 - \n",
      "\n",
      "2021-12-13 07:42:24 - NDCG@1: 0.5562\n",
      "2021-12-13 07:42:24 - NDCG@3: 0.5199\n",
      "2021-12-13 07:42:24 - NDCG@5: 0.4926\n",
      "2021-12-13 07:42:24 - NDCG@10: 0.4527\n",
      "2021-12-13 07:42:24 - NDCG@100: 0.3936\n",
      "2021-12-13 07:42:24 - NDCG@1000: 0.3733\n",
      "2021-12-13 07:42:24 - \n",
      "\n",
      "2021-12-13 07:42:24 - MAP@1: 0.0231\n",
      "2021-12-13 07:42:24 - MAP@3: 0.0539\n",
      "2021-12-13 07:42:24 - MAP@5: 0.0751\n",
      "2021-12-13 07:42:24 - MAP@10: 0.1083\n",
      "2021-12-13 07:42:24 - MAP@100: 0.2006\n",
      "2021-12-13 07:42:24 - MAP@1000: 0.2006\n",
      "2021-12-13 07:42:24 - \n",
      "\n",
      "2021-12-13 07:42:24 - Recall@1: 0.0231\n",
      "2021-12-13 07:42:24 - Recall@3: 0.0608\n",
      "2021-12-13 07:42:24 - Recall@5: 0.0891\n",
      "2021-12-13 07:42:24 - Recall@10: 0.1422\n",
      "2021-12-13 07:42:24 - Recall@100: 0.3854\n",
      "2021-12-13 07:42:24 - Recall@1000: 0.3854\n",
      "2021-12-13 07:42:24 - \n",
      "\n",
      "2021-12-13 07:42:24 - P@1: 0.6064\n",
      "2021-12-13 07:42:24 - P@3: 0.5502\n",
      "2021-12-13 07:42:24 - P@5: 0.5068\n",
      "2021-12-13 07:42:24 - P@10: 0.4313\n",
      "2021-12-13 07:42:24 - P@100: 0.1606\n",
      "2021-12-13 07:42:24 - P@1000: 0.0161\n"
     ]
    }
   ],
   "source": [
    "### Main Loop A: Using fewshot=0, varying prompts with GPT Reranker ###\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "# All datasets\n",
    "datasets = [\"trec-covid\", \"webis-touche2020\", \"nfcorpus\", \"scifact\", \"fiqa\", \"dbpedia-entity\",\n",
    "            \"nq\", \"hotpotqa\", \"quora\", \"fever\", \"climate-fever\", \"arguana\", \"msmarco\", \"scidocs\", \n",
    "            \"trec-news\", \"cqadupstack\"]\n",
    "\n",
    "\n",
    "datasets = [\"robust04\"]\n",
    "\n",
    "\n",
    "# \"trec-covid\", \"webis-touche2020\", \"nfcorpus\", \"scifact\", \"fiqa\", \"dbpedia-entity\",\"nq\", \"hotpotqa\", \"quora\", \n",
    "\n",
    "# Datasets by speed - Used for prompt ablations\n",
    "#datasets = [\"trec-covid\"]#, \"webis-touche2020\", \"nfcorpus\", \"scifact\", \"fiqa\", \"dbpedia-entity\"]\n",
    "\n",
    "# Main prompt\n",
    "prompts = {\"G\": 'Documents are searched to find matches with the same content.\\nThe document \"{}\" is a good search result for \"',}\n",
    "\n",
    "\n",
    "model_out_name = \"aleph\"\n",
    "\n",
    "def clean_titles(corpus):\n",
    "    for k in corpus:\n",
    "        if \"title\" in corpus[k] and corpus[k][\"title\"] is None:\n",
    "            corpus[k][\"title\"] = \"\"\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def run_reranking(results_bm25_path, results_path, data_path, top_k=100, k_values=[1, 3, 5, 10, 100, 1000]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_bm25_path: Path to .json results from bm25 for the dataset\n",
    "        results_path: Path to .json to write rerank results\n",
    "        top_k: How many docs to rerank per query\n",
    "        k_values: For how many docs per query to compute the scores\n",
    "    \"\"\"\n",
    "    \n",
    "    split = \"dev\" if \"msmarco\" in data_path else \"test\"\n",
    "    \n",
    "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=split)\n",
    "    \n",
    "    corpus = clean_titles(corpus) if \"robust04\" in data_path else corpus\n",
    "    \n",
    "    with open(results_bm25_path, 'r') as fp:\n",
    "        results_bm25 = json.load(fp)\n",
    "    \n",
    "    # Optional, make sure results are correct\n",
    "    ndcg_bm25, _map_bm25, recall_bm25, precision_bm25 = EvaluateRetrieval.evaluate(qrels, results_bm25, k_values)\n",
    "\n",
    "    # Rerank top-100 results using the reranker provided\n",
    "    results_rerank = reranker.rerank(corpus, queries, results_bm25, top_k=top_k)\n",
    "    \n",
    "    # Save rerank results\n",
    "    with open(results_path, 'w') as fp:\n",
    "        json.dump(results_rerank, fp)\n",
    "\n",
    "    #### Evaluate retrieval using NDCG@k, MAP@K ...\n",
    "    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, results_rerank, k_values)\n",
    "\n",
    "    return (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision)\n",
    "\n",
    "\n",
    "for prompt_id, prompt_doc in prompts.items():\n",
    "    \n",
    "    scores_out_path = f\"beir_scores_{model_out_name}_{prompt_id}.json\"\n",
    "    if os.path.exists(os.path.join(os.getcwd(), scores_out_path)):\n",
    "        continue\n",
    "\n",
    "    ndcgs_bm25 = {}\n",
    "    ndcgs = {}\n",
    "    \n",
    "    logging.info(f\"\\n{'-' * 20} Running prompt {prompt_id}: {prompt_doc} {'-' * 20}\\n\")\n",
    "    \n",
    "    reranker = Rerank(AARanker(use_prompt=True, prompt_doc=prompt_doc), batch_size=128)\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "\n",
    "        logging.info(f\"\\n{'-' * 10} Running {dataset} {'-' * 10}\\n\")\n",
    "        \n",
    "        if not(os.path.exists(os.path.join(os.getcwd(), 'datasets', dataset))):\n",
    "            url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "            out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "            data_path = util.download_and_unzip(url, out_dir)\n",
    "            print(\"Dataset downloaded here: {}\".format(data_path))\n",
    "            \n",
    "        # Load the dataset into BEIR\n",
    "        data_path = f\"datasets/{dataset}\"\n",
    "\n",
    "        # cqadupstack - Contains several sub datasets\n",
    "        if dataset == \"cqadupstack\":\n",
    "            cqa_ndcgs_bm25, cqa_maps_bm25, cqa_recalls_bm25, cqa_precisions_bm25 = [], [], [], []\n",
    "            cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "            for sub_dataset in os.listdir(data_path):\n",
    "                sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "                \n",
    "                results_bm25_path = f\"results_{dataset}_{sub_dataset}.json\"\n",
    "                results_path = f\"results_{model_out_name}_prompt{prompt_id}_{dataset}_{sub_dataset}.json\"\n",
    "                # Skip if already computed these results\n",
    "                if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                    continue\n",
    "\n",
    "                (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, sub_data_path)\n",
    "\n",
    "                cqa_ndcgs_bm25.append(ndcg)\n",
    "                cqa_maps_bm25.append(_map)\n",
    "                cqa_recalls_bm25.append(recall)\n",
    "                cqa_precisions_bm25.append(precision)\n",
    "\n",
    "                cqa_ndcgs.append(ndcg)\n",
    "                cqa_maps.append(_map)\n",
    "                cqa_recalls.append(recall)\n",
    "                cqa_precisions.append(precision)\n",
    "\n",
    "            for (metric, group) in [(ndcg_bm25, cqa_ndcgs_bm25), (_map_bm25, cqa_maps_bm25), (recall_bm25, cqa_recalls_bm25), (precision_bm25, cqa_precisions_bm25)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "            for (metric, group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "            logging.info(\"CQA Final BM25\")\n",
    "            logging.info(f\"{ndcg_bm25}\")\n",
    "            logging.info(f\"{_map_bm25}\")\n",
    "            logging.info(f\"{recall_bm25}\")\n",
    "            logging.info(f\"{precision_bm25}\")\n",
    "\n",
    "            logging.info(\"CQA Final\")\n",
    "            logging.info(f\"{ndcg}\")\n",
    "            logging.info(f\"{_map}\")\n",
    "            logging.info(f\"{recall}\")\n",
    "            logging.info(f\"{precision}\")\n",
    "\n",
    "        else:\n",
    "            results_bm25_path = f\"results_{dataset}.json\"\n",
    "            results_path = f\"results_{model_out_name}_prompt{prompt_id}_{dataset}.json\"\n",
    "            # Skip if already computed these results\n",
    "            if os.path.exists(os.path.join(os.getcwd(), results_path)):\n",
    "                continue\n",
    "            (ndcg_bm25, _map_bm25, recall_bm25, precision_bm25), (ndcg, _map, recall, precision) = run_reranking(results_bm25_path, results_path, data_path)\n",
    "\n",
    "        ndcgs[dataset] = ndcg\n",
    "        ndcgs_bm25[dataset] = ndcg_bm25\n",
    "\n",
    "        # Optionally clean-up each time to avoid running out of space\n",
    "        # !rm -r datasets\n",
    "\n",
    "    with open(scores_out_path, 'w') as fp:\n",
    "        json.dump(ndcgs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638d72a",
   "metadata": {},
   "source": [
    "##### Compute perfect rerank scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4316c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:57:41 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe6b207accf4fcaa703baae2bc3cf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:57:52 - Loaded 528155 TEST Documents.\n",
      "2021-12-17 16:57:52 - Doc Example: {'text': '\\n\\nPOLITICIANS,  PARTY PREFERENCES \\n\\n   Summary:  Newspapers in the Former Yugoslav Republic of \\n   Macedonia have published the results of opinion polls, \\n   indicating the relative popularity of politicians, \\n   political parties, and attitudes toward the political system. \\n\\n   The 22-23 January edition of the Skopje newspaper VECER in \\nMacedonian published on pages 6-7 the results of an opinion poll \\nconducted by the \"BriMa\" agency in November 1993. According to \\nVECER, 1,036 respondents were classified by age and residence, but \\nthe paper did not explain the methodology or give the margin of \\nerror.  For the purpose of comparison, the paper cited the results \\nof an unidentified poll made in May 1993. The approval/disapproval \\nratings, in percent, for ten Macedonian politicians were: \\n\\n                                           November 1993    May 1993 \\n\\nKiro Gligorov, President of the Republic      76/15           78/13 \\n\\nVasil Tupurkovski, former Macedonian          50/36           43/37 \\n   official in Federal Yugoslavia \\n\\nLjubomir Frckovski, Interior Minister         50/42           42/43 \\n\\nStojan Andov, Parliamentary Chairman          48/41           48/39 \\n\\nBranko Crvenkovski, Prime Minister            46/41           44/38 \\n\\nVlado Popovski, Defense Minister              41/41           36/37 \\n\\nStevo Crvenkovski, Foreign Minister           40/43   No Data Given \\n\\nPetar Gosev, Democratic Party leader          34/53           40/42 \\n\\nTodor Petrov, Independent parliamentarian     32/53   No Data Given \\n\\nNikola Popovski, Social Democratic            29/46           32/42 \\n   Party parliamentarian \\n\\n   VECER noted that President Gligorov\\'s very high approval rating \\nof 90 percent among those over age 65 fell off to a still high 70 \\npercent among respondents between 18 and 24.  Residents of Skopje \\nranked the politicians in a slightly different order from the \\nranking given by the whole sample: Gligorov, Tupurkovski, Frckovski, \\nAndov, Gosev, Branko Crvenkovski, Vlado Popovski, Petrov, Nikola \\nPopovski, and Stevo Crvenkovski. \\n\\n   The results of a series of opinion polls conducted by the Agency \\nfor Public Opinion Research and published \"exclusively\" by the \\nSkopje weekly PULS newspaper, confirmed Gligorov\\'s substantial lead \\nin popularity among political figures.  According to the 31 December \\n1993 issue of PULS (pages 16-18), the agency gathered the data by \\nmeans of telephone interviews with 300 residents in the Republic of \\nMacedonia during 20-24 December. PULS also provided data from \\nsurveys made in March, June, and September for comparison.  Some of \\nthe following percentages are approximate values that were derived \\nfrom the graph published by the paper: \\n\\n                         March       June      September    December \\n\\nKiro Gligorov             87          82.33      89.33           89 \\nStevo Crvenkovski         54          65         49              63 \\nStojan Andov              61          62         60              61 \\nBranko Crvenkovski        56          60         54 \\n53.5 \\nLjubomir Frckovski        35          45         48              50 \\nPetar Gosev               50          31         52 \\n49.53 \\nJovan Andonov, \\n Deputy Prime Minister    39          39         50              37 \\nVlado Popovski            18          25         36              35 \\nKiro Popovski, Deputy \\n Chairman, Parliament     26          27         33              32 \\nAnte Popovski, leader of \\n MAAK (Movement for All- \\n Macedonian Action)       29          32         32 \\nindistinct \\nJane Miljovski, Minister \\n without Portfolio        --          23         31              24 \\nVladimir Golubovski \\n VMRO-DP (Internal \\n Macedonian Revolutionary \\n Organization-Democratic \\n Party) leader            --          30         25              23 \\nNevzat Halili \\n Party for Democratic \\n Prosperity official      38.33       38         18              18 \\n\\nLj upco Georgievski \\nVMRO-DPMNE (Internal \\nMacedonian Revolutionary \\nOrganization-Democratic \\nParty for Macedonian \\nNational Unity) \\nofficial                  18          10         16              17 \\nDosta Dimovska \\nVMRO-DPMNE \\nofficial                  --          11         17              16 \\n\\n   On pages 6 and 7 of its 15-16 January issue, VECER also published \\nthe results of a November 1993 survey on party preferences. \\n\"BriMa,\" working with the Gallup organization, interviewed 1,036 \\npeople. \\n\\n   Question: \"If elections were held today, for which party would \\nyou vote?\" (all numbers are percentages) \\n\\nSDSM (Social Democratic Alliance of Macedonia)  22.8 \\nVMRO-DPMNE                                      11.2 \\nDemocratic Party (DP, led by Petar Gosev)        6.3 \\nSocialist Party                                  3.3 \\nLiberal Party (LP)                               3.2 \\nWorkers Party                                    2.9 \\nPCERM (Party for the Full Emancipation of \\n    Romanies in Macedonia)                       1.8 \\nDemocratic Party of Turks in Macedonia           0.8 \\nMAAK                                             0.3 \\nAnother party                                    4.0 \\nUndecided                                       18.6 \\nWould not vote                                   6.6 \\n\\n   VECER noted that some parties fared better in certain cities than \\ntheir overall scores indicate.  For example, the DP was about twice \\nas popular in Skopje as elsewhere, getting 12.1 percent in the \\ncapital; the VMRO-DPMNE was more popular in Bitola, getting 15.7 \\npercent, than in the remainder of the country; and the LP in the \\nBregalnica area got the support of 10.6 percent, substantially \\nhigher than the 3.2 percent support it received overall. \\n\\n   Question: \"Do you have confidence in the following parties?\" (all \\nnumbers are percentages) \\n\\n              Yes           No       Do Not Know \\n\\nSDSM           28           51          21 \\nVMRO-DPMNE     15           72          14 \\nLP             19           59          22 \\nPDP-NDP*       20           73           7 \\n\\n*Party for Democratic Prosperity-People\\'s Democratic Party \\n\\n   The poll clearly indicated that Macedonians have little \\nconfidence in any of the parties currently active in the country. \\nRespondents were also asked whether it would be good for the country \\nto have elections sooner than scheduled; 62 percent agreed, 20 \\npercent disagreed, and 18 percent did not know. These findings were \\ncorrelated with party preferences, producing the following results: \\nOf those who would vote for the SDSM, 54 percent wanted elections \\nsoon, while 34 percent were against early elections. However, 80 \\npercent of VMRO-DPMNE supporters favored elections soon, as did 79 \\npercent of LP supporters and 71 percent of DP supporters. While 80 \\npercent of those surveyed thought that a person should vote (14 \\npercent did not agree), only 40 percent thought that it was very \\nimportant which party won the elections and 27 percent thought it \\nwas somewhat significant. \\n\\n   (AUTHOR:  GALYEAN.  QUESTIONS AND/OR COMMENTS, PLEASE CALL CHIEF, \\nBALKANS BRANCH AT (703) 733-6481) \\n\\nELAG/25 February/POLCHF/EED/DEW 28/2023Z FEB \\n\\n\\n', 'title': None}\n",
      "2021-12-17 16:57:52 - Loading Queries...\n",
      "2021-12-17 16:57:52 - Loaded 249 TEST Queries.\n",
      "2021-12-17 16:57:52 - Query Example: Identify organizations that participate in international criminal\n",
      "activity, the activity, and, if possible, collaborating organizations\n",
      "and the countries involved.\n",
      "2021-12-17 16:57:56 - \n",
      "\n",
      "2021-12-17 16:57:56 - NDCG@1: 0.8594\n",
      "2021-12-17 16:57:56 - NDCG@10: 0.5080\n",
      "2021-12-17 16:57:56 - \n",
      "\n",
      "2021-12-17 16:57:56 - MAP@1: 0.0355\n",
      "2021-12-17 16:57:56 - MAP@10: 0.1418\n",
      "2021-12-17 16:57:56 - \n",
      "\n",
      "2021-12-17 16:57:56 - Recall@1: 0.0355\n",
      "2021-12-17 16:57:56 - Recall@10: 0.1418\n",
      "2021-12-17 16:57:56 - \n",
      "\n",
      "2021-12-17 16:57:56 - P@1: 0.8875\n",
      "2021-12-17 16:57:56 - P@10: 0.4024\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "# Subselect datasets\n",
    "datasets = [\"robust04\"]\n",
    "\n",
    "k_values = [1, 10]\n",
    "max_rerank = 10 # In BEIR 100 documents are reranked for their rerank encoder benchmark\n",
    "\n",
    "# Need to rerank min 100 for @100;\n",
    "assert max_rerank >= max(k_values), \"Max Rerank is too small for the sample scores to compute\"\n",
    "\n",
    "ndcgs = {}\n",
    "\n",
    "def perfect_rerank(results, qrels, max_rerank):\n",
    "    \"\"\"\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    "    results: Dict[str, Dict[str, float]]\n",
    "    \"\"\"\n",
    "    perfect_rerank_results = {}\n",
    "    for qid in qrels:\n",
    "        if qid in results:\n",
    "            # Subselect max rerank items with highest score\n",
    "            topk = sorted(results[qid].items(), key=lambda item: item[1], reverse=True)[:max_rerank]\n",
    "            topk = [key_val_pair[0] for key_val_pair in topk]\n",
    "\n",
    "            perfect_rerank_results[qid] = {doc: float(score) for doc, score in qrels[qid].items() if doc in topk}\n",
    "        else:\n",
    "            # CAUTION: Skipping as we do here inflates the results vs putting an empty list, as the score is ignored\n",
    "            # It might be more appropriate to put an empty list, i.e. no results, i.e. worse total score\n",
    "            # However it seems like the default in BEIR is not to do so - Also only one dataset is affected: NFCorpus\n",
    "            logging.info(f\"Skipping {qid}\")\n",
    "        # Adding all other results with lower ranks is unnecessary as NDCG ignores them anyways\n",
    "        #extra_results = {doc: float(score) * 0.0001 for doc, score in results[qid].items() if doc not in perfect_rerank_results[qid]}\n",
    "        #perfect_rerank_results[qid] = {**perfect_rerank_results[qid], **extra_results}\n",
    "\n",
    "    return perfect_rerank_results\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "\n",
    "    if not(os.path.exists(os.path.join(os.getcwd(), 'datasets', dataset))):\n",
    "        url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "        out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "        data_path = util.download_and_unzip(url, out_dir)\n",
    "        print(\"Dataset downloaded here: {}\".format(data_path))\n",
    "    # Load the dataset into BEIR\n",
    "    data_path = f\"datasets/{dataset}\"\n",
    "    # In the paper it says, BEIR used the dev set for msmarco\n",
    "    split = \"dev\" if dataset == \"msmarco\" else \"test\"\n",
    "\n",
    "    # cqadupstack - Contains several sub datasets\n",
    "    if dataset == \"cqadupstack\":\n",
    "        cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "        for sub_dataset in os.listdir(data_path):\n",
    "            sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "            corpus, queries, qrels = GenericDataLoader(sub_data_path).load(split=split)\n",
    "            with open(f\"./results_{dataset}_{sub_dataset}.json\", 'r') as fp:\n",
    "                results_loaded = json.load(fp)\n",
    "            \n",
    "            perfect_rerank_results = perfect_rerank(results_loaded, qrels, max_rerank)\n",
    "\n",
    "            with open(f\"./beir_perfect_rerank_{max_rerank}_{dataset}_{sub_dataset}.json\", 'w') as fp:\n",
    "                json.dump(perfect_rerank_results, fp)\n",
    "\n",
    "            ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, perfect_rerank_results, k_values)\n",
    "\n",
    "            cqa_ndcgs.append(ndcg)\n",
    "            cqa_maps.append(_map)\n",
    "            cqa_recalls.append(recall)\n",
    "            cqa_precisions.append(precision)\n",
    "\n",
    "        for (metric, group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "            for k in metric.keys():\n",
    "                metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "        logging.info(\"CQA Final\")\n",
    "        logging.info(f\"{ndcg}\")\n",
    "        logging.info(f\"{_map}\")\n",
    "        logging.info(f\"{recall}\")\n",
    "        logging.info(f\"{precision}\")\n",
    "\n",
    "    else:\n",
    "        corpus, queries, qrels = GenericDataLoader(data_path).load(split=split)\n",
    "        with open(f\"./results_{dataset}.json\", 'r') as fp:\n",
    "            results_loaded = json.load(fp)\n",
    "        perfect_rerank_results = perfect_rerank(results_loaded, qrels, max_rerank)\n",
    "        ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, perfect_rerank_results, k_values)\n",
    "        with open(f\"./beir_perfect_rerank_{max_rerank}_{dataset}.json\", 'w') as fp:\n",
    "            json.dump(perfect_rerank_results, fp)\n",
    "\n",
    "    ndcgs[dataset] = ndcg\n",
    "\n",
    "    # Cleanup if necessary\n",
    "    #if (\"signal1m\" not in data_path) and (\"trec-news\" not in data_path):\n",
    "    #    !rm -r {data_path}\n",
    "    #    !rm -r {data_path}.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32e49c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:56:35 - Loading faiss with AVX2 support.\n",
      "2021-12-20 13:56:35 - Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2021-12-20 13:56:35 - Loading faiss.\n",
      "2021-12-20 13:56:35 - Successfully loaded faiss.\n",
      "2021-12-20 13:56:35 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f750160488046fcb0b4e3eeb8629920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:56:36 - Loaded 171332 TEST Documents.\n",
      "2021-12-20 13:56:36 - Doc Example: {'text': 'OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'title': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia'}\n",
      "2021-12-20 13:56:36 - Loading Queries...\n",
      "2021-12-20 13:56:36 - Loaded 50 TEST Queries.\n",
      "2021-12-20 13:56:36 - Query Example: what is the origin of COVID-19\n",
      "2021-12-20 13:56:36 - \n",
      "\n",
      "2021-12-20 13:56:36 - NDCG@1: 0.9900\n",
      "2021-12-20 13:56:36 - NDCG@3: 0.9717\n",
      "2021-12-20 13:56:36 - NDCG@5: 0.9110\n",
      "2021-12-20 13:56:36 - NDCG@10: 0.7496\n",
      "2021-12-20 13:56:36 - NDCG@100: 0.1645\n",
      "2021-12-20 13:56:36 - \n",
      "\n",
      "2021-12-20 13:56:36 - MAP@1: 0.0027\n",
      "2021-12-20 13:56:36 - MAP@3: 0.0080\n",
      "2021-12-20 13:56:36 - MAP@5: 0.0124\n",
      "2021-12-20 13:56:36 - MAP@10: 0.0191\n",
      "2021-12-20 13:56:36 - MAP@100: 0.0191\n",
      "2021-12-20 13:56:36 - \n",
      "\n",
      "2021-12-20 13:56:36 - Recall@1: 0.0027\n",
      "2021-12-20 13:56:36 - Recall@3: 0.0080\n",
      "2021-12-20 13:56:36 - Recall@5: 0.0124\n",
      "2021-12-20 13:56:36 - Recall@10: 0.0191\n",
      "2021-12-20 13:56:36 - Recall@100: 0.0191\n",
      "2021-12-20 13:56:36 - \n",
      "\n",
      "2021-12-20 13:56:36 - P@1: 1.0000\n",
      "2021-12-20 13:56:36 - P@3: 1.0000\n",
      "2021-12-20 13:56:36 - P@5: 0.9360\n",
      "2021-12-20 13:56:36 - P@10: 0.7340\n",
      "2021-12-20 13:56:36 - P@100: 0.0734\n",
      "2021-12-20 13:56:37 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c10fba77955440d98253e822d771088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:56:41 - Loaded 382545 TEST Documents.\n",
      "2021-12-20 13:56:41 - Doc Example: {'text': 'My opponent forfeited every round. None of my arguments were answered. I don’t like the idea of winning by default, but here we are.Tule: it’s good for students to get involved and address big issues like teen pregnancy. You need to be able to answer arguments like mine and not simply prepare for an abstinence-only type of response. You should also be aware that, in the U.S., condoms may be sold to minors in ANY state. A retailer who says it is illegal to sell you them is, frankly, wrong.', 'title': 'Contraceptive Forms for High School Students'}\n",
      "2021-12-20 13:56:41 - Loading Queries...\n",
      "2021-12-20 13:56:41 - Loaded 49 TEST Queries.\n",
      "2021-12-20 13:56:41 - Query Example: Should teachers get tenure?\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - NDCG@1: 0.8571\n",
      "2021-12-20 13:56:41 - NDCG@3: 0.7704\n",
      "2021-12-20 13:56:41 - NDCG@5: 0.6615\n",
      "2021-12-20 13:56:41 - NDCG@10: 0.4666\n",
      "2021-12-20 13:56:41 - NDCG@100: 0.3609\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - MAP@1: 0.0573\n",
      "2021-12-20 13:56:41 - MAP@3: 0.1477\n",
      "2021-12-20 13:56:41 - MAP@5: 0.2002\n",
      "2021-12-20 13:56:41 - MAP@10: 0.2122\n",
      "2021-12-20 13:56:41 - MAP@100: 0.2122\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - Recall@1: 0.0573\n",
      "2021-12-20 13:56:41 - Recall@3: 0.1477\n",
      "2021-12-20 13:56:41 - Recall@5: 0.2002\n",
      "2021-12-20 13:56:41 - Recall@10: 0.2122\n",
      "2021-12-20 13:56:41 - Recall@100: 0.2122\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - P@1: 0.8980\n",
      "2021-12-20 13:56:41 - P@3: 0.7755\n",
      "2021-12-20 13:56:41 - P@5: 0.6204\n",
      "2021-12-20 13:56:41 - P@10: 0.3306\n",
      "2021-12-20 13:56:41 - P@100: 0.0331\n",
      "2021-12-20 13:56:41 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ebb40c4821476ea94e23c789ad8928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:56:41 - Loaded 3633 TEST Documents.\n",
      "2021-12-20 13:56:41 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
      "2021-12-20 13:56:41 - Loading Queries...\n",
      "2021-12-20 13:56:41 - Loaded 323 TEST Queries.\n",
      "2021-12-20 13:56:41 - Query Example: Do Cholesterol Statin Drugs Cause Breast Cancer?\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - NDCG@1: 0.6556\n",
      "2021-12-20 13:56:41 - NDCG@3: 0.5266\n",
      "2021-12-20 13:56:41 - NDCG@5: 0.4581\n",
      "2021-12-20 13:56:41 - NDCG@10: 0.3637\n",
      "2021-12-20 13:56:41 - NDCG@100: 0.2295\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - MAP@1: 0.0719\n",
      "2021-12-20 13:56:41 - MAP@3: 0.1158\n",
      "2021-12-20 13:56:41 - MAP@5: 0.1274\n",
      "2021-12-20 13:56:41 - MAP@10: 0.1350\n",
      "2021-12-20 13:56:41 - MAP@100: 0.1350\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - Recall@1: 0.0719\n",
      "2021-12-20 13:56:41 - Recall@3: 0.1158\n",
      "2021-12-20 13:56:41 - Recall@5: 0.1274\n",
      "2021-12-20 13:56:41 - Recall@10: 0.1350\n",
      "2021-12-20 13:56:41 - Recall@100: 0.1350\n",
      "2021-12-20 13:56:41 - \n",
      "\n",
      "2021-12-20 13:56:41 - P@1: 0.6722\n",
      "2021-12-20 13:56:41 - P@3: 0.4857\n",
      "2021-12-20 13:56:41 - P@5: 0.3755\n",
      "2021-12-20 13:56:41 - P@10: 0.2262\n",
      "2021-12-20 13:56:41 - P@100: 0.0226\n",
      "2021-12-20 13:56:41 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11644149990a480685fe2443d6f5f327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:56:42 - Loaded 5183 TEST Documents.\n",
      "2021-12-20 13:56:42 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2021-12-20 13:56:42 - Loading Queries...\n",
      "2021-12-20 13:56:42 - Loaded 300 TEST Queries.\n",
      "2021-12-20 13:56:42 - Query Example: 0-dimensional biomaterials show inductive properties.\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - NDCG@1: 0.7400\n",
      "2021-12-20 13:56:42 - NDCG@3: 0.7299\n",
      "2021-12-20 13:56:42 - NDCG@5: 0.7287\n",
      "2021-12-20 13:56:42 - NDCG@10: 0.7287\n",
      "2021-12-20 13:56:42 - NDCG@100: 0.7287\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - MAP@1: 0.7051\n",
      "2021-12-20 13:56:42 - MAP@3: 0.7241\n",
      "2021-12-20 13:56:42 - MAP@5: 0.7249\n",
      "2021-12-20 13:56:42 - MAP@10: 0.7249\n",
      "2021-12-20 13:56:42 - MAP@100: 0.7249\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - Recall@1: 0.7051\n",
      "2021-12-20 13:56:42 - Recall@3: 0.7241\n",
      "2021-12-20 13:56:42 - Recall@5: 0.7249\n",
      "2021-12-20 13:56:42 - Recall@10: 0.7249\n",
      "2021-12-20 13:56:42 - Recall@100: 0.7249\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - P@1: 0.7400\n",
      "2021-12-20 13:56:42 - P@3: 0.2644\n",
      "2021-12-20 13:56:42 - P@5: 0.1593\n",
      "2021-12-20 13:56:42 - P@10: 0.0797\n",
      "2021-12-20 13:56:42 - P@100: 0.0080\n",
      "2021-12-20 13:56:42 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71196bd96ee44141a2f6b319ed272d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:56:42 - Loaded 57638 TEST Documents.\n",
      "2021-12-20 13:56:42 - Doc Example: {'text': \"I'm not saying I don't like the idea of on-the-job training too, but you can't expect the company to do that. Training workers is not their job - they're building software. Perhaps educational systems in the U.S. (or their students) should worry a little about getting marketable skills in exchange for their massive investment in education, rather than getting out with thousands in student debt and then complaining that they aren't qualified to do anything.\", 'title': ''}\n",
      "2021-12-20 13:56:42 - Loading Queries...\n",
      "2021-12-20 13:56:42 - Loaded 648 TEST Queries.\n",
      "2021-12-20 13:56:42 - Query Example: How to deposit a cheque issued to an associate in my business into my business account?\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - NDCG@1: 0.5015\n",
      "2021-12-20 13:56:42 - NDCG@3: 0.3874\n",
      "2021-12-20 13:56:42 - NDCG@5: 0.3697\n",
      "2021-12-20 13:56:42 - NDCG@10: 0.3632\n",
      "2021-12-20 13:56:42 - NDCG@100: 0.3627\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - MAP@1: 0.2699\n",
      "2021-12-20 13:56:42 - MAP@3: 0.3210\n",
      "2021-12-20 13:56:42 - MAP@5: 0.3244\n",
      "2021-12-20 13:56:42 - MAP@10: 0.3244\n",
      "2021-12-20 13:56:42 - MAP@100: 0.3244\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - Recall@1: 0.2699\n",
      "2021-12-20 13:56:42 - Recall@3: 0.3210\n",
      "2021-12-20 13:56:42 - Recall@5: 0.3244\n",
      "2021-12-20 13:56:42 - Recall@10: 0.3244\n",
      "2021-12-20 13:56:42 - Recall@100: 0.3244\n",
      "2021-12-20 13:56:42 - \n",
      "\n",
      "2021-12-20 13:56:42 - P@1: 0.5015\n",
      "2021-12-20 13:56:42 - P@3: 0.2274\n",
      "2021-12-20 13:56:42 - P@5: 0.1404\n",
      "2021-12-20 13:56:42 - P@10: 0.0702\n",
      "2021-12-20 13:56:42 - P@100: 0.0070\n",
      "2021-12-20 13:56:42 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a0018579da4e94ad91d6e786f5a90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4635922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:57:09 - Loaded 4635922 TEST Documents.\n",
      "2021-12-20 13:57:09 - Doc Example: {'text': \"Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold.   A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.\", 'title': 'Animalia (book)'}\n",
      "2021-12-20 13:57:09 - Loading Queries...\n",
      "2021-12-20 13:57:09 - Loaded 400 TEST Queries.\n",
      "2021-12-20 13:57:09 - Query Example: Szechwan dish food cuisine\n",
      "2021-12-20 13:57:09 - \n",
      "\n",
      "2021-12-20 13:57:09 - NDCG@1: 0.6800\n",
      "2021-12-20 13:57:09 - NDCG@3: 0.5504\n",
      "2021-12-20 13:57:09 - NDCG@5: 0.4787\n",
      "2021-12-20 13:57:09 - NDCG@10: 0.3974\n",
      "2021-12-20 13:57:09 - NDCG@100: 0.3063\n",
      "2021-12-20 13:57:09 - \n",
      "\n",
      "2021-12-20 13:57:09 - MAP@1: 0.1031\n",
      "2021-12-20 13:57:09 - MAP@3: 0.1742\n",
      "2021-12-20 13:57:09 - MAP@5: 0.1947\n",
      "2021-12-20 13:57:09 - MAP@10: 0.2089\n",
      "2021-12-20 13:57:09 - MAP@100: 0.2089\n",
      "2021-12-20 13:57:09 - \n",
      "\n",
      "2021-12-20 13:57:09 - Recall@1: 0.1031\n",
      "2021-12-20 13:57:09 - Recall@3: 0.1742\n",
      "2021-12-20 13:57:09 - Recall@5: 0.1947\n",
      "2021-12-20 13:57:09 - Recall@10: 0.2089\n",
      "2021-12-20 13:57:09 - Recall@100: 0.2089\n",
      "2021-12-20 13:57:09 - \n",
      "\n",
      "2021-12-20 13:57:09 - P@1: 0.8100\n",
      "2021-12-20 13:57:09 - P@3: 0.6033\n",
      "2021-12-20 13:57:09 - P@5: 0.4590\n",
      "2021-12-20 13:57:09 - P@10: 0.2747\n",
      "2021-12-20 13:57:09 - P@100: 0.0275\n",
      "2021-12-20 13:57:10 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9080a6aaa0494236ac437b5c15f68687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2681468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:57:26 - Loaded 2681468 TEST Documents.\n",
      "2021-12-20 13:57:26 - Doc Example: {'text': \"In accounting, minority interest (or non-controlling interest) is the portion of a subsidiary corporation's stock that is not owned by the parent corporation. The magnitude of the minority interest in the subsidiary company is generally less than 50% of outstanding shares, or the corporation would generally cease to be a subsidiary of the parent.[1]\", 'title': 'Minority interest'}\n",
      "2021-12-20 13:57:26 - Loading Queries...\n",
      "2021-12-20 13:57:26 - Loaded 3452 TEST Queries.\n",
      "2021-12-20 13:57:26 - Query Example: what is non controlling interest on balance sheet\n",
      "2021-12-20 13:57:26 - \n",
      "\n",
      "2021-12-20 13:57:26 - NDCG@1: 0.5403\n",
      "2021-12-20 13:57:26 - NDCG@3: 0.5143\n",
      "2021-12-20 13:57:26 - NDCG@5: 0.5143\n",
      "2021-12-20 13:57:26 - NDCG@10: 0.5143\n",
      "2021-12-20 13:57:26 - NDCG@100: 0.5143\n",
      "2021-12-20 13:57:26 - \n",
      "\n",
      "2021-12-20 13:57:26 - MAP@1: 0.4818\n",
      "2021-12-20 13:57:26 - MAP@3: 0.5066\n",
      "2021-12-20 13:57:26 - MAP@5: 0.5067\n",
      "2021-12-20 13:57:26 - MAP@10: 0.5067\n",
      "2021-12-20 13:57:26 - MAP@100: 0.5067\n",
      "2021-12-20 13:57:26 - \n",
      "\n",
      "2021-12-20 13:57:26 - Recall@1: 0.4818\n",
      "2021-12-20 13:57:26 - Recall@3: 0.5066\n",
      "2021-12-20 13:57:26 - Recall@5: 0.5067\n",
      "2021-12-20 13:57:26 - Recall@10: 0.5067\n",
      "2021-12-20 13:57:26 - Recall@100: 0.5067\n",
      "2021-12-20 13:57:26 - \n",
      "\n",
      "2021-12-20 13:57:26 - P@1: 0.5403\n",
      "2021-12-20 13:57:26 - P@3: 0.1979\n",
      "2021-12-20 13:57:26 - P@5: 0.1188\n",
      "2021-12-20 13:57:26 - P@10: 0.0594\n",
      "2021-12-20 13:57:26 - P@100: 0.0059\n",
      "2021-12-20 13:57:27 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bad637e33f347fa8f138a01aad8977d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5233329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:57:56 - Loaded 5233329 TEST Documents.\n",
      "2021-12-20 13:57:56 - Doc Example: {'text': 'Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions. These are often described as stateless societies, although several authors have defined them more specifically as institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary and harmful.', 'title': 'Anarchism'}\n",
      "2021-12-20 13:57:56 - Loading Queries...\n",
      "2021-12-20 13:57:57 - Loaded 7405 TEST Queries.\n",
      "2021-12-20 13:57:57 - Query Example: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "2021-12-20 13:57:57 - \n",
      "\n",
      "2021-12-20 13:57:57 - NDCG@1: 0.8952\n",
      "2021-12-20 13:57:57 - NDCG@3: 0.6896\n",
      "2021-12-20 13:57:57 - NDCG@5: 0.6896\n",
      "2021-12-20 13:57:57 - NDCG@10: 0.6896\n",
      "2021-12-20 13:57:57 - NDCG@100: 0.6896\n",
      "2021-12-20 13:57:57 - \n",
      "\n",
      "2021-12-20 13:57:57 - MAP@1: 0.4476\n",
      "2021-12-20 13:57:57 - MAP@3: 0.6295\n",
      "2021-12-20 13:57:57 - MAP@5: 0.6295\n",
      "2021-12-20 13:57:57 - MAP@10: 0.6295\n",
      "2021-12-20 13:57:57 - MAP@100: 0.6295\n",
      "2021-12-20 13:57:57 - \n",
      "\n",
      "2021-12-20 13:57:57 - Recall@1: 0.4476\n",
      "2021-12-20 13:57:57 - Recall@3: 0.6295\n",
      "2021-12-20 13:57:57 - Recall@5: 0.6295\n",
      "2021-12-20 13:57:57 - Recall@10: 0.6295\n",
      "2021-12-20 13:57:57 - Recall@100: 0.6295\n",
      "2021-12-20 13:57:57 - \n",
      "\n",
      "2021-12-20 13:57:57 - P@1: 0.8952\n",
      "2021-12-20 13:57:57 - P@3: 0.4197\n",
      "2021-12-20 13:57:57 - P@5: 0.2518\n",
      "2021-12-20 13:57:57 - P@10: 0.1259\n",
      "2021-12-20 13:57:57 - P@100: 0.0126\n",
      "2021-12-20 13:57:58 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09e5b01e43f49f28bd0774d94d1cbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/522931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:57:59 - Loaded 522931 TEST Documents.\n",
      "2021-12-20 13:57:59 - Doc Example: {'text': 'What is the step by step guide to invest in share market in india?', 'title': ''}\n",
      "2021-12-20 13:57:59 - Loading Queries...\n",
      "2021-12-20 13:58:00 - Loaded 10000 TEST Queries.\n",
      "2021-12-20 13:58:00 - Query Example: Which question should I ask on Quora?\n",
      "2021-12-20 13:58:00 - \n",
      "\n",
      "2021-12-20 13:58:00 - NDCG@1: 0.9447\n",
      "2021-12-20 13:58:00 - NDCG@3: 0.9233\n",
      "2021-12-20 13:58:00 - NDCG@5: 0.9174\n",
      "2021-12-20 13:58:00 - NDCG@10: 0.9138\n",
      "2021-12-20 13:58:00 - NDCG@100: 0.9125\n",
      "2021-12-20 13:58:00 - \n",
      "\n",
      "2021-12-20 13:58:00 - MAP@1: 0.8140\n",
      "2021-12-20 13:58:00 - MAP@3: 0.8969\n",
      "2021-12-20 13:58:00 - MAP@5: 0.9017\n",
      "2021-12-20 13:58:00 - MAP@10: 0.9026\n",
      "2021-12-20 13:58:00 - MAP@100: 0.9026\n",
      "2021-12-20 13:58:00 - \n",
      "\n",
      "2021-12-20 13:58:00 - Recall@1: 0.8140\n",
      "2021-12-20 13:58:00 - Recall@3: 0.8969\n",
      "2021-12-20 13:58:00 - Recall@5: 0.9017\n",
      "2021-12-20 13:58:00 - Recall@10: 0.9026\n",
      "2021-12-20 13:58:00 - Recall@100: 0.9026\n",
      "2021-12-20 13:58:00 - \n",
      "\n",
      "2021-12-20 13:58:00 - P@1: 0.9447\n",
      "2021-12-20 13:58:00 - P@3: 0.3924\n",
      "2021-12-20 13:58:00 - P@5: 0.2419\n",
      "2021-12-20 13:58:00 - P@10: 0.1220\n",
      "2021-12-20 13:58:00 - P@100: 0.0122\n",
      "2021-12-20 13:58:00 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67da202f39b463dbf489cb3435e2ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:58:32 - Loaded 5416568 TEST Documents.\n",
      "2021-12-20 13:58:32 - Doc Example: {'text': 'The following are the football ( soccer ) events of the year 1928 throughout the world .', 'title': '1928 in association football'}\n",
      "2021-12-20 13:58:32 - Loading Queries...\n",
      "2021-12-20 13:58:33 - Loaded 6666 TEST Queries.\n",
      "2021-12-20 13:58:33 - Query Example: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n",
      "2021-12-20 13:58:33 - \n",
      "\n",
      "2021-12-20 13:58:33 - NDCG@1: 0.8609\n",
      "2021-12-20 13:58:33 - NDCG@3: 0.8258\n",
      "2021-12-20 13:58:33 - NDCG@5: 0.8246\n",
      "2021-12-20 13:58:33 - NDCG@10: 0.8243\n",
      "2021-12-20 13:58:33 - NDCG@100: 0.8243\n",
      "2021-12-20 13:58:33 - \n",
      "\n",
      "2021-12-20 13:58:33 - MAP@1: 0.8019\n",
      "2021-12-20 13:58:33 - MAP@3: 0.8141\n",
      "2021-12-20 13:58:33 - MAP@5: 0.8141\n",
      "2021-12-20 13:58:33 - MAP@10: 0.8141\n",
      "2021-12-20 13:58:33 - MAP@100: 0.8141\n",
      "2021-12-20 13:58:33 - \n",
      "\n",
      "2021-12-20 13:58:33 - Recall@1: 0.8019\n",
      "2021-12-20 13:58:33 - Recall@3: 0.8141\n",
      "2021-12-20 13:58:33 - Recall@5: 0.8141\n",
      "2021-12-20 13:58:33 - Recall@10: 0.8141\n",
      "2021-12-20 13:58:33 - Recall@100: 0.8141\n",
      "2021-12-20 13:58:33 - \n",
      "\n",
      "2021-12-20 13:58:33 - P@1: 0.8609\n",
      "2021-12-20 13:58:33 - P@3: 0.2961\n",
      "2021-12-20 13:58:33 - P@5: 0.1777\n",
      "2021-12-20 13:58:33 - P@10: 0.0888\n",
      "2021-12-20 13:58:33 - P@100: 0.0089\n",
      "2021-12-20 13:58:34 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffc224266074484a39b3f2c0505dad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:59:05 - Loaded 5416593 TEST Documents.\n",
      "2021-12-20 13:59:06 - Doc Example: {'text': 'The following are the football ( soccer ) events of the year 1928 throughout the world .', 'title': '1928 in association football'}\n",
      "2021-12-20 13:59:06 - Loading Queries...\n",
      "2021-12-20 13:59:06 - Loaded 1535 TEST Queries.\n",
      "2021-12-20 13:59:06 - Query Example: Global warming is driving polar bears toward extinction\n",
      "2021-12-20 13:59:06 - \n",
      "\n",
      "2021-12-20 13:59:06 - NDCG@1: 0.4612\n",
      "2021-12-20 13:59:06 - NDCG@3: 0.2963\n",
      "2021-12-20 13:59:06 - NDCG@5: 0.2805\n",
      "2021-12-20 13:59:06 - NDCG@10: 0.2805\n",
      "2021-12-20 13:59:06 - NDCG@100: 0.2805\n",
      "2021-12-20 13:59:06 - \n",
      "\n",
      "2021-12-20 13:59:06 - MAP@1: 0.1957\n",
      "2021-12-20 13:59:06 - MAP@3: 0.2316\n",
      "2021-12-20 13:59:06 - MAP@5: 0.2318\n",
      "2021-12-20 13:59:06 - MAP@10: 0.2318\n",
      "2021-12-20 13:59:06 - MAP@100: 0.2318\n",
      "2021-12-20 13:59:06 - \n",
      "\n",
      "2021-12-20 13:59:06 - Recall@1: 0.1957\n",
      "2021-12-20 13:59:06 - Recall@3: 0.2316\n",
      "2021-12-20 13:59:06 - Recall@5: 0.2318\n",
      "2021-12-20 13:59:06 - Recall@10: 0.2318\n",
      "2021-12-20 13:59:06 - Recall@100: 0.2318\n",
      "2021-12-20 13:59:06 - \n",
      "\n",
      "2021-12-20 13:59:06 - P@1: 0.4612\n",
      "2021-12-20 13:59:06 - P@3: 0.1915\n",
      "2021-12-20 13:59:06 - P@5: 0.1150\n",
      "2021-12-20 13:59:06 - P@10: 0.0575\n",
      "2021-12-20 13:59:06 - P@100: 0.0057\n",
      "2021-12-20 13:59:07 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc33417cff54432b489937b8641dec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:59:07 - Loaded 8674 TEST Documents.\n",
      "2021-12-20 13:59:07 - Doc Example: {'text': \"You don’t have to be vegetarian to be green. Many special environments have been created by livestock farming – for example chalk down land in England and mountain pastures in many countries. Ending livestock farming would see these areas go back to woodland with a loss of many unique plants and animals. Growing crops can also be very bad for the planet, with fertilisers and pesticides polluting rivers, lakes and seas. Most tropical forests are now cut down for timber, or to allow oil palm trees to be grown in plantations, not to create space for meat production.  British farmer and former editor Simon Farrell also states: “Many vegans and vegetarians rely on one source from the U.N. calculation that livestock generates 18% of global carbon emissions, but this figure contains basic mistakes. It attributes all deforestation from ranching to cattle, rather than logging or development. It also muddles up one-off emissions from deforestation with on-going pollution.”  He also refutes the statement of meat production inefficiency: “Scientists have calculated that globally the ratio between the amounts of useful plant food used to produce meat is about 5 to 1. If you feed animals only food that humans can eat — which is, indeed, largely the case in the Western world — that may be true. But animals also eat food we can't eat, such as grass. So the real conversion figure is 1.4 to 1.” [1] At the same time eating a vegetarian diet may be no more environmentally friendly than a meat based diet if it is not sustainably sourced or uses perishable fruit and vegetables that are flown in from around the world. Eating locally sourced food can has as big an impact as being vegetarian. [2]  [1] Tara Kelly, Simon Fairlie: How Eating Meat Can Save the World, 12 October 2010  [2] Lucy Siegle, ‘It is time to become a vegetarian?’ The Observer, 18th May 2008\", 'title': 'animals environment general health health general weight philosophy ethics'}\n",
      "2021-12-20 13:59:07 - Loading Queries...\n",
      "2021-12-20 13:59:07 - Loaded 1406 TEST Queries.\n",
      "2021-12-20 13:59:07 - Query Example: Being vegetarian helps the environment  Becoming a vegetarian is an environmentally friendly thing to do. Modern farming is one of the main sources of pollution in our rivers. Beef farming is one of the main causes of deforestation, and as long as people continue to buy fast food in their billions, there will be a financial incentive to continue cutting down trees to make room for cattle. Because of our desire to eat fish, our rivers and seas are being emptied of fish and many species are facing extinction. Energy resources are used up much more greedily by meat farming than my farming cereals, pulses etc. Eating meat and fish not only causes cruelty to animals, it causes serious harm to the environment and to biodiversity. For example consider Meat production related pollution and deforestation  At Toronto’s 1992 Royal Agricultural Winter Fair, Agriculture Canada displayed two contrasting statistics: “it takes four football fields of land (about 1.6 hectares) to feed each Canadian” and “one apple tree produces enough fruit to make 320 pies.” Think about it — a couple of apple trees and a few rows of wheat on a mere fraction of a hectare could produce enough food for one person! [1]  The 2006 U.N. Food and Agriculture Organization (FAO) report concluded that worldwide livestock farming generates 18% of the planet's greenhouse gas emissions — by comparison, all the world's cars, trains, planes and boats account for a combined 13% of greenhouse gas emissions. [2]  As a result of the above point producing meat damages the environment. The demand for meat drives deforestation. Daniel Cesar Avelino of Brazil's Federal Public Prosecution Office says “We know that the single biggest driver of deforestation in the Amazon is cattle.” This clearing of tropical rainforests such as the Amazon for agriculture is estimated to produce 17% of the world's greenhouse gas emissions. [3] Not only this but the production of meat takes a lot more energy than it ultimately gives us chicken meat production consumes energy in a 4:1 ratio to protein output; beef cattle production requires an energy input to protein output ratio of 54:1.  The same is true with water use due to the same phenomenon of meat being inefficient to produce in terms of the amount of grain needed to produce the same weight of meat, production requires a lot of water. Water is another scarce resource that we will soon not have enough of in various areas of the globe. Grain-fed beef production takes 100,000 liters of water for every kilogram of food. Raising broiler chickens takes 3,500 liters of water to make a kilogram of meat. In comparison, soybean production uses 2,000 liters for kilogram of food produced; rice, 1,912; wheat, 900; and potatoes, 500 liters. [4] This is while there are areas of the globe that have severe water shortages. With farming using up to 70 times more water than is used for domestic purposes: cooking and washing. A third of the population of the world is already suffering from a shortage of water. [5] Groundwater levels are falling all over the world and rivers are beginning to dry up. Already some of the biggest rivers such as China’s Yellow river do not reach the sea. [6]  With a rising population becoming vegetarian is the only responsible way to eat.  [1] Stephen Leckie, ‘How Meat-centred Eating Patterns Affect Food Security and the Environment’, International development research center  [2] Bryan Walsh, Meat: Making Global Warming Worse, Time magazine, 10 September 2008 .  [3] David Adam, Supermarket suppliers ‘helping to destroy Amazon rainforest’, The Guardian, 21st June 2009.  [4] Roger Segelken, U.S. could feed 800 million people with grain that livestock eat, Cornell Science News, 7th August 1997.  [5] Fiona Harvey, Water scarcity affects one in three, FT.com, 21st August 2003  [6] Rupert Wingfield-Hayes, Yellow river ‘drying up’, BBC News, 29th July 2004\n",
      "2021-12-20 13:59:07 - \n",
      "\n",
      "2021-12-20 13:59:07 - NDCG@1: 0.7539\n",
      "2021-12-20 13:59:07 - NDCG@3: 0.7539\n",
      "2021-12-20 13:59:07 - NDCG@5: 0.7539\n",
      "2021-12-20 13:59:07 - NDCG@10: 0.7539\n",
      "2021-12-20 13:59:07 - NDCG@100: 0.7539\n",
      "2021-12-20 13:59:07 - \n",
      "\n",
      "2021-12-20 13:59:07 - MAP@1: 0.7539\n",
      "2021-12-20 13:59:07 - MAP@3: 0.7539\n",
      "2021-12-20 13:59:07 - MAP@5: 0.7539\n",
      "2021-12-20 13:59:07 - MAP@10: 0.7539\n",
      "2021-12-20 13:59:07 - MAP@100: 0.7539\n",
      "2021-12-20 13:59:07 - \n",
      "\n",
      "2021-12-20 13:59:07 - Recall@1: 0.7539\n",
      "2021-12-20 13:59:07 - Recall@3: 0.7539\n",
      "2021-12-20 13:59:07 - Recall@5: 0.7539\n",
      "2021-12-20 13:59:07 - Recall@10: 0.7539\n",
      "2021-12-20 13:59:07 - Recall@100: 0.7539\n",
      "2021-12-20 13:59:07 - \n",
      "\n",
      "2021-12-20 13:59:07 - P@1: 0.7539\n",
      "2021-12-20 13:59:07 - P@3: 0.2513\n",
      "2021-12-20 13:59:07 - P@5: 0.1508\n",
      "2021-12-20 13:59:07 - P@10: 0.0754\n",
      "2021-12-20 13:59:07 - P@100: 0.0075\n",
      "2021-12-20 13:59:07 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb4ab95fda14a1ab70b8dd767ec0aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8841823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:59:50 - Loaded 8841823 DEV Documents.\n",
      "2021-12-20 13:59:51 - Doc Example: {'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.', 'title': ''}\n",
      "2021-12-20 13:59:51 - Loading Queries...\n",
      "2021-12-20 13:59:52 - Loaded 6980 DEV Queries.\n",
      "2021-12-20 13:59:52 - Query Example: how many years did william bradford serve as governor of plymouth colony?\n",
      "2021-12-20 13:59:52 - \n",
      "\n",
      "2021-12-20 13:59:52 - NDCG@1: 0.3888\n",
      "2021-12-20 13:59:52 - NDCG@3: 0.3833\n",
      "2021-12-20 13:59:52 - NDCG@5: 0.3833\n",
      "2021-12-20 13:59:52 - NDCG@10: 0.3833\n",
      "2021-12-20 13:59:52 - NDCG@100: 0.3833\n",
      "2021-12-20 13:59:52 - \n",
      "\n",
      "2021-12-20 13:59:52 - MAP@1: 0.3777\n",
      "2021-12-20 13:59:52 - MAP@3: 0.3817\n",
      "2021-12-20 13:59:52 - MAP@5: 0.3817\n",
      "2021-12-20 13:59:52 - MAP@10: 0.3817\n",
      "2021-12-20 13:59:52 - MAP@100: 0.3817\n",
      "2021-12-20 13:59:52 - \n",
      "\n",
      "2021-12-20 13:59:52 - Recall@1: 0.3777\n",
      "2021-12-20 13:59:52 - Recall@3: 0.3817\n",
      "2021-12-20 13:59:52 - Recall@5: 0.3817\n",
      "2021-12-20 13:59:52 - Recall@10: 0.3817\n",
      "2021-12-20 13:59:52 - Recall@100: 0.3817\n",
      "2021-12-20 13:59:52 - \n",
      "\n",
      "2021-12-20 13:59:52 - P@1: 0.3888\n",
      "2021-12-20 13:59:52 - P@3: 0.1325\n",
      "2021-12-20 13:59:52 - P@5: 0.0795\n",
      "2021-12-20 13:59:52 - P@10: 0.0397\n",
      "2021-12-20 13:59:52 - P@100: 0.0040\n",
      "2021-12-20 13:59:54 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae059c7117cb4c34aea2510e1338d10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:59:55 - Loaded 25657 TEST Documents.\n",
      "2021-12-20 13:59:55 - Doc Example: {'text': 'An evolutionary recurrent network which automates the design of recurrent neural/fuzzy networks using a new evolutionary learning algorithm is proposed in this paper. This new evolutionary learning algorithm is based on a hybrid of genetic algorithm (GA) and particle swarm optimization (PSO), and is thus called HGAPSO. In HGAPSO, individuals in a new generation are created, not only by crossover and mutation operation as in GA, but also by PSO. The concept of elite strategy is adopted in HGAPSO, where the upper-half of the best-performing individuals in a population are regarded as elites. However, instead of being reproduced directly to the next generation, these elites are first enhanced. The group constituted by the elites is regarded as a swarm, and each elite corresponds to a particle within it. In this regard, the elites are enhanced by PSO, an operation which mimics the maturing phenomenon in nature. These enhanced elites constitute half of the population in the new generation, whereas the other half is generated by performing crossover and mutation operation on these enhanced elites. HGAPSO is applied to recurrent neural/fuzzy network design as follows. For recurrent neural network, a fully connected recurrent neural network is designed and applied to a temporal sequence production problem. For recurrent fuzzy network design, a Takagi-Sugeno-Kang-type recurrent fuzzy network is designed and applied to dynamic plant control. The performance of HGAPSO is compared to both GA and PSO in these recurrent networks design problems, demonstrating its superiority.', 'title': 'A hybrid of genetic algorithm and particle swarm optimization for recurrent network design'}\n",
      "2021-12-20 13:59:55 - Loading Queries...\n",
      "2021-12-20 13:59:55 - Loaded 1000 TEST Queries.\n",
      "2021-12-20 13:59:55 - Query Example: A Direct Search Method to solve Economic Dispatch Problem with Valve-Point Effect\n",
      "2021-12-20 13:59:55 - \n",
      "\n",
      "2021-12-20 13:59:55 - NDCG@1: 0.5200\n",
      "2021-12-20 13:59:55 - NDCG@3: 0.3292\n",
      "2021-12-20 13:59:55 - NDCG@5: 0.2446\n",
      "2021-12-20 13:59:55 - NDCG@10: 0.2446\n",
      "2021-12-20 13:59:55 - NDCG@100: 0.2446\n",
      "2021-12-20 13:59:55 - \n",
      "\n",
      "2021-12-20 13:59:55 - MAP@1: 0.1056\n",
      "2021-12-20 13:59:55 - MAP@3: 0.1673\n",
      "2021-12-20 13:59:55 - MAP@5: 0.1737\n",
      "2021-12-20 13:59:55 - MAP@10: 0.1737\n",
      "2021-12-20 13:59:55 - MAP@100: 0.1737\n",
      "2021-12-20 13:59:55 - \n",
      "\n",
      "2021-12-20 13:59:55 - Recall@1: 0.1056\n",
      "2021-12-20 13:59:55 - Recall@3: 0.1673\n",
      "2021-12-20 13:59:55 - Recall@5: 0.1737\n",
      "2021-12-20 13:59:55 - Recall@10: 0.1737\n",
      "2021-12-20 13:59:55 - Recall@100: 0.1737\n",
      "2021-12-20 13:59:55 - \n",
      "\n",
      "2021-12-20 13:59:55 - P@1: 0.5200\n",
      "2021-12-20 13:59:55 - P@3: 0.2750\n",
      "2021-12-20 13:59:55 - P@5: 0.1714\n",
      "2021-12-20 13:59:55 - P@10: 0.0857\n",
      "2021-12-20 13:59:55 - P@100: 0.0086\n",
      "2021-12-20 13:59:55 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c672af37841c4f3eb94f725484aa7e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:59:55 - Loaded 47382 TEST Documents.\n",
      "2021-12-20 13:59:55 - Doc Example: {'text': 'Is there a way to avoid ssh printing warning messages like this?               \"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\\\r\",     \"@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\\\\r\",     \"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\\\r\",      Although the remote host identity has changed but I know it is fine and just want to get rid of this warning.', 'title': 'Force ssh to not to print warnings'}\n",
      "2021-12-20 13:59:55 - Loading Queries...\n",
      "2021-12-20 13:59:59 - Loaded 1072 TEST Queries.\n",
      "2021-12-20 13:59:59 - Query Example: Yanked USB Key During Move\n",
      "2021-12-20 13:59:59 - \n",
      "\n",
      "2021-12-20 13:59:59 - NDCG@1: 0.4366\n",
      "2021-12-20 13:59:59 - NDCG@3: 0.3993\n",
      "2021-12-20 13:59:59 - NDCG@5: 0.3962\n",
      "2021-12-20 13:59:59 - NDCG@10: 0.3957\n",
      "2021-12-20 13:59:59 - NDCG@100: 0.3956\n",
      "2021-12-20 13:59:59 - \n",
      "\n",
      "2021-12-20 13:59:59 - MAP@1: 0.3697\n",
      "2021-12-20 13:59:59 - MAP@3: 0.3841\n",
      "2021-12-20 13:59:59 - MAP@5: 0.3841\n",
      "2021-12-20 13:59:59 - MAP@10: 0.3841\n",
      "2021-12-20 13:59:59 - MAP@100: 0.3841\n",
      "2021-12-20 13:59:59 - \n",
      "\n",
      "2021-12-20 13:59:59 - Recall@1: 0.3697\n",
      "2021-12-20 13:59:59 - Recall@3: 0.3841\n",
      "2021-12-20 13:59:59 - Recall@5: 0.3841\n",
      "2021-12-20 13:59:59 - Recall@10: 0.3841\n",
      "2021-12-20 13:59:59 - Recall@100: 0.3841\n",
      "2021-12-20 13:59:59 - \n",
      "\n",
      "2021-12-20 13:59:59 - P@1: 0.4366\n",
      "2021-12-20 13:59:59 - P@3: 0.1589\n",
      "2021-12-20 13:59:59 - P@5: 0.0953\n",
      "2021-12-20 13:59:59 - P@10: 0.0477\n",
      "2021-12-20 13:59:59 - P@100: 0.0048\n",
      "2021-12-20 13:59:59 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e46cd57e7e54e02b535f58b58151775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:59:59 - Loaded 38316 TEST Documents.\n",
      "2021-12-20 13:59:59 - Doc Example: {'text': \"Let's discuss about $SU(3)$. I understand that the most important representations (relevant to physics) are the defining and the adjoint. In the defining representation of $SU(3)$; namely $\\\\mathbf{3}$, the Gell-Mann matrices are used to represent the generators $$ \\\\left[T^{A}\\\\right]_{ij} = \\\\dfrac{1}{2}\\\\lambda^{A}, $$ where $T^A$ are the generators and $\\\\lambda^A$ the Gell-Mann matrices. In adjoint representation, on the other hand, an $\\\\mathbf{8}$, the generators are represented by matrices according to $$ \\\\left[ T_{i} \\\\right]_{jk} = -if_{ijk}, $$ where $f_{ijk}$ are the structure constants. My question is this, how can one represent the generators in the $\\\\mathbf{10}$ of $SU(3)$, which corresponds to a symmetric tensor with 3 upper or lower indices (or for that matter how to represent the $\\\\mathbf{6}$ with two symmetric indices). What is the general procedure to represent the generators in an arbitrary representation?\", 'title': 'Representation of SU(3) generators'}\n",
      "2021-12-20 13:59:59 - Loading Queries...\n",
      "2021-12-20 14:00:02 - Loaded 1039 TEST Queries.\n",
      "2021-12-20 14:00:02 - Query Example: Magnetic field resistance material: are there any?\n",
      "2021-12-20 14:00:02 - \n",
      "\n",
      "2021-12-20 14:00:02 - NDCG@1: 0.5197\n",
      "2021-12-20 14:00:02 - NDCG@3: 0.4726\n",
      "2021-12-20 14:00:02 - NDCG@5: 0.4658\n",
      "2021-12-20 14:00:02 - NDCG@10: 0.4621\n",
      "2021-12-20 14:00:02 - NDCG@100: 0.4616\n",
      "2021-12-20 14:00:02 - \n",
      "\n",
      "2021-12-20 14:00:02 - MAP@1: 0.4189\n",
      "2021-12-20 14:00:02 - MAP@3: 0.4449\n",
      "2021-12-20 14:00:02 - MAP@5: 0.4456\n",
      "2021-12-20 14:00:02 - MAP@10: 0.4457\n",
      "2021-12-20 14:00:02 - MAP@100: 0.4457\n",
      "2021-12-20 14:00:02 - \n",
      "\n",
      "2021-12-20 14:00:02 - Recall@1: 0.4189\n",
      "2021-12-20 14:00:02 - Recall@3: 0.4449\n",
      "2021-12-20 14:00:02 - Recall@5: 0.4456\n",
      "2021-12-20 14:00:02 - Recall@10: 0.4457\n",
      "2021-12-20 14:00:02 - Recall@100: 0.4457\n",
      "2021-12-20 14:00:02 - \n",
      "\n",
      "2021-12-20 14:00:02 - P@1: 0.5197\n",
      "2021-12-20 14:00:02 - P@3: 0.2011\n",
      "2021-12-20 14:00:02 - P@5: 0.1218\n",
      "2021-12-20 14:00:02 - P@10: 0.0610\n",
      "2021-12-20 14:00:02 - P@100: 0.0061\n",
      "2021-12-20 14:00:02 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d183546ae74c838c1beea351ba13be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:03 - Loaded 48605 TEST Documents.\n",
      "2021-12-20 14:00:03 - Doc Example: {'text': \"In a shortcode context, is there any difference here?               array(             'slideshow' => '',         ),       and               array(             'slideshow' => NULL,         ),       Is there a best practice for that?\", 'title': 'What is the difference between Null vs Empty (Zero Length) string?'}\n",
      "2021-12-20 14:00:03 - Loading Queries...\n",
      "2021-12-20 14:00:04 - Loaded 541 TEST Queries.\n",
      "2021-12-20 14:00:04 - Query Example: How to enqueue script or style in a theme's template file?\n",
      "2021-12-20 14:00:04 - \n",
      "\n",
      "2021-12-20 14:00:04 - NDCG@1: 0.4103\n",
      "2021-12-20 14:00:04 - NDCG@3: 0.3903\n",
      "2021-12-20 14:00:04 - NDCG@5: 0.3886\n",
      "2021-12-20 14:00:04 - NDCG@10: 0.3883\n",
      "2021-12-20 14:00:04 - NDCG@100: 0.3883\n",
      "2021-12-20 14:00:04 - \n",
      "\n",
      "2021-12-20 14:00:04 - MAP@1: 0.3709\n",
      "2021-12-20 14:00:04 - MAP@3: 0.3812\n",
      "2021-12-20 14:00:04 - MAP@5: 0.3816\n",
      "2021-12-20 14:00:04 - MAP@10: 0.3816\n",
      "2021-12-20 14:00:04 - MAP@100: 0.3816\n",
      "2021-12-20 14:00:04 - \n",
      "\n",
      "2021-12-20 14:00:04 - Recall@1: 0.3709\n",
      "2021-12-20 14:00:04 - Recall@3: 0.3812\n",
      "2021-12-20 14:00:04 - Recall@5: 0.3816\n",
      "2021-12-20 14:00:04 - Recall@10: 0.3816\n",
      "2021-12-20 14:00:04 - Recall@100: 0.3816\n",
      "2021-12-20 14:00:04 - \n",
      "\n",
      "2021-12-20 14:00:04 - P@1: 0.4103\n",
      "2021-12-20 14:00:04 - P@3: 0.1466\n",
      "2021-12-20 14:00:04 - P@5: 0.0883\n",
      "2021-12-20 14:00:04 - P@10: 0.0442\n",
      "2021-12-20 14:00:04 - P@100: 0.0044\n",
      "2021-12-20 14:00:04 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a3a3691f4f4a1ba24208f9fe801efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37637 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:04 - Loaded 37637 TEST Documents.\n",
      "2021-12-20 14:00:04 - Doc Example: {'text': \"There is a satellite image it's size is 10 GB and I need to display this image using GeoServer and OpenLayers. When user select the Satellite image in the layer switcher need to display image within 10 seconds. I tried geopdf but the image quality loss isn't acceptable to customer. I want to achieve 10 seconds response time using 32 GB satellite image. Please advice me how to achieve this? Thanks in advance.\", 'title': 'Satellite image display with the help of GeoServer and OpenLayers'}\n",
      "2021-12-20 14:00:04 - Loading Queries...\n",
      "2021-12-20 14:00:07 - Loaded 885 TEST Queries.\n",
      "2021-12-20 14:00:07 - Query Example: Calculating mean upslope aspect from each cell in DEM using Python?\n",
      "2021-12-20 14:00:07 - \n",
      "\n",
      "2021-12-20 14:00:07 - NDCG@1: 0.4316\n",
      "2021-12-20 14:00:07 - NDCG@3: 0.4114\n",
      "2021-12-20 14:00:07 - NDCG@5: 0.4104\n",
      "2021-12-20 14:00:07 - NDCG@10: 0.4101\n",
      "2021-12-20 14:00:07 - NDCG@100: 0.4101\n",
      "2021-12-20 14:00:07 - \n",
      "\n",
      "2021-12-20 14:00:07 - MAP@1: 0.3938\n",
      "2021-12-20 14:00:07 - MAP@3: 0.4036\n",
      "2021-12-20 14:00:07 - MAP@5: 0.4039\n",
      "2021-12-20 14:00:07 - MAP@10: 0.4039\n",
      "2021-12-20 14:00:07 - MAP@100: 0.4039\n",
      "2021-12-20 14:00:07 - \n",
      "\n",
      "2021-12-20 14:00:07 - Recall@1: 0.3938\n",
      "2021-12-20 14:00:07 - Recall@3: 0.4036\n",
      "2021-12-20 14:00:07 - Recall@5: 0.4039\n",
      "2021-12-20 14:00:07 - Recall@10: 0.4039\n",
      "2021-12-20 14:00:07 - Recall@100: 0.4039\n",
      "2021-12-20 14:00:07 - \n",
      "\n",
      "2021-12-20 14:00:07 - P@1: 0.4316\n",
      "2021-12-20 14:00:07 - P@3: 0.1525\n",
      "2021-12-20 14:00:07 - P@5: 0.0920\n",
      "2021-12-20 14:00:07 - P@10: 0.0460\n",
      "2021-12-20 14:00:07 - P@100: 0.0046\n",
      "2021-12-20 14:00:07 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15042f6c222e406c9ed8eac51f608a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:07 - Loaded 45301 TEST Documents.\n",
      "2021-12-20 14:00:07 - Doc Example: {'text': 'What\\'s your Supreme Commander 2 build order. I don\\'t just want \"6 mass extractors, 2 power and a factory\". List of building and units out to the second or third factory, please.', 'title': 'Supreme Commander 2 - Build Orders'}\n",
      "2021-12-20 14:00:07 - Loading Queries...\n",
      "2021-12-20 14:00:13 - Loaded 1595 TEST Queries.\n",
      "2021-12-20 14:00:13 - Query Example: Can the trophy system protect me against bullets?\n",
      "2021-12-20 14:00:13 - \n",
      "\n",
      "2021-12-20 14:00:13 - NDCG@1: 0.6332\n",
      "2021-12-20 14:00:13 - NDCG@3: 0.6022\n",
      "2021-12-20 14:00:13 - NDCG@5: 0.5991\n",
      "2021-12-20 14:00:13 - NDCG@10: 0.5978\n",
      "2021-12-20 14:00:13 - NDCG@100: 0.5970\n",
      "2021-12-20 14:00:13 - \n",
      "\n",
      "2021-12-20 14:00:13 - MAP@1: 0.5486\n",
      "2021-12-20 14:00:13 - MAP@3: 0.5844\n",
      "2021-12-20 14:00:13 - MAP@5: 0.5860\n",
      "2021-12-20 14:00:13 - MAP@10: 0.5864\n",
      "2021-12-20 14:00:13 - MAP@100: 0.5864\n",
      "2021-12-20 14:00:13 - \n",
      "\n",
      "2021-12-20 14:00:13 - Recall@1: 0.5486\n",
      "2021-12-20 14:00:13 - Recall@3: 0.5844\n",
      "2021-12-20 14:00:13 - Recall@5: 0.5860\n",
      "2021-12-20 14:00:13 - Recall@10: 0.5864\n",
      "2021-12-20 14:00:13 - Recall@100: 0.5864\n",
      "2021-12-20 14:00:13 - \n",
      "\n",
      "2021-12-20 14:00:13 - P@1: 0.6332\n",
      "2021-12-20 14:00:13 - P@3: 0.2443\n",
      "2021-12-20 14:00:13 - P@5: 0.1491\n",
      "2021-12-20 14:00:13 - P@10: 0.0750\n",
      "2021-12-20 14:00:13 - P@100: 0.0075\n",
      "2021-12-20 14:00:13 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9545cef444498394c9dc7377c8c3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:13 - Loaded 42269 TEST Documents.\n",
      "2021-12-20 14:00:13 - Doc Example: {'text': \"I'm a beginner in statistics and R, sorry if this question may seem trivial. I've collected data measuring several different parameters in 40 subjects at two time-points (t1 and t2). There are 3 main parameters in which I'm interested, let's call them ParA, ParB, ParC. ParA is a score of disability. It is on an arbitrary scale (so it is an ordinal scale measure, if my understanding is correct) and values range from 0.0 to 10.0. Note that the increments in this scale are by 0.5 unit, so values like, e.g. 1.5 are possible. I have two measures, at t1 and t2, so I can describe at least three variables from ParA: ParA at t1, ParA at t2, and whether a subject progressed or not (0 or 1). Being a ratio scale measure, I think it would not make much sense to compute a difference (eg. ParA at t2 - ParA at t1), but I'm willing to accept suggestions on this matter. ParB and ParC are meausurements of two anatomical structures; they are continuous; Par B is an area measured in mm2, ParC is a volume measured in mm3. I believe they should be considered ratio scale measure. For each one I can describe at least 4 variables: measuments at t1 and t2 (eg. ParB1, ParB2), absolute difference between the two measurements (ParB2-ParB1), percentual difference between the two. What I want to do: I want to do some sort of regression, to see if ParA at t2 is best predicted by B or C (percentual difference, or maybe ParB or ParC at t1, I've yet to decide which makes most sense), but I don't know how to do this. I tried doing some research and concluded that I want to do an ordinal logistic regression, but now I'm unsure how to interpret the results and I'm questioning my choice. Here's some data in case someone wants to reproduce my situation:               require(MASS)          example.df <- data.frame(ParA1=c(1.5,0.0,0.0,1.0,1.5,1.0,1.0,0.0,0.0,0.0,3.0), ParA2 = c(2.5,1.5,1.0,2.0,2.0,1.5,2.0,0.0,0.0,0.0,6.5), Progressed=c(1,1,1,1,1,1,1,0,0,0,1), ParB1=c(222.76,743.07,559.65,642.93,584.36,565.53,590.88,465.31,570.22,543.91,574.80), ParB2=c(214.5,674.71,538.75,560.72,581.9,566.40,499.72, 434.72,528.33,517.61,516.1), ParBAbsolDiff=c(-8.27,-68.36,-20.90,-82.21,-2.46,0.87,-91.16,-30.59,-41.88,-26.31,-58.71), ParBPercentDiff=c(-3.71,-9.20,-3.73,-12.79,-0.42,0.15,-15.43,-6.57,-7.34,-4.84,-10.21), ParC1=c(1585354,1600993,1818728,1595059,1445126,1599984,1454398,1540987,1567783,1559505,1523271), ParC2=c(1578834,1512068,1800791,1514774,1472185,1548337,1440284,1505046,1586734,1622379,1496734), ParCAbsolutDiff=c(-6520.26,-88925.62,-17937.04,-80285.40,27059.77,-51646.81,-14114.52,-35940.91,18951.04,62873.71,-26536.51), ParCPercentDiff=c(-0.41,-5.55,-0.99,-5.03,1.87,-3.23,-0.97,-2.33,1.21,4.03,-1.74))          > myregression <- polr(ParA2 ~ ParBPercentDiff + ParCPercentDiff,data=example.df, Hess=TRUE)     Error in polr(ParA2 ~ ParBPercentDiff + ParCPercentDiff, data = example.df,  :      response must be a factor          > example.df$ParA2 <- factor(example.df$ParA2)     > myregression <- polr(ParA2 ~ ParBPercentDiff + ParCPercentDiff, data=example.df, Hess=TRUE)          > summary(myregression)     Call:     polr(formula = ParA2 ~ ParBPercentDiff + ParCPercentDiff, data = example.df,      Hess = TRUE)          Coefficients:                    Value Std. Error t value     ParBPercentDiff -0.04825     0.1114 -0.4330     ParCPercentDiff -0.13650     0.2079 -0.6566          Intercepts:         Value   Std. Error t value     0|1     -0.4982  0.9546    -0.5219     1|1.5   -0.0267  0.9367    -0.0285     1.5|2    0.7736  0.9874     0.7835     2|2.5    2.1062  1.1628     1.8113     2.5|6.5  2.8957  1.3531     2.1400          Residual Deviance: 35.89846      AIC: 49.89846      > ci <- confint(myregression)     Waiting for profiling to be done...     > exp(cbind(OR= coef(myregression), ci))                        OR     2.5 %   97.5 %     ParBPercentDiff 0.9528960 0.7596362 1.200121     ParCPercentDiff 0.8724038 0.5670611 1.321134      I searched the internet, but I don't understand what I found. There are several questions in stackoverflow, but the answers were too advanced. I'm a medical student, so I have some basics concept in statistics, but this is advanced stuff for my current level of understanding. I'm trying to study statistics in my free time, but at the moment I'm pressed, and I need to understand what this mean; I think it is best if it is explained in simple terms. Getting back to my question... I'm confused by the results I obtained, probably because I'm confused about some underlying concept. What I conclude from looking at summary(myregression) is that ParBPercentDiff is a worse predictor than ParCPercentDiff (a certain increase in ParBPercentDiff gives a decrease of -0.04 in the expected value of ParA2 on the log odds scale - so a decrease of ParBPercentDiff should give an increase of +0.04; these values are higher for ParCPercentDiff); however the OR seems to tell a different story; Like, the odds of ParA2 increasing are greater with an increase in ParBPercentDiff...? My interpretation is surely flawed. I think I could interpret the results more easily if I could plot this regression, but I didn't get what I expected.               myprof <- profile(myregression)     plot(myprof)      So:   * Is ordinal logistic regression really what I want to do in this case?   * If not, what analysis do you suggest?   * If yes, how can I interpret the result I got (please, in simple terms)?\", 'title': 'Is this a case for an ordinal logistic regression? Problems interpreting output'}\n",
      "2021-12-20 14:00:13 - Loading Queries...\n",
      "2021-12-20 14:00:15 - Loaded 652 TEST Queries.\n",
      "2021-12-20 14:00:15 - Query Example: Tool to confirm Gaussian fit\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - NDCG@1: 0.4264\n",
      "2021-12-20 14:00:15 - NDCG@3: 0.3974\n",
      "2021-12-20 14:00:15 - NDCG@5: 0.3950\n",
      "2021-12-20 14:00:15 - NDCG@10: 0.3939\n",
      "2021-12-20 14:00:15 - NDCG@100: 0.3938\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - MAP@1: 0.3761\n",
      "2021-12-20 14:00:15 - MAP@3: 0.3845\n",
      "2021-12-20 14:00:15 - MAP@5: 0.3849\n",
      "2021-12-20 14:00:15 - MAP@10: 0.3849\n",
      "2021-12-20 14:00:15 - MAP@100: 0.3849\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - Recall@1: 0.3761\n",
      "2021-12-20 14:00:15 - Recall@3: 0.3845\n",
      "2021-12-20 14:00:15 - Recall@5: 0.3849\n",
      "2021-12-20 14:00:15 - Recall@10: 0.3849\n",
      "2021-12-20 14:00:15 - Recall@100: 0.3849\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - P@1: 0.4264\n",
      "2021-12-20 14:00:15 - P@3: 0.1508\n",
      "2021-12-20 14:00:15 - P@5: 0.0911\n",
      "2021-12-20 14:00:15 - P@10: 0.0456\n",
      "2021-12-20 14:00:15 - P@100: 0.0046\n",
      "2021-12-20 14:00:15 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c6a4e2f974426bb2bc19b90876cb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:15 - Loaded 17405 TEST Documents.\n",
      "2021-12-20 14:00:15 - Doc Example: {'text': 'I\\'m making a website for a small hotel in php. The hotel owners want a reservation system that uses paypal. They want people to see a calendar and choose a date to make a reservation. If the day has vacancy, they want the user to request booking a room. This would then require the hotel owner to accept the purchase. I have not worked on a project that has this \"request to purchase\" method of buying with paypal. Is this possible? Does anyone know of an open php system that handles this?', 'title': 'Hotel Reservation Request Booking Paypal PHP'}\n",
      "2021-12-20 14:00:15 - Loading Queries...\n",
      "2021-12-20 14:00:15 - Loaded 506 TEST Queries.\n",
      "2021-12-20 14:00:15 - Query Example: Someone else is using our Google Analytics Tracking code number. What do we do?\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - NDCG@1: 0.4644\n",
      "2021-12-20 14:00:15 - NDCG@3: 0.4360\n",
      "2021-12-20 14:00:15 - NDCG@5: 0.4298\n",
      "2021-12-20 14:00:15 - NDCG@10: 0.4244\n",
      "2021-12-20 14:00:15 - NDCG@100: 0.4199\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - MAP@1: 0.3854\n",
      "2021-12-20 14:00:15 - MAP@3: 0.4064\n",
      "2021-12-20 14:00:15 - MAP@5: 0.4082\n",
      "2021-12-20 14:00:15 - MAP@10: 0.4084\n",
      "2021-12-20 14:00:15 - MAP@100: 0.4084\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - Recall@1: 0.3854\n",
      "2021-12-20 14:00:15 - Recall@3: 0.4064\n",
      "2021-12-20 14:00:15 - Recall@5: 0.4082\n",
      "2021-12-20 14:00:15 - Recall@10: 0.4084\n",
      "2021-12-20 14:00:15 - Recall@100: 0.4084\n",
      "2021-12-20 14:00:15 - \n",
      "\n",
      "2021-12-20 14:00:15 - P@1: 0.4644\n",
      "2021-12-20 14:00:15 - P@3: 0.1845\n",
      "2021-12-20 14:00:15 - P@5: 0.1154\n",
      "2021-12-20 14:00:15 - P@10: 0.0583\n",
      "2021-12-20 14:00:15 - P@100: 0.0058\n",
      "2021-12-20 14:00:15 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7257faeaf9f14dc898b2cfaf55c1c56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:16 - Loaded 16705 TEST Documents.\n",
      "2021-12-20 14:00:16 - Doc Example: {'text': \"I'm trying to use `Get` to load some pretty substantial packages from a custom menu in the _Mathematica_ toolbar (added via MenuSetup.tr).   The problem is, the standard 5-second evaluation timeout seems to apply to commands executed with `KernelExecute`, so only a fraction of my `Get` is evaluated before the command times out. I'm wondering whether there's an option that can be passed to `KernelExecute` (or to `Item` / `MenuItem`) that will remove that time constraint so that my command can be executed completely.\", 'title': 'Time constraints on KernelExecute commands or MenuItems?'}\n",
      "2021-12-20 14:00:16 - Loading Queries...\n",
      "2021-12-20 14:00:18 - Loaded 804 TEST Queries.\n",
      "2021-12-20 14:00:18 - Query Example: How to use Automorphisms[] on a graph?\n",
      "2021-12-20 14:00:18 - \n",
      "\n",
      "2021-12-20 14:00:18 - NDCG@1: 0.3769\n",
      "2021-12-20 14:00:18 - NDCG@3: 0.3335\n",
      "2021-12-20 14:00:18 - NDCG@5: 0.3301\n",
      "2021-12-20 14:00:18 - NDCG@10: 0.3286\n",
      "2021-12-20 14:00:18 - NDCG@100: 0.3284\n",
      "2021-12-20 14:00:18 - \n",
      "\n",
      "2021-12-20 14:00:18 - MAP@1: 0.3036\n",
      "2021-12-20 14:00:18 - MAP@3: 0.3158\n",
      "2021-12-20 14:00:18 - MAP@5: 0.3160\n",
      "2021-12-20 14:00:18 - MAP@10: 0.3160\n",
      "2021-12-20 14:00:18 - MAP@100: 0.3160\n",
      "2021-12-20 14:00:18 - \n",
      "\n",
      "2021-12-20 14:00:18 - Recall@1: 0.3036\n",
      "2021-12-20 14:00:18 - Recall@3: 0.3158\n",
      "2021-12-20 14:00:18 - Recall@5: 0.3160\n",
      "2021-12-20 14:00:18 - Recall@10: 0.3160\n",
      "2021-12-20 14:00:18 - Recall@100: 0.3160\n",
      "2021-12-20 14:00:18 - \n",
      "\n",
      "2021-12-20 14:00:18 - P@1: 0.3769\n",
      "2021-12-20 14:00:18 - P@3: 0.1364\n",
      "2021-12-20 14:00:18 - P@5: 0.0823\n",
      "2021-12-20 14:00:18 - P@10: 0.0412\n",
      "2021-12-20 14:00:18 - P@100: 0.0041\n",
      "2021-12-20 14:00:18 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e7ac621caf4a20aae8ac5329df82b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:18 - Loaded 22998 TEST Documents.\n",
      "2021-12-20 14:00:18 - Doc Example: {'text': \"I want to send files to android tablet with a application from PC. - I can send files directly to tablet (2.3 android OS) PC see it as a external usb drive. - But i can't send files to tablet (4.2 android OS), because PC see it as a portable media player.(MTP) - How can i fix this problem ? - How can show my device as a external drive? my application that sent files written via Delphi.\", 'title': 'How can show android tablet as a external storage to PC?'}\n",
      "2021-12-20 14:00:18 - Loading Queries...\n",
      "2021-12-20 14:00:19 - Loaded 699 TEST Queries.\n",
      "2021-12-20 14:00:19 - Query Example: Android chroot ubuntu - is it possible to get ubuntu to recognise usb devices\n",
      "2021-12-20 14:00:19 - \n",
      "\n",
      "2021-12-20 14:00:19 - NDCG@1: 0.5765\n",
      "2021-12-20 14:00:19 - NDCG@3: 0.5343\n",
      "2021-12-20 14:00:19 - NDCG@5: 0.5245\n",
      "2021-12-20 14:00:19 - NDCG@10: 0.5184\n",
      "2021-12-20 14:00:19 - NDCG@100: 0.5155\n",
      "2021-12-20 14:00:19 - \n",
      "\n",
      "2021-12-20 14:00:19 - MAP@1: 0.4575\n",
      "2021-12-20 14:00:19 - MAP@3: 0.4939\n",
      "2021-12-20 14:00:19 - MAP@5: 0.4983\n",
      "2021-12-20 14:00:19 - MAP@10: 0.4989\n",
      "2021-12-20 14:00:19 - MAP@100: 0.4989\n",
      "2021-12-20 14:00:19 - \n",
      "\n",
      "2021-12-20 14:00:19 - Recall@1: 0.4575\n",
      "2021-12-20 14:00:19 - Recall@3: 0.4939\n",
      "2021-12-20 14:00:19 - Recall@5: 0.4983\n",
      "2021-12-20 14:00:19 - Recall@10: 0.4989\n",
      "2021-12-20 14:00:19 - Recall@100: 0.4989\n",
      "2021-12-20 14:00:19 - \n",
      "\n",
      "2021-12-20 14:00:19 - P@1: 0.5765\n",
      "2021-12-20 14:00:19 - P@3: 0.2380\n",
      "2021-12-20 14:00:19 - P@5: 0.1491\n",
      "2021-12-20 14:00:19 - P@10: 0.0754\n",
      "2021-12-20 14:00:19 - P@100: 0.0075\n",
      "2021-12-20 14:00:19 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c37a89a13674fe4a0cd3498b14e7eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:20 - Loaded 32176 TEST Documents.\n",
      "2021-12-20 14:00:20 - Doc Example: {'text': \"I am in the midst of writing a web application for work. Everything is from scratch. I have been a PHP programmer for about 13 years, Node.js programmer for the past 2 years, and have no shortage of experience with JavaScript. I love Node.js, and recently rebuilt the company's API in it... So, in planning this web application, the approach I'm considering is, have the Node.js API for getting data from the server, but render everything in the browser. Use AJAX for retrieving data, History API for loading pages, and a MVC-like pattern for the different components. I have read articles detailing twitters rebuild a few years ago. It was more or less a client-side JavaScript app, but a couple years after launching it, they started moving a lot of processing/rendering back to the server, claiming the app improved dramatically in terms of speed. So, my question is as the title asks, is a client-side centric app substantially slower?\", 'title': 'Are (mostly) client-side JavaScript web apps slower or less efficient?'}\n",
      "2021-12-20 14:00:20 - Loading Queries...\n",
      "2021-12-20 14:00:22 - Loaded 876 TEST Queries.\n",
      "2021-12-20 14:00:22 - Query Example: Why is closure important for JavaScript?\n",
      "2021-12-20 14:00:22 - \n",
      "\n",
      "2021-12-20 14:00:22 - NDCG@1: 0.4772\n",
      "2021-12-20 14:00:22 - NDCG@3: 0.4314\n",
      "2021-12-20 14:00:22 - NDCG@5: 0.4244\n",
      "2021-12-20 14:00:22 - NDCG@10: 0.4218\n",
      "2021-12-20 14:00:22 - NDCG@100: 0.4211\n",
      "2021-12-20 14:00:22 - \n",
      "\n",
      "2021-12-20 14:00:22 - MAP@1: 0.3766\n",
      "2021-12-20 14:00:22 - MAP@3: 0.4051\n",
      "2021-12-20 14:00:22 - MAP@5: 0.4059\n",
      "2021-12-20 14:00:22 - MAP@10: 0.4059\n",
      "2021-12-20 14:00:22 - MAP@100: 0.4059\n",
      "2021-12-20 14:00:22 - \n",
      "\n",
      "2021-12-20 14:00:22 - Recall@1: 0.3766\n",
      "2021-12-20 14:00:22 - Recall@3: 0.4051\n",
      "2021-12-20 14:00:22 - Recall@5: 0.4059\n",
      "2021-12-20 14:00:22 - Recall@10: 0.4059\n",
      "2021-12-20 14:00:22 - Recall@100: 0.4059\n",
      "2021-12-20 14:00:22 - \n",
      "\n",
      "2021-12-20 14:00:22 - P@1: 0.4772\n",
      "2021-12-20 14:00:22 - P@3: 0.1880\n",
      "2021-12-20 14:00:22 - P@5: 0.1135\n",
      "2021-12-20 14:00:22 - P@10: 0.0567\n",
      "2021-12-20 14:00:22 - P@100: 0.0057\n",
      "2021-12-20 14:00:22 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178eb9c035ff49fe899a113179bf8ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:23 - Loaded 40221 TEST Documents.\n",
      "2021-12-20 14:00:23 - Doc Example: {'text': 'An eponym is one way to eternal (if posthumous) fame. But is there a word meaning an eponym someone would sooner not have? (One would presume that Captain Charles _Boycott_ , Mr Justice _Lynch_ , and Patrick _Hooligan_ would not appreciate their undying notoriety.)', 'title': 'Is there a word meaning \"an unwanted eponym\"?'}\n",
      "2021-12-20 14:00:23 - Loading Queries...\n",
      "2021-12-20 14:00:30 - Loaded 1570 TEST Queries.\n",
      "2021-12-20 14:00:30 - Query Example: Is \"a wide range of features\" singular or plural?\n",
      "2021-12-20 14:00:30 - \n",
      "\n",
      "2021-12-20 14:00:30 - NDCG@1: 0.5051\n",
      "2021-12-20 14:00:30 - NDCG@3: 0.4613\n",
      "2021-12-20 14:00:30 - NDCG@5: 0.4531\n",
      "2021-12-20 14:00:30 - NDCG@10: 0.4482\n",
      "2021-12-20 14:00:30 - NDCG@100: 0.4452\n",
      "2021-12-20 14:00:30 - \n",
      "\n",
      "2021-12-20 14:00:30 - MAP@1: 0.3946\n",
      "2021-12-20 14:00:30 - MAP@3: 0.4277\n",
      "2021-12-20 14:00:30 - MAP@5: 0.4295\n",
      "2021-12-20 14:00:30 - MAP@10: 0.4299\n",
      "2021-12-20 14:00:30 - MAP@100: 0.4299\n",
      "2021-12-20 14:00:30 - \n",
      "\n",
      "2021-12-20 14:00:30 - Recall@1: 0.3946\n",
      "2021-12-20 14:00:30 - Recall@3: 0.4277\n",
      "2021-12-20 14:00:30 - Recall@5: 0.4295\n",
      "2021-12-20 14:00:30 - Recall@10: 0.4299\n",
      "2021-12-20 14:00:30 - Recall@100: 0.4299\n",
      "2021-12-20 14:00:30 - \n",
      "\n",
      "2021-12-20 14:00:30 - P@1: 0.5051\n",
      "2021-12-20 14:00:30 - P@3: 0.2064\n",
      "2021-12-20 14:00:30 - P@5: 0.1270\n",
      "2021-12-20 14:00:30 - P@10: 0.0643\n",
      "2021-12-20 14:00:30 - P@100: 0.0064\n",
      "2021-12-20 14:00:30 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547668d19f1443c6a06475ccc9372e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:00:31 - Loaded 68184 TEST Documents.\n",
      "2021-12-20 14:00:31 - Doc Example: {'text': \"I am using a pgfplots stacked bar to display the aggregated energy demand of a houshold and the associated price. When the energy demand exceeds a certain threshold, than a higher price has to be paid. This is visualized by the color red and blue of the bars. The threshold is displayed by the thick red horizontal line. My problem is, that I want this red line to exceed the width of the bar, so that it's width is circa 120 percent of the width of the bar. Is there any possibility to achieve this? Thanks ![enter image description here](http://i.stack.imgur.com/3qeEi.jpg)               \\\\documentclass[tikz]{standalone}     \\\\usepackage{pgfplots}     \\\\pgfplotsset{compat=1.10}     \\\\begin{document}     \\\\begin{tikzpicture}     \\\\begin{axis}[       ymin=0,ymax=4,       samples=3,       enlarge x limits={abs=0.5},       bar width=0.6,       ybar stacked,       legend pos=south east,         every axis/.append style={font=\\\\footnotesize},     ]          \\\\draw[red, very thick] (axis cs:0.7,2) -- (axis cs:1.3,2);     \\\\draw[red, very thick] (axis cs:1.7,2.5) -- (axis cs:2.3,2.5);     \\\\draw[red, very thick] (axis cs:2.7,2.5) -- (axis cs:3.3,2.5);          \\\\addplot     coordinates          {(1,1) (2,2.5) (3,1.5)};          \\\\addplot     coordinates          {(1,0) (2,1) (3,0)};               \\\\legend{low price, high price}     \\\\end{axis}     \\\\end{tikzpicture}     \\\\end{document}\", 'title': 'Adding horizontal lines to pgfplots bar'}\n",
      "2021-12-20 14:00:31 - Loading Queries...\n",
      "2021-12-20 14:01:21 - Loaded 2906 TEST Queries.\n",
      "2021-12-20 14:01:21 - Query Example: How can I learn to make my own packages?\n",
      "2021-12-20 14:01:21 - \n",
      "\n",
      "2021-12-20 14:01:21 - NDCG@1: 0.3648\n",
      "2021-12-20 14:01:21 - NDCG@3: 0.3331\n",
      "2021-12-20 14:01:21 - NDCG@5: 0.3292\n",
      "2021-12-20 14:01:21 - NDCG@10: 0.3276\n",
      "2021-12-20 14:01:21 - NDCG@100: 0.3266\n",
      "2021-12-20 14:01:21 - \n",
      "\n",
      "2021-12-20 14:01:21 - MAP@1: 0.3011\n",
      "2021-12-20 14:01:21 - MAP@3: 0.3162\n",
      "2021-12-20 14:01:21 - MAP@5: 0.3166\n",
      "2021-12-20 14:01:21 - MAP@10: 0.3167\n",
      "2021-12-20 14:01:21 - MAP@100: 0.3167\n",
      "2021-12-20 14:01:21 - \n",
      "\n",
      "2021-12-20 14:01:21 - Recall@1: 0.3011\n",
      "2021-12-20 14:01:21 - Recall@3: 0.3162\n",
      "2021-12-20 14:01:21 - Recall@5: 0.3166\n",
      "2021-12-20 14:01:21 - Recall@10: 0.3167\n",
      "2021-12-20 14:01:21 - Recall@100: 0.3167\n",
      "2021-12-20 14:01:21 - \n",
      "\n",
      "2021-12-20 14:01:21 - P@1: 0.3648\n",
      "2021-12-20 14:01:21 - P@3: 0.1367\n",
      "2021-12-20 14:01:21 - P@5: 0.0829\n",
      "2021-12-20 14:01:21 - P@10: 0.0416\n",
      "2021-12-20 14:01:21 - P@100: 0.0042\n",
      "2021-12-20 14:01:21 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c10d4eddc614f79bdc3ff41f1f224d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2866316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:01:32 - Loaded 2866316 TEST Documents.\n",
      "2021-12-20 14:01:32 - Doc Example: {'text': 'This Boston college professor who lives in #NH is on leave after being arrested for child pornography, endangerment:', 'title': ''}\n",
      "2021-12-20 14:01:32 - Loading Queries...\n",
      "2021-12-20 14:01:32 - Loaded 97 TEST Queries.\n",
      "2021-12-20 14:01:32 - Query Example: VIDEO:Good Samaritans Stop Alleged Hit-and-Run Driver in Miami\n",
      "2021-12-20 14:01:32 - \n",
      "\n",
      "2021-12-20 14:01:32 - NDCG@1: 0.6804\n",
      "2021-12-20 14:01:32 - NDCG@3: 0.5734\n",
      "2021-12-20 14:01:32 - NDCG@5: 0.5029\n",
      "2021-12-20 14:01:32 - NDCG@10: 0.3900\n",
      "2021-12-20 14:01:32 - NDCG@100: 0.2722\n",
      "2021-12-20 14:01:32 - \n",
      "\n",
      "2021-12-20 14:01:32 - MAP@1: 0.0523\n",
      "2021-12-20 14:01:32 - MAP@3: 0.1099\n",
      "2021-12-20 14:01:32 - MAP@5: 0.1364\n",
      "2021-12-20 14:01:32 - MAP@10: 0.1607\n",
      "2021-12-20 14:01:32 - MAP@100: 0.1607\n",
      "2021-12-20 14:01:32 - \n",
      "\n",
      "2021-12-20 14:01:32 - Recall@1: 0.0523\n",
      "2021-12-20 14:01:32 - Recall@3: 0.1099\n",
      "2021-12-20 14:01:32 - Recall@5: 0.1364\n",
      "2021-12-20 14:01:32 - Recall@10: 0.1607\n",
      "2021-12-20 14:01:32 - Recall@100: 0.1607\n",
      "2021-12-20 14:01:32 - \n",
      "\n",
      "2021-12-20 14:01:32 - P@1: 0.7732\n",
      "2021-12-20 14:01:32 - P@3: 0.6117\n",
      "2021-12-20 14:01:32 - P@5: 0.4990\n",
      "2021-12-20 14:01:32 - P@10: 0.3083\n",
      "2021-12-20 14:01:32 - P@100: 0.0308\n",
      "2021-12-20 14:01:33 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf98bed06a864dca806a2498ea03c1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594977 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:01:48 - Loaded 594977 TEST Documents.\n",
      "2021-12-20 14:01:48 - Doc Example: {'text': 'NEW ORLEANS — Whenever a Virginia Tech offensive coach is asked how the most prolific receiving duo in school history came to be, inevitably the first road game in 2008 against North Carolina comes up. Midway through the first quarter, Virginia Tech had to call two timeouts in a row because then-freshmen Jarrett Boykin and Danny Coale couldn’t seem to line up right, and “they had those big eyes out there looking around,” Kevin Sherman, their position coach, said recently. Now that Boykin and Coale have only Tuesday’s Sugar Bowl remaining before leaving Virginia Tech with every major school record for a wide receiver, they’ve taken a different stance. “I still don’t think that was on us. Macho [Harris] was in the game and he lined up wrong,” said Boykin, as Coale sat next to him nodding in agreement. Just add that to the list of slights these seniors have had to overcome. Boykin has been the team’s leading receiver the past three seasons, using hands that need size XXXL gloves and a knack for out-maneuvering opposing cornerbacks in the air to set a single-season school record for receptions this year (57). He will end his career with more catches (180) and yards (2,854) than any other Hokies receiver. Coale, an Episcopal High graduate, leads Virginia Tech with 785 receiving yards this year. He is right behind Boykin in the school record books and became the team’s starting punter by the end of this season. Coach Frank Beamer has frequently marveled how “Danny just always seems to be open.” And yet neither warranted even honorable mention all-ACC status this year, a snub that quarterback Logan Thomas said made him “extremely upset” and left Beamer wondering about the media members who participated in the voting. In retrospect, Boykin said he recognizes the lack of notoriety is partly due to Virginia Tech’s offensive philosophy. The Hokies have always been known for their rushing attack, and this year was no different. Running back David Wilson earned ACC player of the year honors during a year when Thomas set multiple records for a first-year quarterback. “There’s just some things that we were held back from being able to show,” Boykin said, “that we’re just as good as [South Carolina wide receiver] Alshon Jeffrey and [Oklahoma State wide receiver] Justin Blackmon. I feel like they’re great athletes, but at the same time we’re right up there with them. “It’s great playing wide receiver here because once we throw the ball, you have opportunities to get big chunks of yardage. What we can’t do is we’re not going to catch 100 balls for 1,500 yards and 22 touchdowns.” The other issue is that neither has the sort of attention-grabbing personality or pedigree associated with big-time wide receivers these days. Coale has graduated with a degree in finance and was named the ACC’s top scholar-athlete this year. He speaks in measured tones reminiscent of a CEO and has yet to join Facebook or Twitter. Boykin is so quiet around the team facility that Beamer said he sometimes doesn’t notice him until he’s making catches on the practice field or in games. Coming out of high school, Coale was barely recruited. Before showing up to a camp in Blacksburg one summer, his only scholarship offer was from VMI, where his father is the head of strength and conditioning. Coale still jokes that when he spent his redshirt year (2007) on the scout team, former Virginia Tech wide receivers and future NFL wideouts Eddie Royal, Andre Davis and Josh Morgan “must have thought I was a walk-on. I prefer to just fly under the radar.” But their accomplishments haven’t gone unnoticed now that the clock is ticking on their careers. Quarterbacks coach Mike O’Cain said Thomas’s comfort level during his record-setting first year under center is a direct reflection of Boykin and Coale. “Not only are they gonna run the right route with the right timing, you know they’re gonna catch the ball,” he said. Years of lining up together has also created a special bond between the two, and it played out before the ACC championship game this year. Boykin was supposed to deliver the pregame speech, but always reticent about public speaking, he was afraid he might stutter and not be taken seriously. He asked Coale to take his place. “I’ve been through his struggles, he’s been through mine,” Coale said. “He’s a guy that I know I can count on, whether it’s five years from now, I just know I can count on him and he’ll be there. I know when I look back, part of my Tech experience is going to be him.”', 'title': 'Danny Coale, Jarrett Boykin are a perfect 1-2 punch for Virginia Tech'}\n",
      "2021-12-20 14:01:48 - Loading Queries...\n",
      "2021-12-20 14:01:48 - Loaded 57 TEST Queries.\n",
      "2021-12-20 14:01:48 - Query Example: Baseball’s minor leaguers pursue their dreams below the poverty line\n",
      "2021-12-20 14:01:48 - \n",
      "\n",
      "2021-12-20 14:01:48 - NDCG@1: 0.7734\n",
      "2021-12-20 14:01:48 - NDCG@3: 0.6894\n",
      "2021-12-20 14:01:48 - NDCG@5: 0.6277\n",
      "2021-12-20 14:01:48 - NDCG@10: 0.4922\n",
      "2021-12-20 14:01:48 - NDCG@100: 0.2871\n",
      "2021-12-20 14:01:48 - \n",
      "\n",
      "2021-12-20 14:01:48 - MAP@1: 0.0280\n",
      "2021-12-20 14:01:48 - MAP@3: 0.0774\n",
      "2021-12-20 14:01:48 - MAP@5: 0.1110\n",
      "2021-12-20 14:01:48 - MAP@10: 0.1424\n",
      "2021-12-20 14:01:48 - MAP@100: 0.1424\n",
      "2021-12-20 14:01:48 - \n",
      "\n",
      "2021-12-20 14:01:48 - Recall@1: 0.0280\n",
      "2021-12-20 14:01:48 - Recall@3: 0.0774\n",
      "2021-12-20 14:01:48 - Recall@5: 0.1110\n",
      "2021-12-20 14:01:48 - Recall@10: 0.1424\n",
      "2021-12-20 14:01:48 - Recall@100: 0.1424\n",
      "2021-12-20 14:01:48 - \n",
      "\n",
      "2021-12-20 14:01:48 - P@1: 0.8772\n",
      "2021-12-20 14:01:48 - P@3: 0.8246\n",
      "2021-12-20 14:01:48 - P@5: 0.7439\n",
      "2021-12-20 14:01:48 - P@10: 0.4965\n",
      "2021-12-20 14:01:48 - P@100: 0.0496\n",
      "2021-12-20 14:01:48 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1b73a638774cdf89e81e57eabe517d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14914714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:03:55 - Loaded 14914604 TEST Documents.\n",
      "2021-12-20 14:03:56 - Doc Example: {'text': 'Depressive disorder is one of the most widespread forms of mental disorders which lead to a significant public health concern, such as disability, suicide, and so on. Its etiology remains vague but it is believed that depressive disorder is a multifactorial disease which is induced by the interaction of social, psychological, and biological factors. Thus, there is no clear and definite pathological theory could illustrate its mechanism independently until now, involving genetics, neuroimaging, neuroinflammation, neuroendocrine, and others. Comprehensive assessment to patients with depression is the starting point for a right diagnosis. History-taking of physical condition is as important as psychiatric interview and rational usage of scales would be beneficial for screening. There are many kinds of therapeutic measures for depressive patients nowadays, including general intervention, pharmacotherapy, psychotherapy, and physical therapy. For now, anti-depressants used in clinical practice is almost monoamine-based drugs while much more progress have been made in developing new antidepressant medications, like prototypical N-methyl-D-aspartate (NMDA) receptor antagonists, opioid agonists, gamma-aminobutyric acid (GABAA) receptors, and psychedelics. Once these novel drugs are proved to be practicable, it will create a historical evolution in the field of psychiatry. In addition, we advocate that measurement-based care (MBC) should run through the whole duration of treatment and goals of MBC in every stage are different. As brain projects in many countries are conducting in inspiring ways, we believe that our understanding about depressive disorder, of course, and other neuropsychiatric disorders will be better in the future.', 'title': 'Introduction'}\n",
      "2021-12-20 14:03:56 - Loading Queries...\n",
      "2021-12-20 14:03:56 - Loaded 500 TEST Queries.\n",
      "2021-12-20 14:03:56 - Query Example: Is poliosis circumscripta another term for a white or unpigmented patch of hair or skin?\n",
      "2021-12-20 14:03:56 - \n",
      "\n",
      "2021-12-20 14:03:56 - NDCG@1: 0.7700\n",
      "2021-12-20 14:03:56 - NDCG@3: 0.6834\n",
      "2021-12-20 14:03:56 - NDCG@5: 0.6370\n",
      "2021-12-20 14:03:56 - NDCG@10: 0.5875\n",
      "2021-12-20 14:03:56 - NDCG@100: 0.5728\n",
      "2021-12-20 14:03:56 - \n",
      "\n",
      "2021-12-20 14:03:56 - MAP@1: 0.3553\n",
      "2021-12-20 14:03:56 - MAP@3: 0.4678\n",
      "2021-12-20 14:03:56 - MAP@5: 0.4998\n",
      "2021-12-20 14:03:56 - MAP@10: 0.5127\n",
      "2021-12-20 14:03:56 - MAP@100: 0.5127\n",
      "2021-12-20 14:03:56 - \n",
      "\n",
      "2021-12-20 14:03:56 - Recall@1: 0.3553\n",
      "2021-12-20 14:03:56 - Recall@3: 0.4678\n",
      "2021-12-20 14:03:56 - Recall@5: 0.4998\n",
      "2021-12-20 14:03:56 - Recall@10: 0.5127\n",
      "2021-12-20 14:03:56 - Recall@100: 0.5127\n",
      "2021-12-20 14:03:56 - \n",
      "\n",
      "2021-12-20 14:03:56 - P@1: 0.7700\n",
      "2021-12-20 14:03:56 - P@3: 0.4780\n",
      "2021-12-20 14:03:56 - P@5: 0.3452\n",
      "2021-12-20 14:03:56 - P@10: 0.1894\n",
      "2021-12-20 14:03:56 - P@100: 0.0189\n",
      "2021-12-20 14:04:01 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5329b23bc6e547d1860f9752c533f1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 14:04:10 - Loaded 528155 TEST Documents.\n",
      "2021-12-20 14:04:11 - Doc Example: {'text': '\\n\\nPOLITICIANS,  PARTY PREFERENCES \\n\\n   Summary:  Newspapers in the Former Yugoslav Republic of \\n   Macedonia have published the results of opinion polls, \\n   indicating the relative popularity of politicians, \\n   political parties, and attitudes toward the political system. \\n\\n   The 22-23 January edition of the Skopje newspaper VECER in \\nMacedonian published on pages 6-7 the results of an opinion poll \\nconducted by the \"BriMa\" agency in November 1993. According to \\nVECER, 1,036 respondents were classified by age and residence, but \\nthe paper did not explain the methodology or give the margin of \\nerror.  For the purpose of comparison, the paper cited the results \\nof an unidentified poll made in May 1993. The approval/disapproval \\nratings, in percent, for ten Macedonian politicians were: \\n\\n                                           November 1993    May 1993 \\n\\nKiro Gligorov, President of the Republic      76/15           78/13 \\n\\nVasil Tupurkovski, former Macedonian          50/36           43/37 \\n   official in Federal Yugoslavia \\n\\nLjubomir Frckovski, Interior Minister         50/42           42/43 \\n\\nStojan Andov, Parliamentary Chairman          48/41           48/39 \\n\\nBranko Crvenkovski, Prime Minister            46/41           44/38 \\n\\nVlado Popovski, Defense Minister              41/41           36/37 \\n\\nStevo Crvenkovski, Foreign Minister           40/43   No Data Given \\n\\nPetar Gosev, Democratic Party leader          34/53           40/42 \\n\\nTodor Petrov, Independent parliamentarian     32/53   No Data Given \\n\\nNikola Popovski, Social Democratic            29/46           32/42 \\n   Party parliamentarian \\n\\n   VECER noted that President Gligorov\\'s very high approval rating \\nof 90 percent among those over age 65 fell off to a still high 70 \\npercent among respondents between 18 and 24.  Residents of Skopje \\nranked the politicians in a slightly different order from the \\nranking given by the whole sample: Gligorov, Tupurkovski, Frckovski, \\nAndov, Gosev, Branko Crvenkovski, Vlado Popovski, Petrov, Nikola \\nPopovski, and Stevo Crvenkovski. \\n\\n   The results of a series of opinion polls conducted by the Agency \\nfor Public Opinion Research and published \"exclusively\" by the \\nSkopje weekly PULS newspaper, confirmed Gligorov\\'s substantial lead \\nin popularity among political figures.  According to the 31 December \\n1993 issue of PULS (pages 16-18), the agency gathered the data by \\nmeans of telephone interviews with 300 residents in the Republic of \\nMacedonia during 20-24 December. PULS also provided data from \\nsurveys made in March, June, and September for comparison.  Some of \\nthe following percentages are approximate values that were derived \\nfrom the graph published by the paper: \\n\\n                         March       June      September    December \\n\\nKiro Gligorov             87          82.33      89.33           89 \\nStevo Crvenkovski         54          65         49              63 \\nStojan Andov              61          62         60              61 \\nBranko Crvenkovski        56          60         54 \\n53.5 \\nLjubomir Frckovski        35          45         48              50 \\nPetar Gosev               50          31         52 \\n49.53 \\nJovan Andonov, \\n Deputy Prime Minister    39          39         50              37 \\nVlado Popovski            18          25         36              35 \\nKiro Popovski, Deputy \\n Chairman, Parliament     26          27         33              32 \\nAnte Popovski, leader of \\n MAAK (Movement for All- \\n Macedonian Action)       29          32         32 \\nindistinct \\nJane Miljovski, Minister \\n without Portfolio        --          23         31              24 \\nVladimir Golubovski \\n VMRO-DP (Internal \\n Macedonian Revolutionary \\n Organization-Democratic \\n Party) leader            --          30         25              23 \\nNevzat Halili \\n Party for Democratic \\n Prosperity official      38.33       38         18              18 \\n\\nLj upco Georgievski \\nVMRO-DPMNE (Internal \\nMacedonian Revolutionary \\nOrganization-Democratic \\nParty for Macedonian \\nNational Unity) \\nofficial                  18          10         16              17 \\nDosta Dimovska \\nVMRO-DPMNE \\nofficial                  --          11         17              16 \\n\\n   On pages 6 and 7 of its 15-16 January issue, VECER also published \\nthe results of a November 1993 survey on party preferences. \\n\"BriMa,\" working with the Gallup organization, interviewed 1,036 \\npeople. \\n\\n   Question: \"If elections were held today, for which party would \\nyou vote?\" (all numbers are percentages) \\n\\nSDSM (Social Democratic Alliance of Macedonia)  22.8 \\nVMRO-DPMNE                                      11.2 \\nDemocratic Party (DP, led by Petar Gosev)        6.3 \\nSocialist Party                                  3.3 \\nLiberal Party (LP)                               3.2 \\nWorkers Party                                    2.9 \\nPCERM (Party for the Full Emancipation of \\n    Romanies in Macedonia)                       1.8 \\nDemocratic Party of Turks in Macedonia           0.8 \\nMAAK                                             0.3 \\nAnother party                                    4.0 \\nUndecided                                       18.6 \\nWould not vote                                   6.6 \\n\\n   VECER noted that some parties fared better in certain cities than \\ntheir overall scores indicate.  For example, the DP was about twice \\nas popular in Skopje as elsewhere, getting 12.1 percent in the \\ncapital; the VMRO-DPMNE was more popular in Bitola, getting 15.7 \\npercent, than in the remainder of the country; and the LP in the \\nBregalnica area got the support of 10.6 percent, substantially \\nhigher than the 3.2 percent support it received overall. \\n\\n   Question: \"Do you have confidence in the following parties?\" (all \\nnumbers are percentages) \\n\\n              Yes           No       Do Not Know \\n\\nSDSM           28           51          21 \\nVMRO-DPMNE     15           72          14 \\nLP             19           59          22 \\nPDP-NDP*       20           73           7 \\n\\n*Party for Democratic Prosperity-People\\'s Democratic Party \\n\\n   The poll clearly indicated that Macedonians have little \\nconfidence in any of the parties currently active in the country. \\nRespondents were also asked whether it would be good for the country \\nto have elections sooner than scheduled; 62 percent agreed, 20 \\npercent disagreed, and 18 percent did not know. These findings were \\ncorrelated with party preferences, producing the following results: \\nOf those who would vote for the SDSM, 54 percent wanted elections \\nsoon, while 34 percent were against early elections. However, 80 \\npercent of VMRO-DPMNE supporters favored elections soon, as did 79 \\npercent of LP supporters and 71 percent of DP supporters. While 80 \\npercent of those surveyed thought that a person should vote (14 \\npercent did not agree), only 40 percent thought that it was very \\nimportant which party won the elections and 27 percent thought it \\nwas somewhat significant. \\n\\n   (AUTHOR:  GALYEAN.  QUESTIONS AND/OR COMMENTS, PLEASE CALL CHIEF, \\nBALKANS BRANCH AT (703) 733-6481) \\n\\nELAG/25 February/POLCHF/EED/DEW 28/2023Z FEB \\n\\n\\n', 'title': None}\n",
      "2021-12-20 14:04:11 - Loading Queries...\n",
      "2021-12-20 14:04:11 - Loaded 249 TEST Queries.\n",
      "2021-12-20 14:04:11 - Query Example: Identify organizations that participate in international criminal\n",
      "activity, the activity, and, if possible, collaborating organizations\n",
      "and the countries involved.\n",
      "2021-12-20 14:04:11 - \n",
      "\n",
      "2021-12-20 14:04:11 - NDCG@1: 0.8594\n",
      "2021-12-20 14:04:11 - NDCG@3: 0.7539\n",
      "2021-12-20 14:04:11 - NDCG@5: 0.6676\n",
      "2021-12-20 14:04:11 - NDCG@10: 0.5080\n",
      "2021-12-20 14:04:11 - NDCG@100: 0.2561\n",
      "2021-12-20 14:04:11 - \n",
      "\n",
      "2021-12-20 14:04:11 - MAP@1: 0.0355\n",
      "2021-12-20 14:04:11 - MAP@3: 0.0889\n",
      "2021-12-20 14:04:11 - MAP@5: 0.1212\n",
      "2021-12-20 14:04:11 - MAP@10: 0.1418\n",
      "2021-12-20 14:04:11 - MAP@100: 0.1418\n",
      "2021-12-20 14:04:11 - \n",
      "\n",
      "2021-12-20 14:04:11 - Recall@1: 0.0355\n",
      "2021-12-20 14:04:11 - Recall@3: 0.0889\n",
      "2021-12-20 14:04:11 - Recall@5: 0.1212\n",
      "2021-12-20 14:04:11 - Recall@10: 0.1418\n",
      "2021-12-20 14:04:11 - Recall@100: 0.1418\n",
      "2021-12-20 14:04:11 - \n",
      "\n",
      "2021-12-20 14:04:11 - P@1: 0.8875\n",
      "2021-12-20 14:04:11 - P@3: 0.7644\n",
      "2021-12-20 14:04:11 - P@5: 0.6474\n",
      "2021-12-20 14:04:11 - P@10: 0.4024\n",
      "2021-12-20 14:04:11 - P@100: 0.0402\n",
      "trec-covid  & 0.7496\n",
      "webis-touche2020  & 0.46662\n",
      "nfcorpus  & 0.36369\n",
      "scifact  & 0.72865\n",
      "fiqa  & 0.36317\n",
      "dbpedia-entity  & 0.39739\n",
      "nq  & 0.51427\n",
      "hotpotqa  & 0.68963\n",
      "quora  & 0.9138\n",
      "fever  & 0.82428\n",
      "climate-fever  & 0.28051\n",
      "arguana  & 0.75391\n",
      "msmarco  & 0.38326\n",
      "scidocs  & 0.24462\n",
      "cqadupstack  & 0.42640083333333334\n",
      "signal1m  & 0.38996\n",
      "trec-news  & 0.49224\n",
      "bioasq  & 0.58747\n",
      "robust04  & 0.50798\n",
      ": 0.53039\n"
     ]
    }
   ],
   "source": [
    "# Compute scores based on results.json\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "\n",
    "def compute_result(results_path, data_path, top_k=100, k_values=[1, 3, 5, 10, 100]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_path: Path to .json to read rerank results\n",
    "        top_k: How many docs to rerank per query\n",
    "        k_values: For how many docs per query to compute the scores\n",
    "    \"\"\"\n",
    "    split = \"dev\" if \"msmarco\" in data_path else \"test\"\n",
    "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=split)\n",
    "    \n",
    "    with open(results_path, 'r') as fp:\n",
    "        results = json.load(fp)\n",
    "    \n",
    "    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, results, k_values)\n",
    "\n",
    "    return (ndcg, _map, recall, precision)\n",
    "\n",
    "\n",
    "# All datasets\n",
    "datasets = [\"trec-covid\", \"webis-touche2020\", \"nfcorpus\", \"scifact\", \"fiqa\", \"dbpedia-entity\",\n",
    "            \"nq\", \"hotpotqa\", \"quora\", \"fever\", \"climate-fever\", \"arguana\", \"msmarco\", \"scidocs\", \"cqadupstack\",\n",
    "            \"signal1m\", \"trec-news\", \"bioasq\", \"robust04\"]\n",
    "\n",
    "prompts = {\n",
    "    \"\": '',\n",
    "}\n",
    "\n",
    "latex_help = {}\n",
    "latex_help_avgs = {}\n",
    "results_prefix = \"beirbm25perfectrerankresults/\"\n",
    "model_name = \"perfect_rerank\"\n",
    "# Make empty string if no maxrerank in title (if none in title  = 100)\n",
    "maxrerank = \"_10\"\n",
    "\n",
    "for prompt_id, prompt_doc in prompts.items():\n",
    "    \n",
    "    scores_out_path = f\"{results_prefix}beir_{model_name}{prompt_id}{maxrerank}_ndcgs.json\"\n",
    "    ndcgs = {}\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        \n",
    "        data_path = f\"datasets/{dataset}\"\n",
    "\n",
    "        if dataset == \"cqadupstack\":\n",
    "            cqa_ndcgs, cqa_maps, cqa_recalls, cqa_precisions = [], [], [], []\n",
    "            for sub_dataset in os.listdir(data_path):\n",
    "                sub_data_path = f\"datasets/{dataset}/{sub_dataset}\"\n",
    "                results_path = f\"{results_prefix}beir_{model_name}{prompt_id}{maxrerank}_{dataset}_{sub_dataset}.json\"\n",
    "                assert os.path.exists(os.path.join(os.getcwd(), results_path)), f\"Missing path: {results_path}\"\n",
    "\n",
    "                ndcg, _map, recall, precision = compute_result(results_path, sub_data_path)\n",
    "                cqa_ndcgs.append(ndcg)\n",
    "                cqa_maps.append(_map)\n",
    "                cqa_recalls.append(recall)\n",
    "                cqa_precisions.append(precision)\n",
    "\n",
    "            for (metric, group) in [(ndcg, cqa_ndcgs), (_map, cqa_maps), (recall, cqa_recalls), (precision, cqa_precisions)]:\n",
    "                for k in metric.keys():\n",
    "                    metric[k] = sum([score[k] for score in group]) / len(group)\n",
    "\n",
    "        else:\n",
    "            results_path = f\"{results_prefix}beir_{model_name}{prompt_id}{maxrerank}_{dataset}.json\"\n",
    "            assert os.path.exists(os.path.join(os.getcwd(), results_path)), f\"Missing path: {results_path}\"\n",
    "            ndcg, _map, recall, precision = compute_result(results_path, data_path)\n",
    "           \n",
    "        latex_help.setdefault(dataset, \"\")\n",
    "        latex_help[dataset] += f\" & {ndcg['NDCG@10']}\"\n",
    "        latex_help_avgs.setdefault(prompt_id, 0)\n",
    "        latex_help_avgs[prompt_id] += ndcg['NDCG@10']\n",
    "\n",
    "        ndcgs[dataset] = ndcg\n",
    "\n",
    "    with open(scores_out_path, 'w') as fp:\n",
    "        json.dump(ndcgs, fp)\n",
    "        \n",
    "for k, v in latex_help.items():\n",
    "    print(f\"{k} {v}\")\n",
    "\n",
    "print(\"& \".join([f\"{k}: {round(v/len(datasets), 5)}\" for k,v in latex_help_avgs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7055829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
